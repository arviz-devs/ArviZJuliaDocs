[{"id":3,"pagetitle":"Home","title":"ArviZ.jl: Exploratory analysis of Bayesian models in Julia","ref":"/ArviZ/stable/#arvizjl","content":" ArviZ.jl: Exploratory analysis of Bayesian models in Julia ArviZ.jl is a Julia package for exploratory analysis of Bayesian models. It is part of the  ArviZ project , which also includes a related  Python package ."},{"id":4,"pagetitle":"Home","title":"Installation","ref":"/ArviZ/stable/#installation","content":" Installation From the Julia REPL, type  ]  to enter the Pkg REPL mode and run pkg> add ArviZ"},{"id":5,"pagetitle":"Home","title":"Usage","ref":"/ArviZ/stable/#usage","content":" Usage See the  API Overview  for description of functions."},{"id":6,"pagetitle":"Home","title":"Extending ArviZ.jl","ref":"/ArviZ/stable/#extendingarviz","content":" Extending ArviZ.jl To use a custom data type with ArviZ.jl, simply overload  convert_to_inference_data  to convert your input(s) to an  InferenceData ."},{"id":9,"pagetitle":"API Overview","title":"API Overview","ref":"/ArviZ/stable/api/#api","content":" API Overview Data Dataset Diagnostics InferenceData Stats"},{"id":12,"pagetitle":"Data","title":"Data","ref":"/ArviZ/stable/api/data/#data-api","content":" Data ArviZ.from_mcmcchains ArviZ.from_samplechains InferenceObjects.from_netcdf InferenceObjects.to_netcdf"},{"id":13,"pagetitle":"Data","title":"Inference library converters","ref":"/ArviZ/stable/api/data/#Inference-library-converters","content":" Inference library converters"},{"id":14,"pagetitle":"Data","title":"ArviZ.from_mcmcchains","ref":"/ArviZ/stable/api/data/#ArviZ.from_mcmcchains","content":" ArviZ.from_mcmcchains  —  Function from_mcmcchains(posterior::MCMCChains.Chains; kwargs...) -> InferenceData\nfrom_mcmcchains(; kwargs...) -> InferenceData\nfrom_mcmcchains(\n    posterior::MCMCChains.Chains,\n    posterior_predictive,\n    predictions,\n    log_likelihood;\n    kwargs...\n) -> InferenceData Convert data in an  MCMCChains.Chains  format into an  InferenceData . Any keyword argument below without an an explicitly annotated type above is allowed, so long as it can be passed to  convert_to_inference_data . Arguments posterior::MCMCChains.Chains : Draws from the posterior Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution or   name(s) of predictive variables in  posterior predictions : Out-of-sample predictions for the posterior. prior : Draws from the prior prior_predictive : Draws from the prior predictive distribution or name(s) of predictive   variables in  prior observed_data : Observed data on which the  posterior  is conditional. It should only   contain data which is modeled as a random variable. Keys are parameter names and values. constant_data : Model constants, data included in the model that are not modeled as   random variables. Keys are parameter names. predictions_constant_data : Constants relevant to the model predictions (i.e. new  x    values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this   argument as a named tuple whose keys are observed variable names and whose values are log   likelihood arrays. Alternatively, provide the name of variable in  posterior  containing   log likelihoods. library=MCMCChains : Name of library that generated the chains coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions eltypes : Map from variable names to eltypes. This is primarily used to assign discrete   eltypes to discrete variables that were stored in  Chains  as floats. Returns InferenceData : The data with groups corresponding to the provided data source"},{"id":15,"pagetitle":"Data","title":"ArviZ.from_samplechains","ref":"/ArviZ/stable/api/data/#ArviZ.from_samplechains","content":" ArviZ.from_samplechains  —  Function from_samplechains(\n    posterior=nothing;\n    prior=nothing,\n    library=SampleChains,\n    kwargs...,\n) -> InferenceData Convert SampleChains samples to an  InferenceData . Either  posterior  or  prior  may be a  SampleChains.AbstractChain  or  SampleChains.MultiChain  object. For descriptions of remaining  kwargs , see  from_namedtuple . source"},{"id":16,"pagetitle":"Data","title":"IO / Conversion","ref":"/ArviZ/stable/api/data/#IO-/-Conversion","content":" IO / Conversion"},{"id":17,"pagetitle":"Data","title":"InferenceObjects.from_netcdf","ref":"/ArviZ/stable/api/data/#InferenceObjects.from_netcdf","content":" InferenceObjects.from_netcdf  —  Function from_netcdf(path::AbstractString; kwargs...) -> InferenceData Load an  InferenceData  from an unopened NetCDF file. Remaining  kwargs  are passed to  NCDatasets.NCDataset . This method loads data eagerly. To instead load data lazily, pass an opened  NCDataset  to  from_netcdf . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_netcdf(\"centered_eight.nc\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data from_netcdf(ds::NCDatasets.NCDataset; load_mode) -> InferenceData Load an  InferenceData  from an opened NetCDF file. load_mode  defaults to  :lazy , which avoids reading variables into memory. Operations on these arrays will be slow.  load_mode  can also be  :eager , which copies all variables into memory. It is then safe to close  ds . If  load_mode  is  :lazy  and  ds  is closed after constructing  InferenceData , using the variable arrays will have undefined behavior. Examples Here is how we might load an  InferenceData  from an  InferenceData  lazily from a web-hosted NetCDF file. julia> using HTTP, InferenceObjects, NCDatasets\n\njulia> resp = HTTP.get(\"https://github.com/arviz-devs/arviz_example_data/blob/main/data/centered_eight.nc?raw=true\");\n\njulia> ds = NCDataset(\"centered_eight\", \"r\"; memory = resp.body);\n\njulia> idata = from_netcdf(ds)\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\njulia> idata_copy = copy(idata); # disconnect from the loaded dataset\n\njulia> close(ds);"},{"id":18,"pagetitle":"Data","title":"InferenceObjects.to_netcdf","ref":"/ArviZ/stable/api/data/#InferenceObjects.to_netcdf","content":" InferenceObjects.to_netcdf  —  Function to_netcdf(data, dest::AbstractString; group::Symbol=:posterior, kwargs...)\nto_netcdf(data, dest::NCDatasets.NCDataset; group::Symbol=:posterior) Write  data  to a NetCDF file. data  is any type that can be converted to an  InferenceData  using  convert_to_inference_data . If not an  InferenceData , then  group  specifies which group the data represents. dest  specifies either the path to the NetCDF file or an opened NetCDF file. If  dest  is a path, remaining  kwargs  are passed to  NCDatasets.NCDataset . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_namedtuple((; x = randn(4, 100, 3), z = randn(4, 100)))\nInferenceData with groups:\n  > posterior\n\njulia> to_netcdf(idata, \"data.nc\")\n\"data.nc\""},{"id":21,"pagetitle":"Dataset","title":"Dataset","ref":"/ArviZ/stable/api/dataset/#dataset-api","content":" Dataset InferenceObjects.Dataset InferenceObjects.convert_to_dataset InferenceObjects.namedtuple_to_dataset"},{"id":22,"pagetitle":"Dataset","title":"Type definition","ref":"/ArviZ/stable/api/dataset/#Type-definition","content":" Type definition"},{"id":23,"pagetitle":"Dataset","title":"InferenceObjects.Dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.Dataset","content":" InferenceObjects.Dataset  —  Type Dataset{L} <: DimensionalData.AbstractDimStack{L} Container of dimensional arrays sharing some dimensions. This type is an  DimensionalData.AbstractDimStack  that implements the same interface as  DimensionalData.DimStack  and has identical usage. When a  Dataset  is passed to Python, it is converted to an  xarray.Dataset  without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an  xarray.Dataset  is passed to Julia, its data must be copied. Constructors Dataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n) In most cases, use  convert_to_dataset  to create a  Dataset  instead of directly using a constructor."},{"id":24,"pagetitle":"Dataset","title":"General conversion","ref":"/ArviZ/stable/api/dataset/#General-conversion","content":" General conversion"},{"id":25,"pagetitle":"Dataset","title":"InferenceObjects.convert_to_dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.convert_to_dataset","content":" InferenceObjects.convert_to_dataset  —  Function convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset Convert a supported object to a  Dataset . In most cases, this function calls  convert_to_inference_data  and returns the corresponding  group ."},{"id":26,"pagetitle":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.namedtuple_to_dataset","content":" InferenceObjects.namedtuple_to_dataset  —  Function namedtuple_to_dataset(data; kwargs...) -> Dataset Convert  NamedTuple  mapping variable names to arrays to a  Dataset . Any non-array values will be converted to a 0-dimensional array. Keywords attrs::AbstractDict{<:AbstractString} : a collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable. library::Union{String,Module} : library used for performing inference. Will be attached to the  attrs  metadata. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated."},{"id":27,"pagetitle":"Dataset","title":"DimensionalData","ref":"/ArviZ/stable/api/dataset/#DimensionalData","content":" DimensionalData As a  DimensionalData.AbstractDimStack ,  Dataset  also implements the  AbstractDimStack  API and can be used like a  DimStack . See  DimensionalData's documentation  for example usage."},{"id":28,"pagetitle":"Dataset","title":"Tables inteface","ref":"/ArviZ/stable/api/dataset/#Tables-inteface","content":" Tables inteface Dataset  implements the  Tables  interface. This allows  Dataset s to be used as sources for any function that can accept a table. For example, it's straightforward to: write to CSV with CSV.jl flatten to a DataFrame with DataFrames.jl plot with StatsPlots.jl plot with AlgebraOfGraphics.jl"},{"id":31,"pagetitle":"Diagnostics","title":"Diagnostics","ref":"/ArviZ/stable/api/diagnostics/#diagnostics-api","content":" Diagnostics MCMCDiagnosticTools.AutocovMethod MCMCDiagnosticTools.BDAAutocovMethod MCMCDiagnosticTools.FFTAutocovMethod MCMCDiagnosticTools.bfmi MCMCDiagnosticTools.ess MCMCDiagnosticTools.ess_rhat MCMCDiagnosticTools.mcse MCMCDiagnosticTools.rhat MCMCDiagnosticTools.rstar"},{"id":32,"pagetitle":"Diagnostics","title":"Bayesian fraction of missing information","ref":"/ArviZ/stable/api/diagnostics/#bfmi","content":" Bayesian fraction of missing information"},{"id":33,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.bfmi","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.bfmi","content":" MCMCDiagnosticTools.bfmi  —  Function bfmi(energy::AbstractVector{<:Real}) -> Real\nbfmi(energy::AbstractMatrix{<:Real}; dims::Int=1) -> AbstractVector{<:Real} Calculate the estimated Bayesian fraction of missing information (BFMI). When sampling with Hamiltonian Monte Carlo (HMC), BFMI quantifies how well momentum resampling matches the marginal energy distribution. The current advice is that values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may change. A BFMI value below the threshold often indicates poor adaptation of sampling parameters or that the target distribution has heavy tails that were not well explored by the Markov chain. For more information, see Section 6.1 of  [Betancourt2018]  or  [Betancourt2016]  for a complete account. energy  is either a vector of Hamiltonian energies of draws or a matrix of energies of draws for multiple chains.  dims  indicates the dimension in  energy  that contains the draws. The default  dims=1  assumes  energy  has the shape  draws  or  (draws, chains) . If a different shape is provided,  dims  must be set accordingly. If  energy  is a vector, a single BFMI value is returned. Otherwise, a vector of BFMI values for each chain is returned."},{"id":34,"pagetitle":"Diagnostics","title":"Effective sample size and  $\\widehat{R}$  diagnostic","ref":"/ArviZ/stable/api/diagnostics/#ess_rhat","content":" Effective sample size and  $\\widehat{R}$  diagnostic"},{"id":35,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.ess","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.ess","content":" MCMCDiagnosticTools.ess  —  Function ess(data::InferenceData; kwargs...) -> Dataset\ness(data::Dataset; kwargs...) -> Dataset Calculate the effective sample size (ESS) for each parameter in the data. ess(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind=:bulk,\n    relative::Bool=false,\n    autocov_method=AutocovMethod(),\n    split_chains::Int=2,\n    maxlag::Int=250,\n    kwargs...\n) Estimate the effective sample size (ESS) of the  samples  of shape  (draws, [chains[, parameters...]])  with the  autocov_method . Optionally, the  kind  of ESS estimate to be computed can be specified (see below). Some  kind s accept additional  kwargs . If  relative  is  true , the relative ESS is returned, i.e.  ess / (draws * chains) . split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. There must be at least 3 draws in each chain after splitting. maxlag  indicates the maximum lag for which autocovariance is computed and must be greater than 0. For a given estimand, it is recommended that the ESS is at least  100 * chains  and that  $\\widehat{R} < 1.01$ . [VehtariGelman2021] See also:  AutocovMethod ,  FFTAutocovMethod ,  BDAAutocovMethod ,  rhat ,  ess_rhat ,  mcse Kinds of ESS estimates If  kind  isa a  Symbol , it may take one of the following values: :bulk : basic ESS computed on rank-normalized draws. This kind diagnoses poor convergence   in the bulk of the distribution due to trends or different locations of the chains. :tail : minimum of the quantile-ESS for the symmetric quantiles where    tail_prob=0.1  is the probability in the tails. This kind diagnoses poor convergence in   the tails of the distribution. If this kind is chosen,  kwargs  may contain a    tail_prob  keyword. :basic : basic ESS, equivalent to specifying  kind=Statistics.mean . Note While Bulk-ESS is conceptually related to basic ESS, it is well-defined even if the chains do not have finite variance. [VehtariGelman2021]  For each parameter, rank-normalization proceeds by first ranking the inputs using \"tied ranking\" and then transforming the ranks to normal quantiles so that the result is standard normally distributed. This transform is monotonic. Otherwise,  kind  specifies one of the following estimators, whose ESS is to be estimated: Statistics.mean Statistics.median Statistics.std StatsBase.mad Base.Fix2(Statistics.quantile, p::Real)"},{"id":36,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.rhat","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.rhat","content":" MCMCDiagnosticTools.rhat  —  Function rhat(data::InferenceData; kwargs...) -> Dataset\nrhat(data::Dataset; kwargs...) -> Dataset Calculate the  $\\widehat{R}$  diagnostic for each parameter in the data. rhat(samples::AbstractArray{Union{Real,Missing}}; kind::Symbol=:rank, split_chains=2) Compute the  $\\widehat{R}$  diagnostics for each parameter in  samples  of shape  (draws, [chains[, parameters...]]) . [VehtariGelman2021] kind  indicates the kind of  $\\widehat{R}$  to compute (see below). split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. See also  ess ,  ess_rhat ,  rstar Kinds of  $\\widehat{R}$ The following  kind s are supported: :rank : maximum of  $\\widehat{R}$  with  kind=:bulk  and  kind=:tail . :bulk : basic  $\\widehat{R}$  computed on rank-normalized draws. This kind diagnoses   poor convergence in the bulk of the distribution due to trends or different locations of   the chains. :tail :  $\\widehat{R}$  computed on draws folded around the median and then   rank-normalized. This kind diagnoses poor convergence in the tails of the distribution   due to different scales of the chains. :basic : Classic  $\\widehat{R}$ ."},{"id":37,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.ess_rhat","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.ess_rhat","content":" MCMCDiagnosticTools.ess_rhat  —  Function ess_rhat(data::InferenceData; kwargs...) -> Dataset\ness_rhat(data::Dataset; kwargs...) -> Dataset Calculate the effective sample size (ESS) and  $\\widehat{R}$  diagnostic for each parameter in the data. ess_rhat(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind::Symbol=:rank,\n    kwargs...,\n) -> NamedTuple{(:ess, :rhat)} Estimate the effective sample size and  $\\widehat{R}$  of the  samples  of shape  (draws, [chains[, parameters...]]) . When both ESS and  $\\widehat{R}$  are needed, this method is often more efficient than calling  ess  and  rhat  separately. See  rhat  for a description of supported  kind s and  ess  for a description of  kwargs . The following autocovariance methods are supported:"},{"id":38,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.AutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.AutocovMethod","content":" MCMCDiagnosticTools.AutocovMethod  —  Type AutocovMethod <: AbstractAutocovMethod The  AutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021]  and uses the biased estimator of the autocovariance, as discussed by  [Geyer1992] ."},{"id":39,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.FFTAutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.FFTAutocovMethod","content":" MCMCDiagnosticTools.FFTAutocovMethod  —  Type FFTAutocovMethod <: AbstractAutocovMethod The  FFTAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. The algorithm is the same as the one of  AutocovMethod  but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation. Info To be able to use this method, you have to load a package that implements the  AbstractFFTs.jl  interface such as  FFTW.jl  or  FastTransforms.jl ."},{"id":40,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.BDAAutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.BDAAutocovMethod","content":" MCMCDiagnosticTools.BDAAutocovMethod  —  Type BDAAutocovMethod <: AbstractAutocovMethod The  BDAAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021] . and uses the variogram estimator of the autocorrelation function discussed by  [BDA3] ."},{"id":41,"pagetitle":"Diagnostics","title":"Monte Carlo standard error","ref":"/ArviZ/stable/api/diagnostics/#mcse","content":" Monte Carlo standard error"},{"id":42,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.mcse","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.mcse","content":" MCMCDiagnosticTools.mcse  —  Function ess(data::InferenceData; kwargs...) -> Dataset\ness(data::Dataset; kwargs...) -> Dataset Calculate the Monte Carlo standard error (MCSE) for each parameter in the data. mcse(samples::AbstractArray{<:Union{Missing,Real}}; kind=Statistics.mean, kwargs...) Estimate the Monte Carlo standard errors (MCSE) of the estimator  kind  applied to  samples  of shape  (draws, [chains[, parameters...]]) . See also:  ess Kinds of MCSE estimates The estimator whose MCSE should be estimated is specified with  kind .  kind  must accept a vector of the same  eltype  as  samples  and return a real estimate. For the following estimators, the effective sample size  ess  and an estimate of the asymptotic variance are used to compute the MCSE, and  kwargs  are forwarded to  ess : Statistics.mean Statistics.median Statistics.std Base.Fix2(Statistics.quantile, p::Real) For other estimators, the subsampling bootstrap method (SBM) [FlegalJones2011] [Flegal2012]  is used as a fallback, and the only accepted  kwargs  are  batch_size , which indicates the size of the overlapping batches used to estimate the MCSE, defaulting to  floor(Int, sqrt(draws * chains)) . Note that SBM tends to underestimate the MCSE, especially for highly autocorrelated chains. One should verify that autocorrelation is low by checking the bulk- and tail-ESS values."},{"id":43,"pagetitle":"Diagnostics","title":"$R^*$  diagnostic","ref":"/ArviZ/stable/api/diagnostics/#rstar","content":" $R^*$  diagnostic"},{"id":44,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.rstar","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.rstar","content":" MCMCDiagnosticTools.rstar  —  Function rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    data::Union{InferenceData,Dataset};\n    kwargs...,\n) Calculate the  $R^*$  diagnostic for the data. rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples,\n    chain_indices::AbstractVector{Int};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the table  samples  with the  classifier . samples  must be either an  AbstractMatrix , an  AbstractVector , or a table (i.e. implements the Tables.jl interface) whose rows are draws and whose columns are parameters. chain_indices  indicates the chain ids of each row of  samples . This method supports ragged chains, i.e. chains of nonequal lengths. rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples::AbstractArray{<:Real};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the  samples  with the  classifier . samples  is an array of draws with the shape  (draws, [chains[, parameters...]]) .` This implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari. The  classifier  has to be a supervised classifier of the MLJ framework (see the  MLJ documentation  for a list of supported models). It is trained with a  subset  of the samples from each chain. Each chain is split into  split_chains  separate chains to additionally check for within-chain convergence. The training of the classifier can be inspected by adjusting the  verbosity  level. If the classifier is deterministic, i.e., if it predicts a class, the value of the  $R^*$  statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the  $R^*$  statistic is returned (algorithm 2). Note The correctness of the statistic depends on the convergence of the  classifier  used internally in the statistic. Examples julia> using MLJBase, MLJIteration, EvoTrees, Statistics\n\njulia> samples = fill(4.0, 100, 3, 2); One can compute the distribution of the  $R^*$  statistic (algorithm 2) with a probabilistic classifier. For instance, we can use a gradient-boosted trees model with  nrounds = 100  sequentially stacked trees and learning rate  eta = 0.05 : julia> model = EvoTreeClassifier(; nrounds=100, eta=0.05);\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 Note, however, that it is recommended to determine  nrounds  based on early-stopping. With the MLJ framework, this can be achieved in the following way (see the  MLJ documentation  for additional explanations): julia> model = IteratedModel(;\n           model=EvoTreeClassifier(; eta=0.05),\n           iteration_parameter=:nrounds,\n           resampling=Holdout(),\n           measures=log_loss,\n           controls=[Step(5), Patience(2), NumberLimit(100)],\n           retrain=true,\n       );\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 For deterministic classifiers, a single  $R^*$  statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models. julia> evotree_deterministic = Pipeline(model; operation=predict_mode);\n\njulia> value = rstar(evotree_deterministic, samples);\n\njulia> round(value; digits=2)\n1.0 References Lambert, B., & Vehtari, A. (2020).  $R^*$ : A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers. Betancourt2018 Betancourt M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo.  arXiv:1701.02434v2  [stat.ME] Betancourt2016 Betancourt M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.  arXiv:1604.00695v1  [stat.ME] VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 Geyer1992 Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483. VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 BDA3 Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press. FlegalJones2011 Flegal JM, Jones GL. (2011) Implementing MCMC: estimating with confidence.                 Handbook of Markov Chain Monte Carlo. pp. 175-97.                  pdf Flegal2012 Flegal JM. (2012) Applicability of subsampling bootstrap methods in Markov chain Monte Carlo.            Monte Carlo and Quasi-Monte Carlo Methods 2010. pp. 363-72.            doi:  10.1007/978-3-642-27440-4_18"},{"id":47,"pagetitle":"InferenceData","title":"InferenceData","ref":"/ArviZ/stable/api/inference_data/#inferencedata-api","content":" InferenceData InferenceObjects.InferenceData Base.cat Base.getindex Base.getproperty Base.merge Base.propertynames Base.setindex InferenceObjects.convert_to_inference_data InferenceObjects.from_dict InferenceObjects.from_namedtuple"},{"id":48,"pagetitle":"InferenceData","title":"Type definition","ref":"/ArviZ/stable/api/inference_data/#Type-definition","content":" Type definition"},{"id":49,"pagetitle":"InferenceData","title":"InferenceObjects.InferenceData","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.InferenceData","content":" InferenceObjects.InferenceData  —  Type InferenceData{group_names,group_types} Container for inference data storage using DimensionalData. This object implements the  InferenceData schema . Internally, groups are stored in a  NamedTuple , which can be accessed using  parent(::InferenceData) . Constructors InferenceData(groups::NamedTuple)\nInferenceData(; groups...) Construct an inference data from either a  NamedTuple  or keyword arguments of groups. Groups must be  Dataset  objects. Instead of directly creating an  InferenceData , use the exported  from_xyz  functions or  convert_to_inference_data ."},{"id":50,"pagetitle":"InferenceData","title":"Property interface","ref":"/ArviZ/stable/api/inference_data/#Property-interface","content":" Property interface"},{"id":51,"pagetitle":"InferenceData","title":"Base.getproperty","ref":"/ArviZ/stable/api/inference_data/#Base.getproperty","content":" Base.getproperty  —  Function getproperty(data::InferenceData, name::Symbol) -> Dataset Get group with the specified  name ."},{"id":52,"pagetitle":"InferenceData","title":"Base.propertynames","ref":"/ArviZ/stable/api/inference_data/#Base.propertynames","content":" Base.propertynames  —  Function propertynames(data::InferenceData) -> Tuple{Symbol} Get names of groups"},{"id":53,"pagetitle":"InferenceData","title":"Indexing interface","ref":"/ArviZ/stable/api/inference_data/#Indexing-interface","content":" Indexing interface"},{"id":54,"pagetitle":"InferenceData","title":"Base.getindex","ref":"/ArviZ/stable/api/inference_data/#Base.getindex","content":" Base.getindex  —  Function Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData Return a new  InferenceData  containing the specified groups sliced to the specified coords. coords  specifies a dimension name mapping to an index, a  DimensionalData.Selector , or an  IntervalSets.AbstractInterval . If one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised. Examples Select data from all groups for just the specified id values. julia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\" Select data from just the posterior, returning a  Dataset  if the indices index more than one element from any of the variables: julia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:19:25.982\" Note that if a single index is provided, the behavior is still to slice so that the dimension is preserved."},{"id":55,"pagetitle":"InferenceData","title":"Base.setindex","ref":"/ArviZ/stable/api/inference_data/#Base.setindex","content":" Base.setindex  —  Function Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData Create a new  InferenceData  containing the  group  with the specified  name . If a group with  name  is already in  data , it is replaced."},{"id":56,"pagetitle":"InferenceData","title":"Iteration interface","ref":"/ArviZ/stable/api/inference_data/#Iteration-interface","content":" Iteration interface InferenceData  also implements the same iteration interface as its underlying  NamedTuple . That is, iterating over an  InferenceData  iterates over its groups."},{"id":57,"pagetitle":"InferenceData","title":"General conversion","ref":"/ArviZ/stable/api/inference_data/#General-conversion","content":" General conversion"},{"id":58,"pagetitle":"InferenceData","title":"InferenceObjects.convert_to_inference_data","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.convert_to_inference_data","content":" InferenceObjects.convert_to_inference_data  —  Function convert_to_inference_data(obj; group, kwargs...) -> InferenceData Convert a supported object to an  InferenceData  object. If  obj  converts to a single dataset,  group  specifies which dataset in the resulting  InferenceData  that is. See  convert_to_dataset Arguments obj  can be many objects. Basic supported types are: InferenceData : return unchanged Dataset / DimensionalData.AbstractDimStack : add to  InferenceData  as the only group NamedTuple / AbstractDict : create a  Dataset  as the only group AbstractArray{<:Real} : create a  Dataset  as the only group, given an arbitrary name, if the name is not set More specific types may be documented separately. Keywords group::Symbol = :posterior : If  obj  converts to a single dataset, assign the resulting dataset to this group. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. kwargs : remaining keywords forwarded to converter functions"},{"id":59,"pagetitle":"InferenceData","title":"InferenceObjects.from_dict","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.from_dict","content":" InferenceObjects.from_dict  —  Function from_dict(posterior::AbstractDict; kwargs...) -> InferenceData Convert a  Dict  to an  InferenceData . Arguments posterior : The data to be converted. Its strings must be  Symbol  or  AbstractString , and its values must be arrays. Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior::Dict=nothing : Draws from the prior prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata = Dict(\n    :x => rand(ndraws, nchains),\n    :y => randn(2, ndraws, nchains),\n    :z => randn(3, 2, ndraws, nchains),\n)\nidata = from_dict(data)"},{"id":60,"pagetitle":"InferenceData","title":"InferenceObjects.from_namedtuple","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.from_namedtuple","content":" InferenceObjects.from_namedtuple  —  Function from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData Convert a  NamedTuple  or container of  NamedTuple s to an  InferenceData . If containers are passed, they are flattened into a single  NamedTuple  with array elements whose first dimensions correspond to the dimensions of the containers. Arguments posterior : The data to be converted. It may be of the following types: ::NamedTuple : The keys are the variable names and the values are arrays with dimensions  (ndraws, nchains[, sizes...]) . ::Vector{Vector{<:NamedTuple}} : A vector of length  nchains  whose elements have length  ndraws . Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior=nothing : Draws from the prior. Accepts the same types as  posterior . prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Note If a  NamedTuple  is provided for  observed_data ,  constant_data , or predictions constant data`, any non-array values (e.g. integers) are converted to 0-dimensional arrays. Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata1 = (\n    x=rand(ndraws, nchains), y=randn(ndraws, nchains, 2), z=randn(ndraws, nchains, 3, 2)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata2 = from_namedtuple(data2)"},{"id":61,"pagetitle":"InferenceData","title":"General functions","ref":"/ArviZ/stable/api/inference_data/#General-functions","content":" General functions"},{"id":62,"pagetitle":"InferenceData","title":"Base.cat","ref":"/ArviZ/stable/api/inference_data/#Base.cat","content":" Base.cat  —  Function cat(data::InferenceData...; [groups=keys(data[1]),] dims) -> InferenceData Concatenate  InferenceData  objects along the specified dimension  dims . Only the groups in  groups  are concatenated. Remaining groups are  merge d into the new  InferenceData  object. Examples Here is how we can concatenate all groups of two  InferenceData  objects along the existing  chain  dimension: julia> coords = (; a_dim=[\"x\", \"y\", \"z\"]);\n\njulia> dims = dims=(; a=[:a_dim]);\n\njulia> data = Dict(:a => randn(100, 4, 3), :b => randn(100, 4));\n\njulia> idata = from_dict(data; coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1 = cat(idata, idata; dims=:chain)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim} (100×8×3)\n  :b Float64 dims: Dim{:draw}, Dim{:chain} (100×8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\" Alternatively, we can concatenate along a new  run  dimension, which will be created. julia> idata_cat2 = cat(idata, idata; dims=:run)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat2.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\" We can also concatenate only a subset of groups and merge the rest, which is useful when some groups are present only in some of the  InferenceData  objects or will be identical in all of them: julia> observed_data = Dict(:y => randn(10));\n\njulia> idata2 = from_dict(data; observed_data=observed_data, coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3 = cat(idata, idata2; groups=(:posterior,), dims=:run)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\"\n\njulia> idata_cat3.observed_data\nDataset with dimensions: Dim{:y_dim_1}\nand 1 layer:\n  :y Float64 dims: Dim{:y_dim_1} (10)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-02-17T15:11:00.59\""},{"id":63,"pagetitle":"InferenceData","title":"Base.merge","ref":"/ArviZ/stable/api/inference_data/#Base.merge","content":" Base.merge  —  Function merge(data::InferenceData...) -> InferenceData Merge  InferenceData  objects. The result contains all groups in  data  and  others . If a group appears more than once, the one that occurs last is kept. See also:  cat Examples Here we merge an  InferenceData  containing only a posterior group with one containing only a prior group to create a new one containing both groups. julia> idata1 = from_dict(Dict(:a => randn(100, 4, 3), :b => randn(100, 4)))\nInferenceData with groups:\n  > posterior\n\njulia> idata2 = from_dict(; prior=Dict(:a => randn(100, 1, 3), :c => randn(100, 1)))\nInferenceData with groups:\n  > prior\n\njulia> idata_merged = merge(idata1, idata2)\nInferenceData with groups:\n  > posterior\n  > prior"},{"id":66,"pagetitle":"Stats","title":"Stats","ref":"/ArviZ/stable/api/stats/#stats-api","content":" Stats ArviZ.ArviZStats.AbstractELPDResult ArviZ.ArviZStats.AbstractModelWeightsMethod ArviZ.ArviZStats.BootstrappedPseudoBMA ArviZ.ArviZStats.ModelComparisonResult ArviZ.ArviZStats.PSISLOOResult ArviZ.ArviZStats.PseudoBMA ArviZ.ArviZStats.Stacking ArviZ.ArviZStats.SummaryStats ArviZ.ArviZStats.WAICResult PSIS.PSISResult ArviZ.ArviZStats.compare ArviZ.ArviZStats.elpd_estimates ArviZ.ArviZStats.hdi ArviZ.ArviZStats.hdi! ArviZ.ArviZStats.information_criterion ArviZ.ArviZStats.loo ArviZ.ArviZStats.loo_pit ArviZ.ArviZStats.model_weights ArviZ.ArviZStats.r2_score ArviZ.ArviZStats.smooth_data ArviZ.ArviZStats.waic PSIS.psis PSIS.psis! StatsBase.summarystats"},{"id":67,"pagetitle":"Stats","title":"Summary statistics","ref":"/ArviZ/stable/api/stats/#Summary-statistics","content":" Summary statistics"},{"id":68,"pagetitle":"Stats","title":"ArviZ.ArviZStats.SummaryStats","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.SummaryStats","content":" ArviZ.ArviZStats.SummaryStats  —  Type A container for a column table of values computed by  summarystats . This object implements the Tables and TableTraits interfaces and has a custom  show  method. data : The summary statistics for each variable, with the first entry containing the variable names source"},{"id":69,"pagetitle":"Stats","title":"StatsBase.summarystats","ref":"/ArviZ/stable/api/stats/#StatsBase.summarystats","content":" StatsBase.summarystats  —  Function summarystats(data::InferenceData; group=:posterior, kwargs...)\nsummarystats(data::Dataset; kwargs...) Compute summary statistics and diagnostics on the  data . Keywords prob_interval::Real : The value of the  prob  argument to  hdi  used to compute the highest density interval. Defaults to 0.94. return_type::Type : The type of object to return. Valid options are  Dataset  and  SummaryStats . Defaults to  SummaryStats . metric_dim : The dimension name or type to use for the computed metrics. Only used if  return_type  is  Dataset . Defaults to  Dim{:_metric} . compact_labels::Bool : Whether to use compact names for the variables. Only used if  return_type  is  SummaryStats . Defaults to  true . kind::Symbol : Whether to compute just statistics ( :stats ), just diagnostics ( :diagnostics ), or both ( :both ). Defaults to  :both . Examples Compute the summary statistics and diagnostics on posterior draws of the centered eight model: julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> summarystats(idata.posterior[(:mu, :tau)])\nSummaryStats\n      mean  std  hdi_3%  hdi_97%  mcse_mean  mcse_std  ess_tail  ess_bulk  rha ⋯\n mu    4.5  3.5  -1.62     10.7        0.23      0.11       659       241  1.0 ⋯\n tau   4.1  3.1   0.896     9.67       0.26      0.17        38        67  1.0 ⋯\n                                                                1 column omitted Compute just the statistics on all variables: julia> summarystats(idata.posterior; kind=:stats)\nSummaryStats\n                          mean   std  hdi_3%  hdi_97%\n mu                       4.49  3.49  -1.62     10.7\n theta[Choate]            6.46  5.87  -4.56     17.1\n theta[Deerfield]         5.03  4.88  -4.31     14.3\n theta[Phillips Andover]  3.94  5.69  -7.77     13.7\n theta[Phillips Exeter]   4.87  5.01  -4.49     14.7\n theta[Hotchkiss]         3.67  4.96  -6.47     11.7\n theta[Lawrenceville]     3.97  5.19  -7.04     12.2\n theta[St. Paul's]        6.58  5.11  -3.09     16.3\n theta[Mt. Hermon]        4.77  5.74  -5.86     16.0\n tau                      4.12  3.10   0.896     9.67 Compute the statistics and diagnostics from the posterior group of an  InferenceData  and store in a  Dataset : julia> summarystats(idata; return_type=Dataset)\nDataset with dimensions:\n  Dim{:_metric} Categorical{String} String[mean, std, …, ess_bulk, rhat] Unordered,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:_metric} (9)\n  :theta Float64 dims: Dim{:school}, Dim{:_metric} (8×9)\n  :tau   Float64 dims: Dim{:_metric} (9) source"},{"id":70,"pagetitle":"Stats","title":"General statistics","ref":"/ArviZ/stable/api/stats/#General-statistics","content":" General statistics"},{"id":71,"pagetitle":"Stats","title":"ArviZ.ArviZStats.hdi","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.hdi","content":" ArviZ.ArviZStats.hdi  —  Function hdi(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) Estimate the unimodal highest density interval (HDI) of  samples  for the probability  prob . The HDI is the minimum width Bayesian credible interval (BCI). That is, it is the smallest possible interval containing at least  (100*prob) % of the draws. [Hyndman1996] samples  is an array of shape  (draws[, chains[, params...]]) . If multiple parameters are present, then  lower  and  upper  are arrays with the shape  (params...,) , computed separately for each marginal. This implementation uses the algorithm of  [ChenShao1999] . Note Any default value of  prob  is arbitrary. The default value of  prob=0.94  instead of a more common default like  prob=0.95  is chosen to reminder the user of this arbitrariness. Examples Here we calculate the 83% HDI for a normal random variable: julia> using ArviZ\n\njulia> x = randn(2_000);\n\njulia> hdi(x; prob=0.83) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => -1.38266\n  :upper => 1.25982 We can also calculate the HDI for a 3-dimensional array of samples: julia> x = randn(1_000, 1, 1) .+ reshape(0:5:10, 1, 1, :);\n\njulia> hdi(x) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => [-1.9674, 3.0326, 8.0326]\n  :upper => [1.90028, 6.90028, 11.9003] source hdi(data::InferenceData; prob=0.94) -> Dataset\nhdi(data::Dataset; prob=0.94) -> Dataset Calculate the highest density interval (HDI) for each parameter in the data. Examples Calculate HDI for all parameters in the  posterior  group of an  InferenceData : julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> hdi(idata)\nDataset with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:hdi_bound} Categorical{Symbol} Symbol[:lower, :upper] ForwardOrdered\nand 3 layers:\n  :mu    Float64 dims: Dim{:hdi_bound} (2)\n  :theta Float64 dims: Dim{:school}, Dim{:hdi_bound} (8×2)\n  :tau   Float64 dims: Dim{:hdi_bound} (2) We can also calculate the HDI for a subset of variables: julia> hdi(idata.posterior[(:theta,)]).theta\n8×2 DimArray{Float64,2} theta with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:hdi_bound} Categorical{Symbol} Symbol[:lower, :upper] ForwardOrdered\n                        :lower    :upper\n  \"Choate\"            -4.56375  17.1324\n  \"Deerfield\"         -4.31055  14.2535\n  \"Phillips Andover\"  -7.76922  13.6755\n  \"Phillips Exeter\"   -4.48955  14.6635\n  \"Hotchkiss\"         -6.46991  11.7191\n  \"Lawrenceville\"     -7.04111  12.2087\n  \"St. Paul's\"        -3.09262  16.2685\n  \"Mt. Hermon\"        -5.85834  16.0143 source"},{"id":72,"pagetitle":"Stats","title":"ArviZ.ArviZStats.hdi!","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.hdi!","content":" ArviZ.ArviZStats.hdi!  —  Function hdi!(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) A version of  hdi  that sorts  samples  in-place while computing the HDI. source"},{"id":73,"pagetitle":"Stats","title":"ArviZ.ArviZStats.r2_score","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.r2_score","content":" ArviZ.ArviZStats.r2_score  —  Function r2_score(y_true::AbstractVector, y_pred::AbstractVecOrMat) -> (; r2, r2_std) $R²$  for linear Bayesian regression models. [GelmanGoodrich2019] Arguments y_true : Observed data of length  noutputs y_pred : Predicted data with size  (ndraws[, nchains], noutputs) Examples julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"regression1d\");\n\njulia> y_true = idata.observed_data.y;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.y, (:draw, :chain, :y_dim_0));\n\njulia> r2_score(y_true, y_pred) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.683197\n  :r2_std => 0.0368838 source r2_score(idata::InferenceData; y_name, y_pred_name) -> (; r2, r2_std) Compute  $R²$  from  idata , automatically formatting the predictions to the correct shape. Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. Examples julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"regression10d\");\n\njulia> r2_score(idata) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.998385\n  :r2_std => 0.000100621 source"},{"id":74,"pagetitle":"Stats","title":"Pareto-smoothed importance sampling","ref":"/ArviZ/stable/api/stats/#Pareto-smoothed-importance-sampling","content":" Pareto-smoothed importance sampling"},{"id":75,"pagetitle":"Stats","title":"PSIS.PSISResult","ref":"/ArviZ/stable/api/stats/#PSIS.PSISResult","content":" PSIS.PSISResult  —  Type PSISResult Result of Pareto-smoothed importance sampling (PSIS) using  psis . Properties log_weights : un-normalized Pareto-smoothed log weights weights : normalized Pareto-smoothed weights (allocates a copy) pareto_shape : Pareto  $k=ξ$  shape parameter nparams : number of parameters in  log_weights ndraws : number of draws in  log_weights nchains : number of chains in  log_weights reff : the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size. ess : estimated effective sample size of estimate of mean using smoothed importance samples (see  ess_is ) tail_length : length of the upper tail of  log_weights  that was smoothed tail_dist : the generalized Pareto distribution that was fit to the tail of  log_weights . Note that the tail weights are scaled to have a maximum of 1, so  tail_dist * exp(maximum(log_ratios))  is the corresponding fit directly to the tail of  log_ratios . normalized::Bool :indicates whether  log_weights  are log-normalized along the sample dimensions. Diagnostic The  pareto_shape  parameter  $k=ξ$  of the generalized Pareto distribution  tail_dist  can be used to diagnose reliability and convergence of estimates using the importance weights  [VehtariSimpson2021] . if  $k < \\frac{1}{3}$ , importance sampling is stable, and importance sampling (IS) and PSIS both are reliable. if  $k ≤ \\frac{1}{2}$ , then the importance ratio distributon has finite variance, and the central limit theorem holds. As  $k$  approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE. if  $\\frac{1}{2} < k ≤ 0.7$ , then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime. if  $0.7 < k ≤ 1$ , then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended. if  $k > 1$ , then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes. See  PSISPlots.paretoshapeplot  for a diagnostic plot."},{"id":76,"pagetitle":"Stats","title":"PSIS.psis","ref":"/ArviZ/stable/api/stats/#PSIS.psis","content":" PSIS.psis  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot."},{"id":77,"pagetitle":"Stats","title":"PSIS.psis!","ref":"/ArviZ/stable/api/stats/#PSIS.psis!","content":" PSIS.psis!  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot."},{"id":78,"pagetitle":"Stats","title":"LOO and WAIC","ref":"/ArviZ/stable/api/stats/#LOO-and-WAIC","content":" LOO and WAIC"},{"id":79,"pagetitle":"Stats","title":"ArviZ.ArviZStats.AbstractELPDResult","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.AbstractELPDResult","content":" ArviZ.ArviZStats.AbstractELPDResult  —  Type abstract type AbstractELPDResult An abstract type representing the result of an ELPD computation. Every subtype stores estimates of both the expected log predictive density ( elpd ) and the effective number of parameters  p , as well as standard errors and pointwise estimates of each, from which other relevant estimates can be computed. Subtypes implement the following functions: elpd_estimates information_criterion source"},{"id":80,"pagetitle":"Stats","title":"ArviZ.ArviZStats.PSISLOOResult","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.PSISLOOResult","content":" ArviZ.ArviZStats.PSISLOOResult  —  Type Results of Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO). See also:  loo ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates psis_result : Pareto-smoothed importance sampling (PSIS) results source"},{"id":81,"pagetitle":"Stats","title":"ArviZ.ArviZStats.WAICResult","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.WAICResult","content":" ArviZ.ArviZStats.WAICResult  —  Type Results of computing the widely applicable information criterion (WAIC). See also:  waic ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates source"},{"id":82,"pagetitle":"Stats","title":"ArviZ.ArviZStats.elpd_estimates","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.elpd_estimates","content":" ArviZ.ArviZStats.elpd_estimates  —  Function elpd_estimates(result::AbstractELPDResult; pointwise=false) -> (; elpd, elpd_mcse, lpd) Return the (E)LPD estimates from the  result . source"},{"id":83,"pagetitle":"Stats","title":"ArviZ.ArviZStats.information_criterion","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.information_criterion","content":" ArviZ.ArviZStats.information_criterion  —  Function information_criterion(elpd, scale::Symbol) Compute the information criterion for the given  scale  from the  elpd  estimate. scale  must be one of (:deviance, :log, :negative_log). See also:  effective_number_of_parameters ,  loo ,  waic source information_criterion(result::AbstractELPDResult, scale::Symbol; pointwise=false) Compute information criterion for the given  scale  from the existing ELPD  result . scale  must be one of (:deviance, :log, :negative_log). If  pointwise=true , then pointwise estimates are returned. source"},{"id":84,"pagetitle":"Stats","title":"ArviZ.ArviZStats.loo","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.loo","content":" ArviZ.ArviZStats.loo  —  Function loo(log_likelihood; reff=nothing, kwargs...) -> PSISLOOResult{<:NamedTuple,<:NamedTuple} Compute the Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO).  [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . Keywords reff::Union{Real,AbstractArray{<:Real}} : The relative effective sample size(s) of the  likelihood  values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using  ess . kwargs : Remaining keywords are forwarded to  psis . See also:  PSISLOOResult ,  waic Examples Manually compute  $R_\\mathrm{eff}$  and calculate PSIS-LOO of a model: julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> reff = ess(log_like; kind=:basic, split_chains=1, relative=true);\n\njulia> loo(log_like; reff)\nPSISLOOResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.34\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  7 (87.5%)  151\n  (0.5, 0.7]  okay  1 (12.5%)  446 source loo(data::Dataset; [var_name::Symbol,] kwargs...) -> PSISLOOResult{<:NamedTuple,<:Dataset}\nloo(data::InferenceData; [var_name::Symbol,] kwargs...) -> PSISLOOResult{<:NamedTuple,<:Dataset} Compute PSIS-LOO from log-likelihood values in  data . If more than one log-likelihood variable is present, then  var_name  must be provided. Examples Calculate PSIS-LOO of a model: julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo(idata)\nPSISLOOResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.34\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  6 (75.0%)  135\n  (0.5, 0.7]  okay  2 (25.0%)  421 source"},{"id":85,"pagetitle":"Stats","title":"ArviZ.ArviZStats.waic","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.waic","content":" ArviZ.ArviZStats.waic  —  Function waic(log_likelihood::AbstractArray) -> WAICResult{<:NamedTuple,<:NamedTuple} Compute the widely applicable information criterion (WAIC). [Watanabe2010] [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . See also:  WAICResult ,  loo Examples Calculate WAIC of a model: julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> waic(log_like)\nWAICResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.33 source waic(data::Dataset; [var_name::Symbol]) -> WAICResult{<:NamedTuple,<:Dataset}\nwaic(data::InferenceData; [var_name::Symbol]) -> WAICResult{<:NamedTuple,<:Dataset} Compute WAIC from log-likelihood values in  data . If more than one log-likelihood variable is present, then  var_name  must be provided. Examples Calculate WAIC of a model: julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> waic(idata)\nWAICResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.33 source"},{"id":86,"pagetitle":"Stats","title":"Model comparison","ref":"/ArviZ/stable/api/stats/#Model-comparison","content":" Model comparison"},{"id":87,"pagetitle":"Stats","title":"ArviZ.ArviZStats.ModelComparisonResult","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.ModelComparisonResult","content":" ArviZ.ArviZStats.ModelComparisonResult  —  Type ModelComparisonResult Result of model comparison using ELPD. This struct implements the Tables and TableTraits interfaces. Each field returns a collection of the corresponding entry for each model: name : Names of the models, if provided. rank : Ranks of the models (ordered by decreasing ELPD) elpd_diff : ELPD of a model subtracted from the largest ELPD of any model elpd_diff_mcse : Monte Carlo standard error of the ELPD difference weight : Model weights computed with  weights_method elpd_result :  AbstactELPDResult s for each model, which can be used to access useful stats like ELPD estimates, pointwise estimates, and Pareto shape values for PSIS-LOO weights_method : Method used to compute model weights with  model_weights source"},{"id":88,"pagetitle":"Stats","title":"ArviZ.ArviZStats.compare","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.compare","content":" ArviZ.ArviZStats.compare  —  Function compare(models; kwargs...) Compare models based on their expected log pointwise predictive density (ELPD). models  is a  Tuple ,  NamedTuple , or  AbstractVector  whose values are either  AbstractELPDResult  entries or any argument to  elpd_method , which must produce an  AbstractELPDResult . The weights are returned in the same type of collection. The argument may be any object with a  pairs  method where each value is either an  InferenceData  or an  AbstractELPDResult . The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353 Arguments models : a  Tuple ,  NamedTuple , or  AbstractVector  whose values are either  AbstractELPDResult  entries or any argument to  elpd_method . Keywords weights_method::AbstractModelWeightsMethod=Stacking() : the method to be used to weight the models. See  model_weights  for details elpd_method=loo : a method that computes an  AbstractELPDResult  from an argument in  models . sort::Bool=true : Whether to sort models by decreasing ELPD. Returns ModelComparisonResult : A container for the model comparison results. Examples Compare the centered and non centered models of the eight school problem using the defaults:  loo  and  Stacking  weights: julia> using ArviZ, ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> mc = compare(models)\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\nModelComparisonResult with Stacking weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0       1.0  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067     0.0  0.9   ⋯\n                                                                1 column omitted Compare the same models from pre-computed PSIS-LOO results and computing  BootstrappedPseudoBMA  weights: julia> elpd_results = mc.elpd_result;\n\njulia> compare(elpd_results; weights_method=BootstrappedPseudoBMA())\nModelComparisonResult with BootstrappedPseudoBMA weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0      0.52  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067    0.48  0.9   ⋯\n                                                                1 column omitted source"},{"id":89,"pagetitle":"Stats","title":"ArviZ.ArviZStats.model_weights","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.model_weights","content":" ArviZ.ArviZStats.model_weights  —  Function model_weights(elpd_results; method=Stacking())\nmodel_weights(method::AbstractModelWeightsMethod, elpd_results) Compute weights for each model in  elpd_results  using  method . elpd_results  is a  Tuple ,  NamedTuple , or  AbstractVector  with  AbstractELPDResult  entries. The weights are returned in the same type of collection. Stacking  is the recommended approach, as it performs well even when the true data generating process is not included among the candidate models. See  [YaoVehtari2018]  for details. See also:  AbstractModelWeightsMethod ,  compare Examples Compute  Stacking  weights for two models: julia> using ArviZ, ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> elpd_results = map(loo, models);\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\n\njulia> model_weights(elpd_results; method=Stacking()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 5.34175e-19\n  :non_centered => 1.0 Now we compute  BootstrappedPseudoBMA  weights for the same models: julia> model_weights(elpd_results; method=BootstrappedPseudoBMA()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 0.483723\n  :non_centered => 0.516277 source The following model weighting methods are available"},{"id":90,"pagetitle":"Stats","title":"ArviZ.ArviZStats.AbstractModelWeightsMethod","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.AbstractModelWeightsMethod","content":" ArviZ.ArviZStats.AbstractModelWeightsMethod  —  Type abstract type AbstractModelWeightsMethod An abstract type representing methods for computing model weights. Subtypes implement  model_weights (method, elpd_results) . source"},{"id":91,"pagetitle":"Stats","title":"ArviZ.ArviZStats.BootstrappedPseudoBMA","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.BootstrappedPseudoBMA","content":" ArviZ.ArviZStats.BootstrappedPseudoBMA  —  Type struct BootstrappedPseudoBMA{R<:Random.AbstractRNG, T<:Real} <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging using Akaike-type weighting with the Bayesian bootstrap (pseudo-BMA+) [YaoVehtari2018] . The Bayesian bootstrap stabilizes the model weights. BootstrappedPseudoBMA(; rng=Random.default_rng(), samples=1_000, alpha=1)\nBootstrappedPseudoBMA(rng, samples, alpha) Construct the method. rng::Random.AbstractRNG : The random number generator to use for the Bayesian bootstrap samples::Int64 : The number of samples to draw for bootstrapping alpha::Real : The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. The default (1) corresponds to a uniform distribution on the simplex. See also:  Stacking source"},{"id":92,"pagetitle":"Stats","title":"ArviZ.ArviZStats.PseudoBMA","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.PseudoBMA","content":" ArviZ.ArviZStats.PseudoBMA  —  Type struct PseudoBMA <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging (pseudo-BMA) and Akaike-type weighting. PseudoBMA(; regularize=false)\nPseudoBMA(regularize) Construct the method with optional regularization of the weights using the standard error of the ELPD estimate. Note This approach is not recommended, as it produces unstable weight estimates. It is recommended to instead use  BootstrappedPseudoBMA  to stabilize the weights or  Stacking . For details, see  [YaoVehtari2018] . See also:  Stacking source"},{"id":93,"pagetitle":"Stats","title":"ArviZ.ArviZStats.Stacking","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.Stacking","content":" ArviZ.ArviZStats.Stacking  —  Type struct Stacking{O<:Optim.AbstractOptimizer} <: AbstractModelWeightsMethod Model weighting using stacking of predictive distributions [YaoVehtari2018] . Stacking(; optimizer=Optim.LBFGS(), options=Optim.Options()\nStacking(optimizer[, options]) Construct the method, optionally customizing the optimization. optimizer::Optim.AbstractOptimizer : The optimizer to use for the optimization of the weights. The optimizer must support projected gradient optimization viae a  manifold  field. options::Optim.Options : The Optim options to use for the optimization of the weights. See also:  BootstrappedPseudoBMA source"},{"id":94,"pagetitle":"Stats","title":"Predictive checks","ref":"/ArviZ/stable/api/stats/#Predictive-checks","content":" Predictive checks"},{"id":95,"pagetitle":"Stats","title":"ArviZ.ArviZStats.loo_pit","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.loo_pit","content":" ArviZ.ArviZStats.loo_pit  —  Function loo_pit(y, y_pred, log_weights; kwargs...) -> Union{Real,AbstractArray} Compute leave-one-out probability integral transform (LOO-PIT) checks. Arguments y : array of observations with shape  (params...,) y_pred : array of posterior predictive samples with shape  (draws, chains, params...) . log_weights : array of normalized log LOO importance weights with shape  (draws, chains, params...) . Keywords is_discrete : If not provided, then it is set to  true  iff elements of  y  and  y_pred  are all integer-valued. If  true , then data are smoothed using  smooth_data  to make them non-discrete before estimating LOO-PIT values. kwargs : Remaining keywords are forwarded to  smooth_data  if data is discrete. Returns pitvals : LOO-PIT values with same size as  y . If  y  is a scalar, then  pitvals  is a scalar. LOO-PIT is a marginal posterior predictive check. If  $y_{-i}$  is the array  $y$  of observations with the  $i$ th observation left out, and  $y_i^*$  is a posterior prediction of the  $i$ th observation, then the LOO-PIT value for the  $i$ th observation is defined as \\[P(y_i^* \\le y_i \\mid y_{-i}) = \\int_{-\\infty}^{y_i} p(y_i^* \\mid y_{-i}) \\mathrm{d} y_i^*\\] The LOO posterior predictions and the corresponding observations should have similar distributions, so if conditional predictive distributions are well-calibrated, then all LOO-PIT values should be approximately uniformly distributed on  $[0, 1]$ . [Gabry2019] Examples Calculate LOO-PIT values using as test quantity the observed values themselves. julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_weights = loo(idata; var_name=:obs).psis_result.log_weights;\n\njulia> loo_pit(\n            idata.observed_data.obs,\n            permutedims(idata.posterior_predictive.obs, (:draw, :chain, :school)),\n            log_weights,\n        )\n8-element DimArray{Float64,1} with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 Calculate LOO-PIT values using as test quantity the square of the difference between each observation and  mu . julia> using DimensionalData, Statistics\n\njulia> T = idata.observed_data.obs .- only(median(idata.posterior.mu; dims=(:draw, :chain)));\n\njulia> T_pred = permutedims(\n           broadcast_dims(-, idata.posterior_predictive.obs, idata.posterior.mu),\n           (:draw, :chain, :school),\n       );\n\njulia> loo_pit(T .^ 2, T_pred .^ 2, log_weights)\n8-element DimArray{Float64,1} with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.873577\n \"Deerfield\"         0.243686\n \"Phillips Andover\"  0.357563\n \"Phillips Exeter\"   0.149908\n \"Hotchkiss\"         0.435094\n \"Lawrenceville\"     0.220627\n \"St. Paul's\"        0.775086\n \"Mt. Hermon\"        0.296706 source loo_pit(idata::InferenceData, log_weights; kwargs...) -> DimArray Compute LOO-PIT values using existing normalized log LOO importance weights. Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. kwargs : Remaining keywords are forwarded to  loo_pit . Examples Calculate LOO-PIT values using already computed log weights. julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo_result = loo(idata; var_name=:obs);\n\njulia> loo_pit(idata, loo_result.psis_result.log_weights; y_name=:obs)\n8-element DimArray{Float64,1} loo_pit_obs with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 source loo_pit(idata::InferenceData; kwargs...) -> DimArray Compute LOO-PIT from groups in  idata  using PSIS-LOO. See also:  loo ,  psis Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. log_likelihood_name : Name of log-likelihood variable in  idata.log_likelihood . If not provided, then  y_name  is used if  idata  has a  log_likelihood  group, otherwise the only variable is used. reff::Union{Real,AbstractArray{<:Real}} : The relative effective sample size(s) of the  likelihood  values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using  ess . kwargs : Remaining keywords are forwarded to  loo_pit . Examples Calculate LOO-PIT values using as test quantity the observed values themselves. julia> using ArviZ, ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo_pit(idata; y_name=:obs)\n8-element DimArray{Float64,1} loo_pit_obs with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 source"},{"id":96,"pagetitle":"Stats","title":"Utilities","ref":"/ArviZ/stable/api/stats/#Utilities","content":" Utilities"},{"id":97,"pagetitle":"Stats","title":"ArviZ.ArviZStats.smooth_data","ref":"/ArviZ/stable/api/stats/#ArviZ.ArviZStats.smooth_data","content":" ArviZ.ArviZStats.smooth_data  —  Function smooth_data(y; dims=:, interp_method=CubicSpline, offset_frac=0.01) Smooth  y  along  dims  using  interp_method . interp_method  is a 2-argument callabale that takes the arguments  y  and  x  and returns a DataInterpolations.jl interpolation method, defaulting to a cubic spline interpolator. offset_frac  is the fraction of the length of  y  to use as an offset when interpolating. source Hyndman1996 Rob J. Hyndman (1996) Computing and Graphing Highest Density Regions,             Amer. Stat., 50(2): 120-6.             DOI:  10.1080/00031305.1996.10474359 jstor . ChenShao1999 Ming-Hui Chen & Qi-Man Shao (1999)              Monte Carlo Estimation of Bayesian Credible and HPD Intervals,              J Comput. Graph. Stat., 8:1, 69-92.              DOI:  10.1080/10618600.1999.10474802 jstor . GelmanGoodrich2019 Andrew Gelman, Ben Goodrich, Jonah Gabry & Aki Vehtari (2019) R-squared for Bayesian Regression Models, The American Statistician, 73:3, 307-9, DOI:  10.1080/00031305.2018.1549100 . VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html Watanabe2010 Watanabe, S. Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. 11(116):3571−3594, 2010. https://jmlr.csail.mit.edu/papers/v11/watanabe10a.html Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using Stacking to Average Bayesian Predictive Distributions. Bayesian Analysis. 13, 3, 917–1007. doi:  10.1214/17-BA1091  arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using Stacking to Average Bayesian Predictive Distributions. Bayesian Analysis. 13, 3, 917–1007. doi:  10.1214/17-BA1091  arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using Stacking to Average Bayesian Predictive Distributions. Bayesian Analysis. 13, 3, 917–1007. doi:  10.1214/17-BA1091  arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using Stacking to Average Bayesian Predictive Distributions. Bayesian Analysis. 13, 3, 917–1007. doi:  10.1214/17-BA1091  arXiv:  1704.02030 Gabry2019 Gabry, J., Simpson, D., Vehtari, A., Betancourt, M. & Gelman, A. Visualization in Bayesian Workflow. J. R. Stat. Soc. Ser. A Stat. Soc. 182, 389–402 (2019). doi:  10.1111/rssa.12378  arXiv:  1709.01449"},{"id":100,"pagetitle":"Creating custom plots","title":"Environment","ref":"/ArviZ/stable/creating_custom_plots/#Environment","content":" Environment using Pkg, InteractiveUtils using PlutoUI with_terminal(Pkg.status; color=false) Status `~/work/ArviZ.jl/ArviZ.jl/docs/Project.toml`\n  [cbdf2221] AlgebraOfGraphics v0.6.16\n  [131c737c] ArviZ v0.9.2 `~/work/ArviZ.jl/ArviZ.jl`\n  [2f96bb34] ArviZExampleData v0.1.5\n  [13f3f980] CairoMakie v0.10.7\n  [a93c6f00] DataFrames v1.6.1\n  [0703355e] DimensionalData v0.24.13\n  [31c24e10] Distributions v0.25.98\n  [e30172f5] Documenter v0.27.25\n  [f6006082] EvoTrees v0.15.1\n  [c7f686f2] MCMCChains v6.0.3\n  [be115224] MCMCDiagnosticTools v0.3.4\n  [a7f614a8] MLJBase v0.21.13\n  [614be32b] MLJIteration v0.5.1\n  [359b1769] PlutoStaticHTML v6.0.14\n  [7f904dfe] PlutoUI v0.7.52\n  [754583d1] SampleChains v0.5.1\n  [f43a241f] Downloads v1.6.0\n  [37e2e46d] LinearAlgebra\n  [10745b16] Statistics v1.9.0\n with_terminal(versioninfo) Julia Version 1.9.2\nCommit e4ee485e909 (2023-07-05 09:39 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 2 × Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, skylake-avx512)\n  Threads: 2 on 2 virtual cores\nEnvironment:\n  JULIA_PROJECT = /home/runner/work/ArviZ.jl/ArviZ.jl/docs/\n  JULIA_DEPOT_PATH = /home/runner/.julia:/opt/hostedtoolcache/julia/1.9.2/x64/local/share/julia:/opt/hostedtoolcache/julia/1.9.2/x64/share/julia\n  JULIA_LOAD_PATH = @:@v#.#:@stdlib\n  JULIA_REVISE_WORKER_ONLY = 1\n  JULIA_IMAGE_THREADS = 1\n"},{"id":105,"pagetitle":"Working with InferenceData","title":"Working with  InferenceData","ref":"/ArviZ/stable/working_with_inference_data/#working-with-inference-data","content":" Working with  InferenceData using ArviZ, ArviZExampleData, DimensionalData, Statistics Here we present a collection of common manipulations you can use while working with  InferenceData . Let's load one of ArviZ's example datasets.  posterior ,  posterior_predictive , etc are the groups stored in  idata , and they are stored as  Dataset s. In this HTML view, you can click a group name to expand a summary of the group. idata = load_example_data(\"centered_eight\") InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" Info Dataset s are  DimensionalData.AbstractDimStack s and can be used identically.   The variables a  Dataset  contains are called \"layers\", and dimensions of the same name that appear in more than one layer within a  Dataset  must have the same indices. InferenceData  behaves like a  NamedTuple  and can be used similarly. Note that unlike a  NamedTuple , the groups always appear in a specific order. length(idata) # number of groups 8 keys(idata) # group names (:posterior, :posterior_predictive, :log_likelihood, :sample_stats, :prior, :prior_predictive, :observed_data, :constant_data)"},{"id":106,"pagetitle":"Working with InferenceData","title":"Get the dataset corresponding to a single group","ref":"/ArviZ/stable/working_with_inference_data/#Get-the-dataset-corresponding-to-a-single-group","content":" Get the dataset corresponding to a single group Group datasets can be accessed both as properties or as indexed items. post = idata.posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\" post  is the dataset itself, so this is a non-allocating operation. idata[:posterior] === post true InferenceData  supports a more advanced indexing syntax, which we'll see later."},{"id":107,"pagetitle":"Working with InferenceData","title":"Getting a new  InferenceData  with a subset of groups","ref":"/ArviZ/stable/working_with_inference_data/#Getting-a-new-InferenceData-with-a-subset-of-groups","content":" Getting a new  InferenceData  with a subset of groups We can index by a collection of group names to get a new  InferenceData  with just those groups. This is also non-allocating. idata_sub = idata[(:posterior, :posterior_predictive)] InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\""},{"id":108,"pagetitle":"Working with InferenceData","title":"Adding groups to an  InferenceData","ref":"/ArviZ/stable/working_with_inference_data/#Adding-groups-to-an-InferenceData","content":" Adding groups to an  InferenceData InferenceData  is immutable, so to add or replace groups we use  merge  to create a new object. merge(idata_sub, idata[(:observed_data, :prior)]) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" We can also use  Base.setindex  to out-of-place add or replace a single group. Base.setindex(idata_sub, idata.prior, :prior) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\""},{"id":109,"pagetitle":"Working with InferenceData","title":"Add a new variable","ref":"/ArviZ/stable/working_with_inference_data/#Add-a-new-variable","content":" Add a new variable Dataset  is also immutable. So while the values within the underlying data arrays can be mutated, layers cannot be added or removed from  Dataset s, and groups cannot be added/removed from  InferenceData . Instead, we do this out-of-place also using  merge . merge(post, (log_tau=log.(post[:tau]),)) Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 4 layers:\n  :mu      Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta   Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :log_tau Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\""},{"id":110,"pagetitle":"Working with InferenceData","title":"Obtain an array for a given parameter","ref":"/ArviZ/stable/working_with_inference_data/#Obtain-an-array-for-a-given-parameter","content":" Obtain an array for a given parameter Let’s say we want to get the values for  mu  as an array. Parameters can be accessed with either property or index syntax. post.tau 500×4 DimArray{Float64,2} tau with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\n      0        1         2        3\n   0  4.72574  1.97083   3.50128  6.07326\n   1  3.90899  2.04903   2.89324  3.77187\n   2  4.84403  2.12376   4.27329  3.17054\n   3  1.8567   3.39183  11.8965   6.00193\n   4  4.74841  4.84368   7.11325  3.28632\n   ⋮                              \n 494  8.15827  1.61268   4.96249  3.13966\n 495  7.56498  1.61268   3.56495  2.78607\n 496  2.24702  1.84816   2.55959  4.28196\n 497  1.89384  2.17459   4.08978  2.74061\n 498  5.92006  1.32755   2.72017  2.93238\n 499  4.3259   1.21199   1.91701  4.46125 post[:tau] === post.tau true To remove the dimensions, just use  parent  to retrieve the underlying array. parent(post.tau) 500×4 Matrix{Float64}:\n 4.72574   1.97083   3.50128  6.07326\n 3.90899   2.04903   2.89324  3.77187\n 4.84403   2.12376   4.27329  3.17054\n 1.8567    3.39183  11.8965   6.00193\n 4.74841   4.84368   7.11325  3.28632\n 3.51387  10.8872    7.18892  2.16314\n 4.20898   4.01889   9.0977   7.68505\n 2.6834    4.28584   7.84286  4.08612\n 1.16889   3.70403  17.1548   5.1157\n 1.21052   3.15829  16.7573   4.86939\n ⋮                            \n 2.05742   1.09087  10.8168   5.08507\n 2.72536   1.09087   2.16788  6.1552\n 5.97049   1.67101   5.19169  8.23756\n 8.15827   1.61268   4.96249  3.13966\n 7.56498   1.61268   3.56495  2.78607\n 2.24702   1.84816   2.55959  4.28196\n 1.89384   2.17459   4.08978  2.74061\n 5.92006   1.32755   2.72017  2.93238\n 4.3259    1.21199   1.91701  4.46125"},{"id":111,"pagetitle":"Working with InferenceData","title":"Get the dimension lengths","ref":"/ArviZ/stable/working_with_inference_data/#Get-the-dimension-lengths","content":" Get the dimension lengths Let’s check how many groups are in our hierarchical model. size(idata.observed_data, :school) 8"},{"id":112,"pagetitle":"Working with InferenceData","title":"Get coordinate/index values","ref":"/ArviZ/stable/working_with_inference_data/#Get-coordinate/index-values","content":" Get coordinate/index values What are the names of the groups in our hierarchical model? You can access them from the coordinate name  school  in this case. DimensionalData.index(idata.observed_data, :school) 8-element Vector{String}:\n \"Choate\"\n \"Deerfield\"\n \"Phillips Andover\"\n \"Phillips Exeter\"\n \"Hotchkiss\"\n \"Lawrenceville\"\n \"St. Paul's\"\n \"Mt. Hermon\""},{"id":113,"pagetitle":"Working with InferenceData","title":"Get a subset of chains","ref":"/ArviZ/stable/working_with_inference_data/#Get-a-subset-of-chains","content":" Get a subset of chains Let’s keep only chain 0 here. For the subset to take effect on all relevant  InferenceData  groups –  posterior ,  sample_stats ,  log_likelihood , and  posterior_predictive  – we will index  InferenceData  instead of  Dataset . Here we use DimensionalData's  At  selector. Its  other selectors  are also supported. idata[chain=At(0)] InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×1)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×1)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×1)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" Note that in this case,  prior  only has a chain of 0. If it also had the other chains, we could have passed  chain=At([0, 2])  to subset by chains 0 and 2. Warning If we used  idata[chain=[0, 2]]  without the  At  selector, this is equivalent to  idata[chain=DimensionalData.index(idata.posterior, :chain)[0, 2]] , that is,  [0, 2]  indexes an array of dimension indices, which here would error.   But if we had requested  idata[chain=[1, 2]]  we would not hit an error, but we would index the wrong chains.   So it's important to always use a selector to index by values of dimension indices."},{"id":114,"pagetitle":"Working with InferenceData","title":"Remove the first  $n$  draws (burn-in)","ref":"/ArviZ/stable/working_with_inference_data/#Remove-the-first-n-draws-(burn-in)","content":" Remove the first  $n$  draws (burn-in) Let’s say we want to remove the first 100 draws from all the chains and all  InferenceData  groups with draws. To do this we use the  ..  syntax from IntervalSets.jl, which is exported by DimensionalData. idata[draw=100 .. Inf] InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (400×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (400×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (400×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (400×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (400×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" If you check the object you will see that the groups  posterior ,  posterior_predictive ,  prior , and  sample_stats  have 400 draws compared to  idata , which has 500. The group  observed_data  has not been affected because it does not have the  draw  dimension. Alternatively, you can change a subset of groups by combining indexing styles with  merge . Here we use this to build a new  InferenceData  where we have discarded the first 100 draws only from  posterior . merge(idata, idata[(:posterior,), draw=100 .. Inf]) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\""},{"id":115,"pagetitle":"Working with InferenceData","title":"Compute posterior mean values along draw and chain dimensions","ref":"/ArviZ/stable/working_with_inference_data/#Compute-posterior-mean-values-along-draw-and-chain-dimensions","content":" Compute posterior mean values along draw and chain dimensions To compute the mean value of the posterior samples, do the following: mean(post) (mu = 4.485933103402338,\n theta = 4.911515591394205,\n tau = 4.124222787491913,) This computes the mean along all dimensions, discarding all dimensions and returning the result as a  NamedTuple . This may be what you wanted for  mu  and  tau , which have only two dimensions ( chain  and  draw ), but maybe not what you expected for  theta , which has one more dimension  school . You can specify along which dimension you want to compute the mean (or other functions), which instead returns a  Dataset . mean(post; dims=(:chain, :draw)) Dataset with dimensions: \n  Dim{:draw} Sampled{Float64} Float64[249.5] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Float64} Float64[1.5] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (1×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×1×1)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (1×1)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\" The singleton dimensions of  chain  and  draw  now contain meaningless indices, so you may want to discard them, which you can do with  dropdims . dropdims(mean(post; dims=(:chain, :draw)); dims=(:chain, :draw)) Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: \n  :theta Float64 dims: Dim{:school} (8)\n  :tau   Float64 dims: \n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\""},{"id":116,"pagetitle":"Working with InferenceData","title":"Renaming a dimension","ref":"/ArviZ/stable/working_with_inference_data/#Renaming-a-dimension","content":" Renaming a dimension We can rename a dimension in a  Dataset  using DimensionalData's  set  method: theta_bis = set(post.theta; school=:school_bis) 8×500×4 DimArray{Float64,3} theta with dimensions: \n  Dim{:school_bis} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\n[:, :, 1]\n                       0        …  497         498        499\n  \"Choate\"            12.3207       -0.213828   10.4025     6.66131\n  \"Deerfield\"          9.90537       1.35515     6.90741    7.41377\n  \"Phillips Andover\"  14.9516        6.98269    -4.96414   -9.3226\n  \"Phillips Exeter\"   11.0115        3.71681     3.13584    2.69192\n  \"Hotchkiss\"          5.5796   …    5.32446    -2.2243    -0.502331\n  \"Lawrenceville\"     16.9018        6.96589    -2.83504   -4.25487\n  \"St. Paul's\"        13.1981        4.9302      5.39106    7.56657\n  \"Mt. Hermon\"        15.0614        3.0586      6.38124    9.98762\n[and 3 more slices...] We can use this, for example, to broadcast functions across multiple arrays, automatically matching up shared dimensions, using  DimensionalData.broadcast_dims . theta_school_diff = broadcast_dims(-, post.theta, theta_bis) 8×500×4×8 DimArray{Float64,4} theta with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school_bis} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n[:, :, 1, 1]\n                       0         …  497        498        499\n  \"Choate\"             0.0            0.0        0.0        0.0\n  \"Deerfield\"         -2.41532        1.56898   -3.49509    0.752459\n  \"Phillips Andover\"   2.63093        7.19652  -15.3666   -15.9839\n  \"Phillips Exeter\"   -1.3092         3.93064   -7.26666   -3.96939\n  \"Hotchkiss\"         -6.74108   …    5.53829  -12.6268    -7.16364\n  \"Lawrenceville\"      4.58111        7.17972  -13.2375   -10.9162\n  \"St. Paul's\"         0.877374       5.14403   -5.01144    0.905263\n  \"Mt. Hermon\"         2.74068        3.27243   -4.02126    3.32631\n[and 31 more slices...]"},{"id":117,"pagetitle":"Working with InferenceData","title":"Compute and store posterior pushforward quantities","ref":"/ArviZ/stable/working_with_inference_data/#Compute-and-store-posterior-pushforward-quantities","content":" Compute and store posterior pushforward quantities We use “posterior pushfoward quantities” to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables. You can compute these pushforward operations and store them as a new variable in a copy of the posterior group. Here we'll create a new  InferenceData  with  theta_school_diff  in the posterior: idata_new = Base.setindex(idata, merge(post, (; theta_school_diff)), :posterior) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:school_bis} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 4 layers:\n  :mu                Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta             Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau               Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta_school_diff Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain}, Dim{:school_bis} (8×500×4×8)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" Once you have these pushforward quantities in an  InferenceData , you’ll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them, or save and share the  InferenceData  object with the pushforward quantities included. Here we compute the  mcse  of  theta_school_diff : mcse(idata_new.posterior).theta_school_diff 8×8 DimArray{Float64,2} theta_school_diff with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:school_bis} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n                         \"Choate\"  …     \"St. Paul's\"     \"Mt. Hermon\"\n  \"Choate\"            NaN               0.117476         0.219695\n  \"Deerfield\"           0.191463        0.16484          0.189386\n  \"Phillips Andover\"    0.255636        0.258001         0.160477\n  \"Phillips Exeter\"     0.162782        0.156724         0.144923\n  \"Hotchkiss\"           0.282881   …    0.283969         0.189015\n  \"Lawrenceville\"       0.259065        0.251988         0.178094\n  \"St. Paul's\"          0.117476      NaN                0.222054\n  \"Mt. Hermon\"          0.219695        0.222054       NaN"},{"id":118,"pagetitle":"Working with InferenceData","title":"Advanced subsetting","ref":"/ArviZ/stable/working_with_inference_data/#Advanced-subsetting","content":" Advanced subsetting To select the value corresponding to the difference between the Choate and Deerfield schools do: school_idx = [\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"]\nschool_bis_idx = [\"Deerfield\", \"Choate\", \"Lawrenceville\"]\ntheta_school_diff[school=At(school_idx), school_bis=At(school_bis_idx)] 3×500×4×3 DimArray{Float64,4} theta with dimensions: \n  Dim{:school} Categorical{String} String[\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school_bis} Categorical{String} String[\"Deerfield\", \"Choate\", \"Lawrenceville\"] Unordered\n[:, :, 1, 1]\n                 0         1        …  497        498         499\n  \"Choate\"       2.41532   2.1563       -1.56898    3.49509    -0.752459\n  \"Hotchkiss\"   -4.32577  -1.31781       3.96931   -9.13171    -7.9161\n  \"Mt. Hermon\"   5.156    -2.9526        1.70345   -0.526168    2.57385\n[and 11 more slices...]"},{"id":119,"pagetitle":"Working with InferenceData","title":"Add new chains using  cat","ref":"/ArviZ/stable/working_with_inference_data/#Add-new-chains-using-cat","content":" Add new chains using  cat Suppose after checking the  mcse  and realizing you need more samples, you rerun the model with two chains and obtain an  idata_rerun  object. idata_rerun = InferenceData(; posterior=set(post[chain=At([0, 1])]; chain=[4, 5])) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[4, 5] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×2)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×2)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×2)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" You can combine the two using  cat . cat(idata[[:posterior]], idata_rerun; dims=:chain) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, …, 4, 5] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×6)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×6)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×6)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\""},{"id":122,"pagetitle":"Home","title":"PSIS","ref":"/PSIS/stable/#PSIS","content":" PSIS PSIS.jl implements the Pareto smoothed importance sampling (PSIS) algorithm from  [VehtariSimpson2021] . Given a set of importance weights used in some estimator, PSIS both improves the reliability of the estimates by smoothing the importance weights and acts as a diagnostic of the reliability of the estimates. See  psis  for details."},{"id":123,"pagetitle":"Home","title":"Example","ref":"/PSIS/stable/#Example","content":" Example In this example, we use PSIS to smooth log importance ratios for importance sampling 30 isotropic Student  $t$ -distributed parameters using standard normal distributions as proposals. using PSIS, Distributions\nproposal = Normal()\ntarget = TDist(7)\nndraws, nchains, nparams = (1_000, 1, 30)\nx = rand(proposal, ndraws, nchains, nparams)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) ┌ Warning: 8 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/work/PSIS.jl/PSIS.jl/src/core.jl:323\n┌ Warning: 1 parameters had Pareto shape values k > 1. Corresponding importance sampling estimates are likely to be unstable and are unlikely to converge with additional samples.\n└ @ PSIS ~/work/PSIS.jl/PSIS.jl/src/core.jl:326 PSISResult with 1000 draws, 1 chains, and 30 parameters\nPareto shape (k) diagnostic values:\n                        Count       Min. ESS\n (-Inf, 0.5]  good       7 (23.3%)  959\n  (0.5, 0.7]  okay      14 (46.7%)  927\n    (0.7, 1]  bad        8 (26.7%)  ——\n    (1, Inf)  very bad   1 (3.3%)   —— As indicated by the warnings, this is a poor choice of a proposal distribution, and estimates are unlikely to converge (see  PSISResult  for an explanation of the shape thresholds). When running PSIS with many parameters, it is useful to plot the Pareto shape values to diagnose convergence. See  Plotting PSIS results  for examples. VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO]"},{"id":126,"pagetitle":"API","title":"API","ref":"/PSIS/stable/api/#API","content":" API"},{"id":127,"pagetitle":"API","title":"Core functionality","ref":"/PSIS/stable/api/#Core-functionality","content":" Core functionality"},{"id":128,"pagetitle":"API","title":"PSIS.PSISResult","ref":"/PSIS/stable/api/#PSIS.PSISResult","content":" PSIS.PSISResult  —  Type PSISResult Result of Pareto-smoothed importance sampling (PSIS) using  psis . Properties log_weights : un-normalized Pareto-smoothed log weights weights : normalized Pareto-smoothed weights (allocates a copy) pareto_shape : Pareto  $k=ξ$  shape parameter nparams : number of parameters in  log_weights ndraws : number of draws in  log_weights nchains : number of chains in  log_weights reff : the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size. ess : estimated effective sample size of estimate of mean using smoothed importance samples (see  ess_is ) tail_length : length of the upper tail of  log_weights  that was smoothed tail_dist : the generalized Pareto distribution that was fit to the tail of  log_weights . Note that the tail weights are scaled to have a maximum of 1, so  tail_dist * exp(maximum(log_ratios))  is the corresponding fit directly to the tail of  log_ratios . normalized::Bool :indicates whether  log_weights  are log-normalized along the sample dimensions. Diagnostic The  pareto_shape  parameter  $k=ξ$  of the generalized Pareto distribution  tail_dist  can be used to diagnose reliability and convergence of estimates using the importance weights  [VehtariSimpson2021] . if  $k < \\frac{1}{3}$ , importance sampling is stable, and importance sampling (IS) and PSIS both are reliable. if  $k ≤ \\frac{1}{2}$ , then the importance ratio distributon has finite variance, and the central limit theorem holds. As  $k$  approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE. if  $\\frac{1}{2} < k ≤ 0.7$ , then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime. if  $0.7 < k ≤ 1$ , then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended. if  $k > 1$ , then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes. See  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":129,"pagetitle":"API","title":"PSIS.psis","ref":"/PSIS/stable/api/#PSIS.psis","content":" PSIS.psis  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":130,"pagetitle":"API","title":"PSIS.ess_is","ref":"/PSIS/stable/api/#PSIS.ess_is","content":" PSIS.ess_is  —  Function ess_is(weights; reff=1) Estimate effective sample size (ESS) for importance sampling over the sample dimensions. Given normalized weights  $w_{1:n}$ , the ESS is estimated using the L2-norm of the weights: \\[\\mathrm{ESS}(w_{1:n}) = \\frac{r_{\\mathrm{eff}}}{\\sum_{i=1}^n w_i^2}\\] where  $r_{\\mathrm{eff}}$  is the relative efficiency of the  log_weights . ess_is(result::PSISResult; bad_shape_nan=true) Estimate ESS for Pareto-smoothed importance sampling. Note ESS estimates for Pareto shape values  $k > 0.7$ , which are unreliable and misleadingly high, are set to  NaN . To avoid this, set  bad_shape_nan=false . source"},{"id":131,"pagetitle":"API","title":"Plotting","ref":"/PSIS/stable/api/#Plotting","content":" Plotting"},{"id":132,"pagetitle":"API","title":"PSIS.PSISPlots","ref":"/PSIS/stable/api/#PSIS.PSISPlots","content":" PSIS.PSISPlots  —  Module A module defining  paretoshapeplot  for plotting Pareto shape values with Plots.jl source"},{"id":133,"pagetitle":"API","title":"PSIS.PSISPlots.paretoshapeplot","ref":"/PSIS/stable/api/#PSIS.PSISPlots.paretoshapeplot","content":" PSIS.PSISPlots.paretoshapeplot  —  Function paretoshapeplot(values; showlines=false, ...)\nparetoshapeplot!(values; showlines=false, kwargs...) Plot shape parameters of fitted Pareto tail distributions for diagnosing convergence. values  may be either a vector of Pareto shape parameters or a  PSIS.PSISResult . If  showlines==true , horizontal lines indicating relevant Pareto shape thresholds are drawn. See  PSIS.PSISResult  for an explanation of the thresholds. All remaining  kwargs  are forwarded to the plotting function. See  psis ,  PSISResult . Examples using PSIS, Distributions, Plots\nproposal = Normal()\ntarget = TDist(7)\nx = rand(proposal, 1_000, 100)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios)\nparetoshapeplot(result) We can also plot the Pareto shape parameters directly: paretoshapeplot(result.pareto_shape) We can also use  plot  directly: plot(result.pareto_shape; showlines=true) source VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO]"},{"id":136,"pagetitle":"Internal","title":"Internal","ref":"/PSIS/stable/internal/#Internal","content":" Internal"},{"id":137,"pagetitle":"Internal","title":"PSIS.GeneralizedPareto","ref":"/PSIS/stable/internal/#PSIS.GeneralizedPareto","content":" PSIS.GeneralizedPareto  —  Type GeneralizedPareto{T<:Real} The generalized Pareto distribution. This is equivalent to  Distributions.GeneralizedPareto  and can be converted to one with  convert(Distributions.GeneralizedPareto, d) . Constructor GeneralizedPareto(μ, σ, k) Construct the generalized Pareto distribution (GPD) with location parameter  $μ$ , scale parameter  $σ$  and shape parameter  $k$ . Note The shape parameter  $k$  is equivalent to the commonly used shape parameter  $ξ$ . This is the same parameterization used by  [VehtariSimpson2021]  and is related to that used by  [ZhangStephens2009]  as  $k \\mapsto -k$ . source"},{"id":138,"pagetitle":"Internal","title":"PSIS.fit_gpd","ref":"/PSIS/stable/internal/#PSIS.fit_gpd-Tuple{AbstractArray}","content":" PSIS.fit_gpd  —  Method fit_gpd(x; μ=0, kwargs...) Fit a  GeneralizedPareto  with location  μ  to the data  x . The fit is performed using the Empirical Bayes method of  [ZhangStephens2009] . Keywords prior_adjusted::Bool=true , If  true , a weakly informative Normal prior centered on  $\\frac{1}{2}$  is used for the shape  $k$ . sorted::Bool=issorted(x) : If  true ,  x  is assumed to be sorted. If  false , a sorted copy of  x  is made. min_points::Int=30 : The minimum number of quadrature points to use when estimating the posterior mean of  $\\theta = \\frac{\\xi}{\\sigma}$ . source ZhangStephens2009 Jin Zhang & Michael A. Stephens (2009) A New and Efficient Estimation Method for the Generalized Pareto Distribution, Technometrics, 51:3, 316-325, DOI:  10.1198/tech.2009.08017"},{"id":141,"pagetitle":"Plotting","title":"Plotting PSIS results","ref":"/PSIS/stable/plotting/#Plotting-PSIS-results","content":" Plotting PSIS results PSIS.jl includes plotting recipes for  PSISResult  using any Plots.jl backend, as well as the utility plotting function  PSISPlots.paretoshapeplot . We demonstrate this with a simple example. using PSIS, Distributions\nproposal = Normal()\ntarget = TDist(7)\nndraws, nchains, nparams = (1_000, 1, 20)\nx = rand(proposal, ndraws, nchains, nparams)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) PSISResult with 1000 draws, 1 chains, and 20 parameters\nPareto shape (k) diagnostic values:\n                    Count       Min. ESS\n (-Inf, 0.5]  good   4 (20.0%)  959\n  (0.5, 0.7]  okay  10 (50.0%)  927\n    (0.7, 1]  bad    6 (30.0%)  ——"},{"id":142,"pagetitle":"Plotting","title":"Plots.jl","ref":"/PSIS/stable/plotting/#Plots.jl","content":" Plots.jl PSISResult  objects can be plotted directly: using Plots\nplot(result; showlines=true, marker=:+, legend=false, linewidth=2) This is equivalent to calling  PSISPlots.paretoshapeplot(result; kwargs...) ."},{"id":147,"pagetitle":"Home","title":"MCMCDiagnosticTools","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools","content":" MCMCDiagnosticTools"},{"id":148,"pagetitle":"Home","title":"Effective sample size and  $\\widehat{R}$","ref":"/MCMCDiagnosticTools/stable/#Effective-sample-size-and-\\\\widehat{R}","content":" Effective sample size and  $\\widehat{R}$"},{"id":149,"pagetitle":"Home","title":"MCMCDiagnosticTools.ess","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.ess","content":" MCMCDiagnosticTools.ess  —  Function ess(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind=:bulk,\n    relative::Bool=false,\n    autocov_method=AutocovMethod(),\n    split_chains::Int=2,\n    maxlag::Int=250,\n    kwargs...\n) Estimate the effective sample size (ESS) of the  samples  of shape  (draws, [chains[, parameters...]])  with the  autocov_method . Optionally, the  kind  of ESS estimate to be computed can be specified (see below). Some  kind s accept additional  kwargs . If  relative  is  true , the relative ESS is returned, i.e.  ess / (draws * chains) . split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. There must be at least 3 draws in each chain after splitting. maxlag  indicates the maximum lag for which autocovariance is computed and must be greater than 0. For a given estimand, it is recommended that the ESS is at least  100 * chains  and that  $\\widehat{R} < 1.01$ . [VehtariGelman2021] See also:  AutocovMethod ,  FFTAutocovMethod ,  BDAAutocovMethod ,  rhat ,  ess_rhat ,  mcse Kinds of ESS estimates If  kind  isa a  Symbol , it may take one of the following values: :bulk : basic ESS computed on rank-normalized draws. This kind diagnoses poor convergence   in the bulk of the distribution due to trends or different locations of the chains. :tail : minimum of the quantile-ESS for the symmetric quantiles where    tail_prob=0.1  is the probability in the tails. This kind diagnoses poor convergence in   the tails of the distribution. If this kind is chosen,  kwargs  may contain a    tail_prob  keyword. :basic : basic ESS, equivalent to specifying  kind=Statistics.mean . Note While Bulk-ESS is conceptually related to basic ESS, it is well-defined even if the chains do not have finite variance. [VehtariGelman2021]  For each parameter, rank-normalization proceeds by first ranking the inputs using \"tied ranking\" and then transforming the ranks to normal quantiles so that the result is standard normally distributed. This transform is monotonic. Otherwise,  kind  specifies one of the following estimators, whose ESS is to be estimated: Statistics.mean Statistics.median Statistics.std StatsBase.mad Base.Fix2(Statistics.quantile, p::Real) source"},{"id":150,"pagetitle":"Home","title":"MCMCDiagnosticTools.rhat","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rhat","content":" MCMCDiagnosticTools.rhat  —  Function rhat(samples::AbstractArray{Union{Real,Missing}}; kind::Symbol=:rank, split_chains=2) Compute the  $\\widehat{R}$  diagnostics for each parameter in  samples  of shape  (draws, [chains[, parameters...]]) . [VehtariGelman2021] kind  indicates the kind of  $\\widehat{R}$  to compute (see below). split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. See also  ess ,  ess_rhat ,  rstar Kinds of  $\\widehat{R}$ The following  kind s are supported: :rank : maximum of  $\\widehat{R}$  with  kind=:bulk  and  kind=:tail . :bulk : basic  $\\widehat{R}$  computed on rank-normalized draws. This kind diagnoses   poor convergence in the bulk of the distribution due to trends or different locations of   the chains. :tail :  $\\widehat{R}$  computed on draws folded around the median and then   rank-normalized. This kind diagnoses poor convergence in the tails of the distribution   due to different scales of the chains. :basic : Classic  $\\widehat{R}$ . source"},{"id":151,"pagetitle":"Home","title":"MCMCDiagnosticTools.ess_rhat","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.ess_rhat","content":" MCMCDiagnosticTools.ess_rhat  —  Function ess_rhat(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind::Symbol=:rank,\n    kwargs...,\n) -> NamedTuple{(:ess, :rhat)} Estimate the effective sample size and  $\\widehat{R}$  of the  samples  of shape  (draws, [chains[, parameters...]]) . When both ESS and  $\\widehat{R}$  are needed, this method is often more efficient than calling  ess  and  rhat  separately. See  rhat  for a description of supported  kind s and  ess  for a description of  kwargs . source The following  autocov_method s are supported:"},{"id":152,"pagetitle":"Home","title":"MCMCDiagnosticTools.AutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.AutocovMethod","content":" MCMCDiagnosticTools.AutocovMethod  —  Type AutocovMethod <: AbstractAutocovMethod The  AutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021]  and uses the biased estimator of the autocovariance, as discussed by  [Geyer1992] . source"},{"id":153,"pagetitle":"Home","title":"MCMCDiagnosticTools.FFTAutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.FFTAutocovMethod","content":" MCMCDiagnosticTools.FFTAutocovMethod  —  Type FFTAutocovMethod <: AbstractAutocovMethod The  FFTAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. The algorithm is the same as the one of  AutocovMethod  but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation. Info To be able to use this method, you have to load a package that implements the  AbstractFFTs.jl  interface such as  FFTW.jl  or  FastTransforms.jl . source"},{"id":154,"pagetitle":"Home","title":"MCMCDiagnosticTools.BDAAutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.BDAAutocovMethod","content":" MCMCDiagnosticTools.BDAAutocovMethod  —  Type BDAAutocovMethod <: AbstractAutocovMethod The  BDAAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021] . and uses the variogram estimator of the autocorrelation function discussed by  [BDA3] . source"},{"id":155,"pagetitle":"Home","title":"Monte Carlo standard error","ref":"/MCMCDiagnosticTools/stable/#Monte-Carlo-standard-error","content":" Monte Carlo standard error"},{"id":156,"pagetitle":"Home","title":"MCMCDiagnosticTools.mcse","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.mcse","content":" MCMCDiagnosticTools.mcse  —  Function mcse(samples::AbstractArray{<:Union{Missing,Real}}; kind=Statistics.mean, kwargs...) Estimate the Monte Carlo standard errors (MCSE) of the estimator  kind  applied to  samples  of shape  (draws, [chains[, parameters...]]) . See also:  ess Kinds of MCSE estimates The estimator whose MCSE should be estimated is specified with  kind .  kind  must accept a vector of the same  eltype  as  samples  and return a real estimate. For the following estimators, the effective sample size  ess  and an estimate of the asymptotic variance are used to compute the MCSE, and  kwargs  are forwarded to  ess : Statistics.mean Statistics.median Statistics.std Base.Fix2(Statistics.quantile, p::Real) For other estimators, the subsampling bootstrap method (SBM) [FlegalJones2011] [Flegal2012]  is used as a fallback, and the only accepted  kwargs  are  batch_size , which indicates the size of the overlapping batches used to estimate the MCSE, defaulting to  floor(Int, sqrt(draws * chains)) . Note that SBM tends to underestimate the MCSE, especially for highly autocorrelated chains. One should verify that autocorrelation is low by checking the bulk- and tail-ESS values. source"},{"id":157,"pagetitle":"Home","title":"R⋆ diagnostic","ref":"/MCMCDiagnosticTools/stable/#R-diagnostic","content":" R⋆ diagnostic"},{"id":158,"pagetitle":"Home","title":"MCMCDiagnosticTools.rstar","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rstar","content":" MCMCDiagnosticTools.rstar  —  Function rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples,\n    chain_indices::AbstractVector{Int};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the table  samples  with the  classifier . samples  must be either an  AbstractMatrix , an  AbstractVector , or a table (i.e. implements the Tables.jl interface) whose rows are draws and whose columns are parameters. chain_indices  indicates the chain ids of each row of  samples . This method supports ragged chains, i.e. chains of nonequal lengths. source rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples::AbstractArray{<:Real};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the  samples  with the  classifier . samples  is an array of draws with the shape  (draws, [chains[, parameters...]]) .` This implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari. The  classifier  has to be a supervised classifier of the MLJ framework (see the  MLJ documentation  for a list of supported models). It is trained with a  subset  of the samples from each chain. Each chain is split into  split_chains  separate chains to additionally check for within-chain convergence. The training of the classifier can be inspected by adjusting the  verbosity  level. If the classifier is deterministic, i.e., if it predicts a class, the value of the  $R^*$  statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the  $R^*$  statistic is returned (algorithm 2). Note The correctness of the statistic depends on the convergence of the  classifier  used internally in the statistic. Examples julia> using MLJBase, MLJIteration, EvoTrees, Statistics\n\njulia> samples = fill(4.0, 100, 3, 2); One can compute the distribution of the  $R^*$  statistic (algorithm 2) with a probabilistic classifier. For instance, we can use a gradient-boosted trees model with  nrounds = 100  sequentially stacked trees and learning rate  eta = 0.05 : julia> model = EvoTreeClassifier(; nrounds=100, eta=0.05);\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 Note, however, that it is recommended to determine  nrounds  based on early-stopping. With the MLJ framework, this can be achieved in the following way (see the  MLJ documentation  for additional explanations): julia> model = IteratedModel(;\n           model=EvoTreeClassifier(; eta=0.05),\n           iteration_parameter=:nrounds,\n           resampling=Holdout(),\n           measures=log_loss,\n           controls=[Step(5), Patience(2), NumberLimit(100)],\n           retrain=true,\n       );\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 For deterministic classifiers, a single  $R^*$  statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models. julia> evotree_deterministic = Pipeline(model; operation=predict_mode);\n\njulia> value = rstar(evotree_deterministic, samples);\n\njulia> round(value; digits=2)\n1.0 References Lambert, B., & Vehtari, A. (2020).  $R^*$ : A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers. source"},{"id":159,"pagetitle":"Home","title":"Bayesian fraction of missing information","ref":"/MCMCDiagnosticTools/stable/#Bayesian-fraction-of-missing-information","content":" Bayesian fraction of missing information"},{"id":160,"pagetitle":"Home","title":"MCMCDiagnosticTools.bfmi","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.bfmi","content":" MCMCDiagnosticTools.bfmi  —  Function bfmi(energy::AbstractVector{<:Real}) -> Real\nbfmi(energy::AbstractMatrix{<:Real}; dims::Int=1) -> AbstractVector{<:Real} Calculate the estimated Bayesian fraction of missing information (BFMI). When sampling with Hamiltonian Monte Carlo (HMC), BFMI quantifies how well momentum resampling matches the marginal energy distribution. The current advice is that values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may change. A BFMI value below the threshold often indicates poor adaptation of sampling parameters or that the target distribution has heavy tails that were not well explored by the Markov chain. For more information, see Section 6.1 of  [Betancourt2018]  or  [Betancourt2016]  for a complete account. energy  is either a vector of Hamiltonian energies of draws or a matrix of energies of draws for multiple chains.  dims  indicates the dimension in  energy  that contains the draws. The default  dims=1  assumes  energy  has the shape  draws  or  (draws, chains) . If a different shape is provided,  dims  must be set accordingly. If  energy  is a vector, a single BFMI value is returned. Otherwise, a vector of BFMI values for each chain is returned. source"},{"id":161,"pagetitle":"Home","title":"Other diagnostics","ref":"/MCMCDiagnosticTools/stable/#Other-diagnostics","content":" Other diagnostics"},{"id":162,"pagetitle":"Home","title":"MCMCDiagnosticTools.discretediag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.discretediag","content":" MCMCDiagnosticTools.discretediag  —  Function discretediag(samples::AbstractArray{<:Real,3}; frac=0.3, method=:weiss, nsim=1_000) Compute discrete diagnostic on  samples  with shape  (draws, chains, parameters) . method  can be one of  :weiss ,  :hangartner ,  :DARBOOT ,  :MCBOOT ,  :billinsgley , and  :billingsleyBOOT . References Benjamin E. Deonovic, & Brian J. Smith. (2017). Convergence diagnostics for MCMC draws of a categorical variable. source"},{"id":163,"pagetitle":"Home","title":"MCMCDiagnosticTools.gelmandiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gelmandiag","content":" MCMCDiagnosticTools.gelmandiag  —  Function gelmandiag(samples::AbstractArray{<:Real,3}; alpha::Real=0.95) Compute the Gelman, Rubin and Brooks diagnostics  [Gelman1992] [Brooks1998]  on  samples  with shape  (draws, chains, parameters) .  Values of the diagnostic’s potential scale reduction factor (PSRF) that are close to one suggest convergence.  As a rule-of-thumb, convergence is rejected if the 97.5 percentile of a PSRF is greater than 1.2. source"},{"id":164,"pagetitle":"Home","title":"MCMCDiagnosticTools.gelmandiag_multivariate","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gelmandiag_multivariate","content":" MCMCDiagnosticTools.gelmandiag_multivariate  —  Function gelmandiag_multivariate(samples::AbstractArray{<:Real,3}; alpha::Real=0.05) Compute the multivariate Gelman, Rubin and Brooks diagnostics on  samples  with shape  (draws, chains, parameters) . source"},{"id":165,"pagetitle":"Home","title":"MCMCDiagnosticTools.gewekediag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gewekediag","content":" MCMCDiagnosticTools.gewekediag  —  Function gewekediag(x::AbstractVector{<:Real}; first::Real=0.1, last::Real=0.5, kwargs...) Compute the Geweke diagnostic  [Geweke1991]  from the  first  and  last  proportion of samples  x . The diagnostic is designed to asses convergence of posterior means estimated with autocorrelated samples.  It computes a normal-based test statistic comparing the sample means in two windows containing proportions of the first and last iterations.  Users should ensure that there is sufficient separation between the two windows to assume that their samples are independent.  A non-significant test p-value indicates convergence.  Significant p-values indicate non-convergence and the possible need to discard initial samples as a burn-in sequence or to simulate additional samples. kwargs  are forwarded to  mcse . source"},{"id":166,"pagetitle":"Home","title":"MCMCDiagnosticTools.heideldiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.heideldiag","content":" MCMCDiagnosticTools.heideldiag  —  Function heideldiag(\n    x::AbstractVector{<:Real}; alpha::Real=0.05, eps::Real=0.1, start::Int=1, kwargs...\n) Compute the Heidelberger and Welch diagnostic  [Heidelberger1983] . This diagnostic tests for non-convergence (non-stationarity) and whether ratios of estimation interval halfwidths to means are within a target ratio. Stationarity is rejected (0) for significant test p-values. Halfwidth tests are rejected (0) if observed ratios are greater than the target, as is the case for  s2  and  beta[1] . kwargs  are forwarded to  mcse . source"},{"id":167,"pagetitle":"Home","title":"MCMCDiagnosticTools.rafterydiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rafterydiag","content":" MCMCDiagnosticTools.rafterydiag  —  Function rafterydiag(\n    x::AbstractVector{<:Real}; q=0.025, r=0.005, s=0.95, eps=0.001, range=1:length(x)\n) Compute the Raftery and Lewis diagnostic  [Raftery1992] . This diagnostic is used to determine the number of iterations required to estimate a specified quantile  q  within a desired degree of accuracy.  The diagnostic is designed to determine the number of autocorrelated samples required to estimate a specified quantile  $\\theta_q$ , such that  $\\Pr(\\theta \\le \\theta_q) = q$ , within a desired degree of accuracy. In particular, if  $\\hat{\\theta}_q$  is the estimand and  $\\Pr(\\theta \\le \\hat{\\theta}_q) = \\hat{P}_q$  the estimated cumulative probability, then accuracy is specified in terms of  r  and  s , where  $\\Pr(q - r < \\hat{P}_q < q + r) = s$ . Thinning may be employed in the calculation of the diagnostic to satisfy its underlying assumptions. However, users may not want to apply the same (or any) thinning when estimating posterior summary statistics because doing so results in a loss of information. Accordingly, sample sizes estimated by the diagnostic tend to be conservative (too large). Furthermore, the argument  r  specifies the margin of error for estimated cumulative probabilities and  s  the probability for the margin of error.  eps  specifies the tolerance within which the probabilities of transitioning from initial to retained iterations are within the equilibrium probabilities for the chain. This argument determines the number of samples to discard as a burn-in sequence and is typically left at its default value. source VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 Geyer1992 Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483. VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 BDA3 Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press. FlegalJones2011 Flegal JM, Jones GL. (2011) Implementing MCMC: estimating with confidence.                 Handbook of Markov Chain Monte Carlo. pp. 175-97.                  pdf Flegal2012 Flegal JM. (2012) Applicability of subsampling bootstrap methods in Markov chain Monte Carlo.            Monte Carlo and Quasi-Monte Carlo Methods 2010. pp. 363-72.            doi:  10.1007/978-3-642-27440-4_18 Betancourt2018 Betancourt M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo.  arXiv:1701.02434v2  [stat.ME] Betancourt2016 Betancourt M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.  arXiv:1604.00695v1  [stat.ME] Gelman1992 Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical science, 7(4), 457-472. Brooks1998 Brooks, S. P., & Gelman, A. (1998). General methods for monitoring convergence of iterative simulations. Journal of computational and graphical statistics, 7(4), 434-455. Geweke1991 Geweke, J. F. (1991). Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments (No. 148). Federal Reserve Bank of Minneapolis. Heidelberger1983 Heidelberger, P., & Welch, P. D. (1983). Simulation run length control in the presence of an initial transient. Operations Research, 31(6), 1109-1144. Raftery1992 A L Raftery and S Lewis. Bayesian Statistics, chapter How Many Iterations in the Gibbs Sampler? Volume 4. Oxford University Press, New York, 1992."},{"id":172,"pagetitle":"Home","title":"InferenceObjects","ref":"/InferenceObjects/stable/#InferenceObjects","content":" InferenceObjects InferenceObjects.jl is a Julia implementation of the  InferenceData schema  for storing results of Bayesian inference. Its purpose is to serve the following three goals: Usefulness in the analysis of Bayesian inference results. Reproducibility of Bayesian inference analysis. Interoperability between different inference backends and programming languages. The implementation consists primarily of the  InferenceData  and  Dataset  structures. InferenceObjects also provides the function  convert_to_inference_data , which may be overloaded by inference packages to define how various inference outputs can be converted to an  InferenceData . For examples of how  InferenceData  can be used, see the  ArviZ.jl documentation ."},{"id":175,"pagetitle":"Dataset","title":"Dataset","ref":"/InferenceObjects/stable/dataset/#Dataset","content":" Dataset InferenceObjects.Dataset InferenceObjects.convert_to_dataset InferenceObjects.namedtuple_to_dataset"},{"id":176,"pagetitle":"Dataset","title":"Type definition","ref":"/InferenceObjects/stable/dataset/#Type-definition","content":" Type definition"},{"id":177,"pagetitle":"Dataset","title":"InferenceObjects.Dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.Dataset","content":" InferenceObjects.Dataset  —  Type Dataset{L} <: DimensionalData.AbstractDimStack{L} Container of dimensional arrays sharing some dimensions. This type is an  DimensionalData.AbstractDimStack  that implements the same interface as  DimensionalData.DimStack  and has identical usage. When a  Dataset  is passed to Python, it is converted to an  xarray.Dataset  without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an  xarray.Dataset  is passed to Julia, its data must be copied. Constructors Dataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n) In most cases, use  convert_to_dataset  to create a  Dataset  instead of directly using a constructor. source"},{"id":178,"pagetitle":"Dataset","title":"General conversion","ref":"/InferenceObjects/stable/dataset/#General-conversion","content":" General conversion"},{"id":179,"pagetitle":"Dataset","title":"InferenceObjects.convert_to_dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.convert_to_dataset","content":" InferenceObjects.convert_to_dataset  —  Function convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset Convert a supported object to a  Dataset . In most cases, this function calls  convert_to_inference_data  and returns the corresponding  group . source"},{"id":180,"pagetitle":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.namedtuple_to_dataset","content":" InferenceObjects.namedtuple_to_dataset  —  Function namedtuple_to_dataset(data; kwargs...) -> Dataset Convert  NamedTuple  mapping variable names to arrays to a  Dataset . Any non-array values will be converted to a 0-dimensional array. Keywords attrs::AbstractDict{<:AbstractString} : a collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable. library::Union{String,Module} : library used for performing inference. Will be attached to the  attrs  metadata. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. source"},{"id":181,"pagetitle":"Dataset","title":"DimensionalData","ref":"/InferenceObjects/stable/dataset/#DimensionalData","content":" DimensionalData As a  DimensionalData.AbstractDimStack ,  Dataset  also implements the  AbstractDimStack  API and can be used like a  DimStack . See  DimensionalData's documentation  for example usage."},{"id":182,"pagetitle":"Dataset","title":"Tables inteface","ref":"/InferenceObjects/stable/dataset/#Tables-inteface","content":" Tables inteface Dataset  implements the  Tables  interface. This allows  Dataset s to be used as sources for any function that can accept a table. For example, it's straightforward to: write to CSV with CSV.jl flatten to a DataFrame with DataFrames.jl plot with StatsPlots.jl plot with AlgebraOfGraphics.jl"},{"id":185,"pagetitle":"InferenceData","title":"InferenceData","ref":"/InferenceObjects/stable/inference_data/#InferenceData","content":" InferenceData InferenceObjects.InferenceData Base.cat Base.getindex Base.getproperty Base.merge Base.propertynames Base.setindex InferenceObjects.convert_to_inference_data InferenceObjects.from_dict InferenceObjects.from_namedtuple InferenceObjects.from_netcdf InferenceObjects.to_netcdf"},{"id":186,"pagetitle":"InferenceData","title":"Type definition","ref":"/InferenceObjects/stable/inference_data/#Type-definition","content":" Type definition"},{"id":187,"pagetitle":"InferenceData","title":"InferenceObjects.InferenceData","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.InferenceData","content":" InferenceObjects.InferenceData  —  Type InferenceData{group_names,group_types} Container for inference data storage using DimensionalData. This object implements the  InferenceData schema . Internally, groups are stored in a  NamedTuple , which can be accessed using  parent(::InferenceData) . Constructors InferenceData(groups::NamedTuple)\nInferenceData(; groups...) Construct an inference data from either a  NamedTuple  or keyword arguments of groups. Groups must be  Dataset  objects. Instead of directly creating an  InferenceData , use the exported  from_xyz  functions or  convert_to_inference_data . source"},{"id":188,"pagetitle":"InferenceData","title":"Property interface","ref":"/InferenceObjects/stable/inference_data/#Property-interface","content":" Property interface"},{"id":189,"pagetitle":"InferenceData","title":"Base.getproperty","ref":"/InferenceObjects/stable/inference_data/#Base.getproperty","content":" Base.getproperty  —  Function getproperty(data::InferenceData, name::Symbol) -> Dataset Get group with the specified  name . source"},{"id":190,"pagetitle":"InferenceData","title":"Base.propertynames","ref":"/InferenceObjects/stable/inference_data/#Base.propertynames","content":" Base.propertynames  —  Function propertynames(data::InferenceData) -> Tuple{Symbol} Get names of groups source"},{"id":191,"pagetitle":"InferenceData","title":"Indexing interface","ref":"/InferenceObjects/stable/inference_data/#Indexing-interface","content":" Indexing interface"},{"id":192,"pagetitle":"InferenceData","title":"Base.getindex","ref":"/InferenceObjects/stable/inference_data/#Base.getindex","content":" Base.getindex  —  Function Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData Return a new  InferenceData  containing the specified groups sliced to the specified coords. coords  specifies a dimension name mapping to an index, a  DimensionalData.Selector , or an  IntervalSets.AbstractInterval . If one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised. Examples Select data from all groups for just the specified id values. julia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\" Select data from just the posterior, returning a  Dataset  if the indices index more than one element from any of the variables: julia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:19:25.982\" Note that if a single index is provided, the behavior is still to slice so that the dimension is preserved. source"},{"id":193,"pagetitle":"InferenceData","title":"Base.setindex","ref":"/InferenceObjects/stable/inference_data/#Base.setindex","content":" Base.setindex  —  Function Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData Create a new  InferenceData  containing the  group  with the specified  name . If a group with  name  is already in  data , it is replaced. source"},{"id":194,"pagetitle":"InferenceData","title":"Iteration interface","ref":"/InferenceObjects/stable/inference_data/#Iteration-interface","content":" Iteration interface InferenceData  also implements the same iteration interface as its underlying  NamedTuple . That is, iterating over an  InferenceData  iterates over its groups."},{"id":195,"pagetitle":"InferenceData","title":"General conversion","ref":"/InferenceObjects/stable/inference_data/#General-conversion","content":" General conversion"},{"id":196,"pagetitle":"InferenceData","title":"InferenceObjects.convert_to_inference_data","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.convert_to_inference_data","content":" InferenceObjects.convert_to_inference_data  —  Function convert_to_inference_data(obj; group, kwargs...) -> InferenceData Convert a supported object to an  InferenceData  object. If  obj  converts to a single dataset,  group  specifies which dataset in the resulting  InferenceData  that is. See  convert_to_dataset Arguments obj  can be many objects. Basic supported types are: InferenceData : return unchanged Dataset / DimensionalData.AbstractDimStack : add to  InferenceData  as the only group NamedTuple / AbstractDict : create a  Dataset  as the only group AbstractArray{<:Real} : create a  Dataset  as the only group, given an arbitrary name, if the name is not set More specific types may be documented separately. Keywords group::Symbol = :posterior : If  obj  converts to a single dataset, assign the resulting dataset to this group. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. kwargs : remaining keywords forwarded to converter functions source"},{"id":197,"pagetitle":"InferenceData","title":"InferenceObjects.from_dict","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_dict","content":" InferenceObjects.from_dict  —  Function from_dict(posterior::AbstractDict; kwargs...) -> InferenceData Convert a  Dict  to an  InferenceData . Arguments posterior : The data to be converted. Its strings must be  Symbol  or  AbstractString , and its values must be arrays. Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior::Dict=nothing : Draws from the prior prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata = Dict(\n    :x => rand(ndraws, nchains),\n    :y => randn(2, ndraws, nchains),\n    :z => randn(3, 2, ndraws, nchains),\n)\nidata = from_dict(data) source"},{"id":198,"pagetitle":"InferenceData","title":"InferenceObjects.from_namedtuple","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_namedtuple","content":" InferenceObjects.from_namedtuple  —  Function from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData Convert a  NamedTuple  or container of  NamedTuple s to an  InferenceData . If containers are passed, they are flattened into a single  NamedTuple  with array elements whose first dimensions correspond to the dimensions of the containers. Arguments posterior : The data to be converted. It may be of the following types: ::NamedTuple : The keys are the variable names and the values are arrays with dimensions  (ndraws, nchains[, sizes...]) . ::Vector{Vector{<:NamedTuple}} : A vector of length  nchains  whose elements have length  ndraws . Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior=nothing : Draws from the prior. Accepts the same types as  posterior . prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Note If a  NamedTuple  is provided for  observed_data ,  constant_data , or predictions constant data`, any non-array values (e.g. integers) are converted to 0-dimensional arrays. Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata1 = (\n    x=rand(ndraws, nchains), y=randn(ndraws, nchains, 2), z=randn(ndraws, nchains, 3, 2)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata2 = from_namedtuple(data2) source"},{"id":199,"pagetitle":"InferenceData","title":"General functions","ref":"/InferenceObjects/stable/inference_data/#General-functions","content":" General functions"},{"id":200,"pagetitle":"InferenceData","title":"Base.cat","ref":"/InferenceObjects/stable/inference_data/#Base.cat","content":" Base.cat  —  Function cat(data::InferenceData...; [groups=keys(data[1]),] dims) -> InferenceData Concatenate  InferenceData  objects along the specified dimension  dims . Only the groups in  groups  are concatenated. Remaining groups are  merge d into the new  InferenceData  object. Examples Here is how we can concatenate all groups of two  InferenceData  objects along the existing  chain  dimension: julia> coords = (; a_dim=[\"x\", \"y\", \"z\"]);\n\njulia> dims = dims=(; a=[:a_dim]);\n\njulia> data = Dict(:a => randn(100, 4, 3), :b => randn(100, 4));\n\njulia> idata = from_dict(data; coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1 = cat(idata, idata; dims=:chain)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim} (100×8×3)\n  :b Float64 dims: Dim{:draw}, Dim{:chain} (100×8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\" Alternatively, we can concatenate along a new  run  dimension, which will be created. julia> idata_cat2 = cat(idata, idata; dims=:run)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat2.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\" We can also concatenate only a subset of groups and merge the rest, which is useful when some groups are present only in some of the  InferenceData  objects or will be identical in all of them: julia> observed_data = Dict(:y => randn(10));\n\njulia> idata2 = from_dict(data; observed_data=observed_data, coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3 = cat(idata, idata2; groups=(:posterior,), dims=:run)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\"\n\njulia> idata_cat3.observed_data\nDataset with dimensions: Dim{:y_dim_1}\nand 1 layer:\n  :y Float64 dims: Dim{:y_dim_1} (10)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-02-17T15:11:00.59\" source"},{"id":201,"pagetitle":"InferenceData","title":"Base.merge","ref":"/InferenceObjects/stable/inference_data/#Base.merge","content":" Base.merge  —  Function merge(data::InferenceData...) -> InferenceData Merge  InferenceData  objects. The result contains all groups in  data  and  others . If a group appears more than once, the one that occurs last is kept. See also:  cat Examples Here we merge an  InferenceData  containing only a posterior group with one containing only a prior group to create a new one containing both groups. julia> idata1 = from_dict(Dict(:a => randn(100, 4, 3), :b => randn(100, 4)))\nInferenceData with groups:\n  > posterior\n\njulia> idata2 = from_dict(; prior=Dict(:a => randn(100, 1, 3), :c => randn(100, 1)))\nInferenceData with groups:\n  > prior\n\njulia> idata_merged = merge(idata1, idata2)\nInferenceData with groups:\n  > posterior\n  > prior source"},{"id":202,"pagetitle":"InferenceData","title":"I/O extensions","ref":"/InferenceObjects/stable/inference_data/#I/O-extensions","content":" I/O extensions The following types of storage are provided via extensions."},{"id":203,"pagetitle":"InferenceData","title":"NetCDF I/O using NCDatasets.jl","ref":"/InferenceObjects/stable/inference_data/#NetCDF-I/O-using-NCDatasets.jl","content":" NetCDF I/O using NCDatasets.jl"},{"id":204,"pagetitle":"InferenceData","title":"InferenceObjects.from_netcdf","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_netcdf","content":" InferenceObjects.from_netcdf  —  Function from_netcdf(path::AbstractString; kwargs...) -> InferenceData Load an  InferenceData  from an unopened NetCDF file. Remaining  kwargs  are passed to  NCDatasets.NCDataset . This method loads data eagerly. To instead load data lazily, pass an opened  NCDataset  to  from_netcdf . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_netcdf(\"centered_eight.nc\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data from_netcdf(ds::NCDatasets.NCDataset; load_mode) -> InferenceData Load an  InferenceData  from an opened NetCDF file. load_mode  defaults to  :lazy , which avoids reading variables into memory. Operations on these arrays will be slow.  load_mode  can also be  :eager , which copies all variables into memory. It is then safe to close  ds . If  load_mode  is  :lazy  and  ds  is closed after constructing  InferenceData , using the variable arrays will have undefined behavior. Examples Here is how we might load an  InferenceData  from an  InferenceData  lazily from a web-hosted NetCDF file. julia> using HTTP, InferenceObjects, NCDatasets\n\njulia> resp = HTTP.get(\"https://github.com/arviz-devs/arviz_example_data/blob/main/data/centered_eight.nc?raw=true\");\n\njulia> ds = NCDataset(\"centered_eight\", \"r\"; memory = resp.body);\n\njulia> idata = from_netcdf(ds)\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\njulia> idata_copy = copy(idata); # disconnect from the loaded dataset\n\njulia> close(ds); source"},{"id":205,"pagetitle":"InferenceData","title":"InferenceObjects.to_netcdf","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.to_netcdf","content":" InferenceObjects.to_netcdf  —  Function to_netcdf(data, dest::AbstractString; group::Symbol=:posterior, kwargs...)\nto_netcdf(data, dest::NCDatasets.NCDataset; group::Symbol=:posterior) Write  data  to a NetCDF file. data  is any type that can be converted to an  InferenceData  using  convert_to_inference_data . If not an  InferenceData , then  group  specifies which group the data represents. dest  specifies either the path to the NetCDF file or an opened NetCDF file. If  dest  is a path, remaining  kwargs  are passed to  NCDatasets.NCDataset . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_namedtuple((; x = randn(4, 100, 3), z = randn(4, 100)))\nInferenceData with groups:\n  > posterior\n\njulia> to_netcdf(idata, \"data.nc\")\n\"data.nc\" source"},{"id":210,"pagetitle":"Home","title":"ArviZExampleData","ref":"/ArviZExampleData/stable/#ArviZExampleData","content":" ArviZExampleData This package provides utilities for loading datasets defined in the  arviz_example_data  repository. The resulting objects are  InferenceObjects.jl 's  InferenceData . These utilities are used in  ArviZ.jl ."},{"id":213,"pagetitle":"API","title":"API","ref":"/ArviZExampleData/stable/api/#API","content":" API"},{"id":214,"pagetitle":"API","title":"ArviZExampleData.describe_example_data","ref":"/ArviZExampleData/stable/api/#ArviZExampleData.describe_example_data","content":" ArviZExampleData.describe_example_data  —  Function describe_example_data() -> String Return a string containing descriptions of all available datasets. Examples julia> describe_example_data(\"radon\") |> println\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472 source"},{"id":215,"pagetitle":"API","title":"ArviZExampleData.load_example_data","ref":"/ArviZExampleData/stable/api/#ArviZExampleData.load_example_data","content":" ArviZExampleData.load_example_data  —  Function load_example_data(name; kwargs...) -> InferenceObjects.InferenceData\nload_example_data() -> Dict{String,AbstractFileMetadata} Load a local or remote pre-made dataset. kwargs  are forwarded to  InferenceObjects.from_netcdf . Pass no parameters to get a  Dict  listing all available datasets. Data files are handled by DataDeps.jl. A file is downloaded only when it is requested and then cached for future use. Examples julia> keys(load_example_data())\nKeySet for a OrderedCollections.OrderedDict{String, ArviZExampleData.AbstractFileMetadata} with 9 entries. Keys:\n  \"centered_eight\"\n  \"non_centered_eight\"\n  \"radon\"\n  \"rugby\"\n  \"regression1d\"\n  \"regression10d\"\n  \"classification1d\"\n  \"classification10d\"\n  \"glycan_torsion_angles\"\n\njulia> load_example_data(\"centered_eight\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > log_likelihood\n  > sample_stats\n  > prior\n  > prior_predictive\n  > observed_data\n  > constant_data source"},{"id":218,"pagetitle":"Datasets","title":"Datasets","ref":"/ArviZExampleData/stable/datasets/#Datasets","content":" Datasets The following shows the currently available example datasets: using ArviZExampleData\n\nprintln(describe_example_data()) centered_eight\n==============\n\nA centered parameterization of the eight schools model. Provided as an example of a model that NUTS has trouble fitting. Compare to `non_centered_eight`.\n\nThe eight schools model is a hierarchical model used for an analysis of the effectiveness of classes that were designed to improve students' performance on the Scholastic Aptitude Test.\n\nSee Bayesian Data Analysis (Gelman et. al.) for more details.\n\nlocal: /home/runner/.julia/artifacts/1a48fb48ab2b35cdeeb84e0dcdcda134e24a1c20/arviz_example_data-0.1.1/data/centered_eight.nc\n\nnon_centered_eight\n==================\n\nA non-centered parameterization of the eight schools model. This is a hierarchical model where sampling problems may be fixed by a non-centered parametrization. Compare to `centered_eight`.\n\nThe eight schools model is a hierarchical model used for an analysis of the effectiveness of classes that were designed to improve students' performance on the Scholastic Aptitude Test.\n\nSee Bayesian Data Analysis (Gelman et. al.) for more details.\n\nlocal: /home/runner/.julia/artifacts/1a48fb48ab2b35cdeeb84e0dcdcda134e24a1c20/arviz_example_data-0.1.1/data/non_centered_eight.nc\n\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472\n\nrugby\n=====\n\nThe Six Nations Championship is a yearly rugby competition between Italy, Ireland, Scotland, England, France and Wales. Fifteen games are played each year, representing all combinations of the six teams.\n\nThis example uses and includes results from 2014 - 2017, comprising 60 total games. It models latent parameters for each team's attack and defense, as well as a parameter for home team advantage.\n\nSee https://docs.pymc.io/notebooks/rugby_analytics.html by Peader Coyle for more details and references.\n\nremote: http://ndownloader.figshare.com/files/16254359\n\nregression1d\n============\n\nA synthetic one dimensional linear regression dataset with latent slope, intercept, and noise (\"eps\"). One hundred data points, fit with PyMC3.\n\nTrue slope and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16254899\n\nregression10d\n=============\n\nA synthetic multi-dimensional (10 dimensions) linear regression dataset with latent weights (\"w\"), intercept, and noise (\"eps\"). Five hundred data points, fit with PyMC3.\n\nTrue weights and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16255736\n\nclassification1d\n================\n\nA synthetic one dimensional logistic regression dataset with latent slope and intercept, passed into a Bernoulli random variable. One hundred data points, fit with PyMC3.\n\nTrue slope and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16256678\n\nclassification10d\n=================\n\nA synthetic multi dimensional (10 dimensions) logistic regression dataset with latent weights (\"w\") and intercept, passed into a Bernoulli random variable. Five hundred data points, fit with PyMC3.\n\nTrue weights and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16256681\n\nglycan_torsion_angles\n=====================\n\nTorsion angles phi and psi are critical for determining the three dimensional structure of bio-molecules. Combinations of phi and psi torsion angles that produce clashes between atoms in the bio-molecule result in high energy, unlikely structures.\n\nThis model uses a Von Mises distribution to propose torsion angles for the structure of a glycan molecule (pdb id: 2LIQ), and a Potential to estimate the proposed structure's energy. Said Potential is bound by Boltzman's law.\n\nremote: http://ndownloader.figshare.com/files/22882652"},{"id":221,"pagetitle":"For developers","title":"For developers","ref":"/ArviZExampleData/stable/for_developers/#For-developers","content":" For developers This package has  arviz_example_data  as a data dependency, which is included as an  artifact . When  arviz_example_data  is updated, and a new release is made,  Artifacts.toml  should be updated to point to the new tarball corresponding to the release: julia> using ArtifactUtils\n\njulia> version = v\"0.1.0\";\n\njulia> tarball_url = \"https://github.com/arviz-devs/arviz_example_data/archive/refs/tags/v$version.tar.gz\";\n\njulia> add_artifact!(\"Artifacts.toml\", \"arviz_example_data\", tarball_url; force=true);"},{"id":226,"pagetitle":"DimensionalData.jl","title":"DimensionalData ¤","ref":"/DimensionalData/stable/#dimensionaldata","content":" DimensionalData ¤ DimensionalData.jl provides tools and abstractions for working with datasets that have named dimensions, and optionally a lookup index. DimensionalData is a pluggable, generalised version of  AxisArrays.jl  with a cleaner syntax, and additional functionality found in NamedDims.jl. It has similar goals to pythons  xarray , and is primarily written for use with spatial data in  Rasters.jl ."},{"id":227,"pagetitle":"DimensionalData.jl","title":"Goals ¤","ref":"/DimensionalData/stable/#goals","content":" Goals ¤ Clean, readable syntax. Minimise required parentheses, minimise of exported Zero-cost dimensional indexing  a[Y(4), X(5)]  of a single value. methods, and instead extend Base methods whenever possible. Plotting is easy: data should plot sensibly and correctly with useful labels, by default. Least surprise: everything works the same as in Base, but with named dims. If a method accepts numeric indices or  dims=X  in base, you should be able to use DimensionalData.jl dims. Minimal interface: implementing a dimension-aware type should be easy. Maximum extensibility: always use method dispatch. Regular types over special syntax. Recursion over @generated. Always dispatch on abstract types. Type stability: dimensional methods should be type stable  more often  than Base methods Functional style: structs are always rebuilt, and other than the array data, fields are not mutated in place."},{"id":228,"pagetitle":"DimensionalData.jl","title":"For package developers ¤","ref":"/DimensionalData/stable/#for-package-developers","content":" For package developers ¤"},{"id":229,"pagetitle":"DimensionalData.jl","title":"Data types and the interface ¤","ref":"/DimensionalData/stable/#data-types-and-the-interface","content":" Data types and the interface ¤ DimensionalData.jl provides the concrete  DimArray  type. But its behaviours are intended to be easily applied to other array types. more The main requirement for extending DimensionalData.jl is to define a  dims  method\nthat returns a  Tuple  of  Dimension  that matches the dimension order\nand axis values of your data. Define  rebuild  and base methods for  similar \nand  parent  if you want the metadata to persist through transformations (see\nthe  DimArray  and  AbstractDimArray  types). A  refdims  method\nreturns the lost dimensions of a previous transformation, passed in to the\n rebuild  method.  refdims  can be discarded, the main loss being plot labels\nand ability to reconstruct dimensions in  cat . Inheriting from  AbstractDimArray  in this way will give nearly all the functionality\nof using  DimArray ."},{"id":230,"pagetitle":"DimensionalData.jl","title":"LookupArrays and Dimensions ¤","ref":"/DimensionalData/stable/#lookuparrays-and-dimensions","content":" LookupArrays and Dimensions ¤ Sub modules  LookupArrays  and  Dimensions  define the behviour of dimensions and their lookup index. LookupArrays  and  Dimensions"},{"id":233,"pagetitle":"Crash course - DimensionalData.jl","title":"Dimensions and DimArrays ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#dimensions-and-dimarrays","content":" Dimensions and DimArrays ¤ The core type of DimensionalData.jl is the  Dimension  and the types that inherit from it, such as  Ti ,  X ,  Y ,  Z , the generic  Dim{:x} , or others that you define manually using the  @dim  macro. Dimension s are primarily used in  DimArray , other  AbstractDimArray . We can use dimensions without a value index - these simply label the axis. A  DimArray  with labelled dimensions is constructed by: using   DimensionalData \n A   =   rand ( X ( 5 ),   Y ( 5 )) \n 5×5 DimArray{Float64,2} with dimensions: X, Y\n 0.326399  0.449116  0.922348   0.450403  0.255798\n 0.307302  0.716453  0.396757   0.187814  0.187923\n 0.68166   0.734454  0.0951085  0.883165  0.421633\n 0.42558   0.888717  0.41414    0.719216  0.685233\n 0.837418  0.541231  0.850918   0.340927  0.584232\n get value A [ Y ( 1 ),   X ( 2 )] \n 0.3073022164774323\n As shown above,  Dimension s can be used to construct arrays in  rand ,  ones ,  zeros  and  fill  with either a range for a lookup index or a number for the dimension length. Or we can use the  Dim{X}  dims by using  Symbol s, and indexing with keywords: A   =   DimArray ( rand ( 5 ,   5 ),   ( :a ,   :b )) \n 5×5 DimArray{Float64,2} with dimensions: Dim{:a}, Dim{:b}\n 0.493574   0.116382  0.02315   0.990526    0.969482\n 0.497526   0.180848  0.810566  0.374507    0.170272\n 0.0848161  0.59034   0.32168   0.838886    0.646741\n 0.885645   0.171522  0.735365  0.137074    0.96323\n 0.608382   0.564534  0.418109  0.00941501  0.604216\n get value A [ a = 3 ,   b = 5 ] \n 0.6467413050049334\n Often, we want to provide a lookup index for the dimension: using   Dates \n t   =   DateTime ( 2001 ) : Month ( 1 ) : DateTime ( 2001 , 12 ) \n x   =   10 : 10 : 100 \n A   =   rand ( X ( x ),   Ti ( t )) \n 10×12 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 10:10:100 ForwardOrdered Regular Points,\n  Ti Sampled{DateTime} DateTime(\"2001-01-01T00:00:00\"):Month(1):DateTime(\"2001-12-01T00:00:00\") ForwardOrdered Regular Points\n       2001-01-01T00:00:00  …   2001-12-01T00:00:00\n  10  0.702001                             0.380606\n  20  0.627806                             0.978036\n  30  0.525252                             0.963641\n  40  0.635074                             0.248951\n  50  0.0152012                         …  0.836759\n  60  0.33652                              0.776188\n  70  0.892738                             0.748656\n  80  0.874208                             0.271561\n  90  0.790154                             0.541713\n 100  0.076887                          …  0.866461\n Here both  X  and  Ti  are dimensions from  DimensionalData . The currently exported dimensions are  X, Y, Z, Ti  ( Ti  is shortening of  Time ). The length of each dimension index has to match the size of the corresponding array axis. This can also be done with  Symbol , using  Dim{X} : A2   =   DimArray ( rand ( 12 ,   10 ),   ( time = t ,   distance = x )) \n 12×10 DimArray{Float64,2} with dimensions: \n  Dim{:time} Sampled{DateTime} DateTime(\"2001-01-01T00:00:00\"):Month(1):DateTime(\"2001-12-01T00:00:00\") ForwardOrdered Regular Points,\n  Dim{:distance} Sampled{Int64} 10:10:100 ForwardOrdered Regular Points\n                                   …  80         90          100\n  2001-01-01T00:00:00      0.042442   0.697132     0.126196\n  2001-02-01T00:00:00      0.111109   0.339442     0.741345\n  2001-03-01T00:00:00      0.226634   0.663471     0.691085\n  2001-04-01T00:00:00      0.968837   0.985512     0.0856909\n  2001-05-01T00:00:00  …   0.29492    0.893663     0.104534\n  2001-06-01T00:00:00      0.305249   0.954838     0.762939\n  2001-07-01T00:00:00      0.892422   0.0879024    0.123994\n  2001-08-01T00:00:00      0.268962   0.0918058    0.259626\n  2001-09-01T00:00:00      0.627997   0.665768     0.284332\n  2001-10-01T00:00:00  …   0.190243   0.422224     0.803572\n  2001-11-01T00:00:00      0.363513   0.916428     0.106744\n  2001-12-01T00:00:00      0.156567   0.840281     0.137847\n Symbols can be more convenient to use than defining custom dims with  @dim , but have some downsides. They don't inherit from a specific  Dimension  type, so plots will not know what axis to put them on. They also cannot use the basic constructor methods like  rand  or  zeros , as we cannot dispatch on  Symbol  for Base methods without \"type-piracy\"."},{"id":234,"pagetitle":"Crash course - DimensionalData.jl","title":"Indexing the array by name and index ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#indexing-the-array-by-name-and-index","content":" Indexing the array by name and index ¤ Dimensions can be used to index the array by name, without having to worry about the order of the dimensions. The simplest case is to select a dimension by index. Let's say every 2nd point of the  Ti  dimension and every 3rd point of the  X  dimension. This is done with the simple  Ti(range)  syntax like so: A [ X ( 1 : 3 : 11 ),   Ti ( 1 : 2 : 11 )] \n 4×6 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 10:30:100 ForwardOrdered Regular Points,\n  Ti Sampled{DateTime} DateTime(\"2001-01-01T00:00:00\"):Month(2):DateTime(\"2001-11-01T00:00:00\") ForwardOrdered Regular Points\n       2001-01-01T00:00:00  …   2001-11-01T00:00:00\n  10  0.702001                             0.909128\n  40  0.635074                             0.231497\n  70  0.892738                             0.902822\n 100  0.076887                             0.410301\n When specifying only one dimension, all elements of the other dimensions are assumed to be included: A [ X ( 1 : 3 : 10 )] \n 4×12 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 10:30:100 ForwardOrdered Regular Points,\n  Ti Sampled{DateTime} DateTime(\"2001-01-01T00:00:00\"):Month(1):DateTime(\"2001-12-01T00:00:00\") ForwardOrdered Regular Points\n       2001-01-01T00:00:00  …   2001-12-01T00:00:00\n  10  0.702001                             0.380606\n  40  0.635074                             0.248951\n  70  0.892738                             0.748656\n 100  0.076887                             0.866461\n Indexing Indexing  AbstractDimArray s works with  getindex ,  setindex!  and    view . The result is still an  AbstracDimArray , unless using all single    Int  or  Selector s that resolve to  Int . Dimension s can be used to construct arrays in  rand ,  ones ,  zeros  and  fill  with either a range for a lookup index or a number for the dimension length. using   DimensionalData \n A1   =   ones ( X ( 1 : 40 ),   Y ( 50 )) \n 40×50 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 1:40 ForwardOrdered Regular Points,\n  Y\n  1  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  2  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  3  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  4  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  5  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  6  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  ⋮                      ⋮              ⋱  ⋮                        ⋮    \n 35  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 36  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 37  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 38  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 39  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 40  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n We can also use dim wrappers for indexing, so that the dimension order in the underlying array does not need to be known: A1 [ Y ( 1 ),   X ( 1 : 10 )] \n 10-element DimArray{Float64,1} with dimensions: \n  X Sampled{Int64} 1:10 ForwardOrdered Regular Points\nand reference dimensions: Y\n  1  1.0\n  2  1.0\n  3  1.0\n  4  1.0\n  5  1.0\n  6  1.0\n  7  1.0\n  8  1.0\n  9  1.0\n 10  1.0\n"},{"id":235,"pagetitle":"Crash course - DimensionalData.jl","title":"Indexing Performance ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#indexing-performance","content":" Indexing Performance ¤ Indexing with  Dimension  has no runtime cost: A2   =   ones ( X ( 3 ),   Y ( 3 )) \n 3×3 DimArray{Float64,2} with dimensions: X, Y\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n time ? using   BenchmarkTools \n\n println ( @btime   $ A2 [ X ( 1 ),   Y ( 2 )]) \n   48.831 ns (0 allocations: 0 bytes)\n1.0\n and println ( @btime   parent ( $ A2 )[ 1 ,   2 ]) \n   4.500 ns (0 allocations: 0 bytes)\n1.0\n"},{"id":236,"pagetitle":"Crash course - DimensionalData.jl","title":"Specifying  dims  keyword arguments with  Dimension ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#specifying-dims-keyword-arguments-with-dimension","content":" Specifying  dims  keyword arguments with  Dimension ¤ In many Julia functions like  size  or  sum , you can specify the dimension along which to perform the operation as an  Int . It is also possible to do this using  Dimension  types with  AbstractDimArray : A3   =   rand ( X ( 3 ),   Y ( 4 ),   Ti ( 5 )); \n sum ( A3 ;   dims = Ti ) \n 3×4×1 DimArray{Float64,3} with dimensions: X, Y, Ti\n[:, :, 1]\n 1.59268  2.30022  2.38162  3.60432\n 2.6186   2.0887   2.84429  3.00575\n 3.74909  3.04419  1.93713  1.67302\n This also works in methods from  Statistics : using   Statistics \n mean ( A3 ;   dims = Ti ) \n 3×4×1 DimArray{Float64,3} with dimensions: X, Y, Ti\n[:, :, 1]\n 0.318537  0.460045  0.476323  0.720864\n 0.523719  0.41774   0.568858  0.601151\n 0.749818  0.608838  0.387426  0.334604\n"},{"id":237,"pagetitle":"Crash course - DimensionalData.jl","title":"Methods where dims, dim types, or  Symbol s can be used to indicate the array dimension: ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#methods-where-dims-dim-types-or-symbols-can-be-used-to-indicate-the-array-dimension","content":" Methods where dims, dim types, or  Symbol s can be used to indicate the array dimension: ¤ size ,  axes ,  firstindex ,  lastindex cat ,  reverse ,  dropdims reduce ,  mapreduce sum ,  prod ,  maximum ,  minimum , mean ,  median ,  extrema ,  std ,  var ,  cor ,  cov permutedims ,  adjoint ,  transpose ,  Transpose mapslices ,  eachslice"},{"id":238,"pagetitle":"Crash course - DimensionalData.jl","title":"LookupArrays and Selectors ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#lookuparrays-and-selectors","content":" LookupArrays and Selectors ¤ Indexing by value in  DimensionalData  is done with  Selectors . IntervalSets.jl is now used for selecting ranges of values (formerly  Between ). Selector Description At(x) get the index exactly matching the passed in value(s) Near(x) get the closest index to the passed in value(s) Contains(x) get indices where the value x falls within an interval Where(f) filter the array axis by a function of the dimension index values. [ a..b ] get all indices between two values, inclusively. [ OpenInterval(a, b) ] get all indices between  a  and  b , exclusively. [ Interval{A,B}(a, b) ] get all indices between  a  and  b , as  :closed  or  :open . Selectors find indices in the  LookupArray , for each dimension. Here we use an  Interval  to select a range between integers and  DateTime : A [ X ( 12 .. 35 ),   Ti ( Date ( 2001 ,   5 ) .. Date ( 2001 ,   7 ))] \n 2×3 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 20:10:30 ForwardOrdered Regular Points,\n  Ti Sampled{DateTime} DateTime(\"2001-05-01T00:00:00\"):Month(1):DateTime(\"2001-07-01T00:00:00\") ForwardOrdered Regular Points\n      2001-05-01T00:00:00  …   2001-07-01T00:00:00\n 20  0.635234                             0.151172\n 30  0.918251                             0.519497\n Selectors can be used in  getindex ,  setindex!  and  view  to select indices matching the passed in value(s) We can use selectors inside dim wrappers, here selecting values from  DateTime  and  Int : using   Dates \n timespan   =   DateTime ( 2001 , 1 ) : Month ( 1 ) : DateTime ( 2001 , 12 ) \n A4   =   rand ( Ti ( timespan ),   X ( 10 : 10 : 100 )) \n A4 [ X ( Near ( 35 )),   Ti ( At ( DateTime ( 2001 , 5 )))] \n 0.8591691848803547\n Without dim wrappers selectors must be in the right order, and specify all axes: using   Unitful \n A5   =   rand ( Y (( 1 : 10 : 100 ) u \"m\" ),   Ti (( 1 : 5 : 100 ) u \"s\" )); \n A5 [ 10.5 u \"m\"   ..   50.5 u \"m\" ,   Near ( 23 u \"s\" )] \n 4-element DimArray{Float64,1} with dimensions: \n  Y Sampled{Quantity{Int64, 𝐋, Unitful.FreeUnits{(m,), 𝐋, nothing}}} (11:10:41) m ForwardOrdered Regular Points\nand reference dimensions: \n  Ti Sampled{Quantity{Int64, 𝐓, Unitful.FreeUnits{(s,), 𝐓, nothing}}} (21:5:21) s ForwardOrdered Regular Points\n 11 m  0.803593\n 21 m  0.144398\n 31 m  0.658241\n 41 m  0.905559\n We can also use Linear indices as in standard  Array : A5 [ 1 : 5 ] \n 5-element Vector{Float64}:\n 0.35593117975459065\n 0.8402600690853361\n 0.9816946414738246\n 0.6287005913342731\n 0.12152293840043726\n But unless the  DimArray  is one dimensional, this will return a regular  Array . It is not possible to keep the  LookupArray  or even  Dimension s after linear indexing is used."},{"id":239,"pagetitle":"Crash course - DimensionalData.jl","title":"LookupArrays and traits ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#lookuparrays-and-traits","content":" LookupArrays and traits ¤ Using a regular range or  Vector  as a lookup index has a number of downsides. We cannot use  searchsorted  for fast searches without knowing the order of the array, and this is slow to compute at runtime. It also means  reverse  or rotations cannot be used while keeping the  DimArray  wrapper. Step sizes are also a problem. Some ranges like  LinRange  lose their step size with a length of  1 . Often, instead of a range, multi-dimensional data formats provide a  Vector  of evenly spaced values for a lookup, with a step size specified separately. Converting to a range introduces floating point errors that means points may not be selected with  At  without setting tolerances. This means using a lookup wrapper with traits is more generally robust and versatile than simply using a range or vector. DimensionalData provides types for specifying details about the dimension index, in the  LookupArrays  sub-module: using   DimensionalData \n using   . LookupArrays \n The main  LookupArray  are : Sampled Categorical , NoLookup Each comes with specific traits that are either fixed or variable, depending on the contained index. These enable optimisations with  Selector s, and modified behaviours, such as: Selection of  Intervals  or  Points , which will give slightly different results for selectors like  ..  - as whole intervals are   selected, and have different  bounds  values. Tracking of lookup order. A reverse order is labelled  ReverseOrdered  and will still work with  searchsorted , and for plots to always be the right way   up when either the index or the array is backwards. Reversing a  DimArray    will reverse the  LookupArray  for that dimension, swapping  ReverseOrdered    to  ForwardOrdered . Sampled Intervals  can have index located at a  Locus  of: Start , Center End Which specifies the point of the interval represented in the index, to match different data standards, e.g. GeoTIFF ( Start ) and NetCDF ( Center ). A  Span  specifies the gap between  Points  or the size of Intervals . This may be: Regular , in the case of a range and equally spaced vector, Irregular  for unequally spaced vectors Explicit  for the case where all interval start and end points are specified explicitly - as is common in the NetCDF standard. These traits all for subtypes of  Aligned . Unaligned  also exists to handle dimensions with an index that is rotated or otherwise transformed in relation to the underlying array, such as  Transformed ."},{"id":240,"pagetitle":"Crash course - DimensionalData.jl","title":"LookupArray detection ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#lookuparray-detection","content":" LookupArray detection ¤ Aligned  types will be detected automatically if not specified - which usually isn't required. An empty  Dimension  or a  Type  or  Symbol  will be assigned  NoLookup  - this behaves as a simple named dimension without a lookup index. A  Dimension  containing and index of  String ,  Char ,  Symbol  or mixed types will be given the  Categorical  mode, A range will be assigned  Sampled , defaulting to  Regular ,  Points Other  AbstractVector  will be assigned  Sampled Irregular Points . In all cases the  Order  of  ForwardOrdered  or  ReverseOrdered  will be be detected, otherwise  Unordered  for an unsorted  Array . See the  LookupArray  API docs for more detail."},{"id":241,"pagetitle":"Crash course - DimensionalData.jl","title":"Referenced dimensions ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#referenced-dimensions","content":" Referenced dimensions ¤ The reference dimensions record the previous dimensions that an array was selected from. These can be use for plot labelling, and tracking array changes so that  cat  can reconstruct the lookup array from previous dimensions that have been sliced."},{"id":242,"pagetitle":"Crash course - DimensionalData.jl","title":"Warnings ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#warnings","content":" Warnings ¤ Indexing with unordered or reverse-ordered arrays has undefined behaviour. It will trash the dimension index, break  searchsorted  and nothing will make sense any more. So do it at you own risk. However, indexing with sorted vectors of  Int  can be useful, so it's allowed. But it may do strange things to interval sizes for  Intervals  that are not  Explicit . This selects the first 5 entries of the underlying array. In the case that  A  has only one dimension, it will be retained. Multidimensional  AbstracDimArray  indexed this way will return a regular array. This page was generated using  Literate.jl ."},{"id":245,"pagetitle":"Reference - DimensionalData.jl","title":"API ¤","ref":"/DimensionalData/stable/reference/#api","content":" API ¤"},{"id":246,"pagetitle":"Reference - DimensionalData.jl","title":"Arrays ¤","ref":"/DimensionalData/stable/reference/#arrays","content":" Arrays ¤ # DimensionalData.AbstractDimArray  —  Type . AbstractDimArray   <:   AbstractArray \n Abstract supertype for all \"dim\" arrays. These arrays return a  Tuple  of  Dimension  from a  dims  method, and can be rebuilt using  rebuild . parent  must return the source array. They should have  metadata ,  name  and  refdims  methods, although these are optional. A  rebuild  method for  AbstractDimArray  must accept  data ,  dims ,  refdims ,  name ,  metadata  arguments. Indexing  AbstractDimArray  with non-range  AbstractArray  has undefined effects on the  Dimension  index. Use forward-ordered arrays only\" source # DimensionalData.DimArray  —  Type . DimArray   <:   AbstractDimArray \n\n DimArray ( data ,   dims ,   refdims ,   name ,   metadata ) \n DimArray ( data ,   dims :: Tuple ;   refdims = (),   name = NoName (),   metadata = NoMetadata ()) \n The main concrete subtype of  AbstractDimArray . DimArray  maintains and updates its  Dimension s through transformations and moves dimensions to reference dimension  refdims  after reducing operations (like e.g.  mean ). Arguments data : An  AbstractArray . dims : A  Tuple  of  Dimension name : A string name for the array. Shows in plots and tables. refdims : refence dimensions. Usually set programmatically to track past   slices and reductions of dimension for labelling and reconstruction. metadata :  Dict  or  Metadata  object, or  NoMetadata() Indexing can be done with all regular indices, or with  Dimension s and/or  Selector s.  Indexing  AbstractDimArray  with non-range  AbstractArray  has undefined effects on the  Dimension  index. Use forward-ordered arrays only\" Example: using   Dates ,   DimensionalData \n\n ti   =   ( Ti ( DateTime ( 2001 ) : Month ( 1 ) : DateTime ( 2001 , 12 )), \n x   =   X ( 10 : 10 : 100 )) \n A   =   DimArray ( rand ( 12 , 10 ),   ( ti ,   x ),   \"example\" ) \n\n julia >   A [ X ( Near ([ 12 ,   35 ])),   Ti ( At ( DateTime ( 2001 , 5 )))]; \n\n julia >   A [ Near ( DateTime ( 2001 ,   5 ,   4 )),   Between ( 20 ,   50 )]; \n source"},{"id":247,"pagetitle":"Reference - DimensionalData.jl","title":"Multi-array datasets ¤","ref":"/DimensionalData/stable/reference/#multi-array-datasets","content":" Multi-array datasets ¤ # DimensionalData.AbstractDimStack  —  Type . AbstractDimStack \n Abstract supertype for dimensional stacks. These have multiple layers of data, but share dimensions. Notably, their behaviour lies somewhere between a  DimArray  and a  NamedTuple : indexing with a  Symbol  as in  dimstack[:symbol]  returns a  DimArray  layer. iteration amd  map  are apply over array layers, as indexed with a  Symbol . getindex  and many base methods are applied as for  DimArray  - to avoid the need    to allways use  map . This design gives very succinct code when working with many-layered, mixed-dimension objects.  But it may be jarring initially - the most surprising outcome is that  dimstack[1]  will return a  NamedTuple  of values for the first index in all layers, while  first(dimstack)  will return the first value of the iterator - the  DimArray  for the first layer. See  DimStack  for the concrete implementation. Most methods are defined on the abstract type. To extend  AbstractDimStack , implement argument and keyword version of   rebuild  and also  rebuild_from_arrays . The constructor of an  AbstractDimStack  must accept a  NamedTuple . source # DimensionalData.DimStack  —  Type . DimStack   <:   AbstractDimStack \n\n DimStack ( data :: AbstractDimArray ... ) \n DimStack ( data :: Tuple { Vararg { AbstractDimArray }}) \n DimStack ( data :: NamedTuple { Keys , Vararg { AbstractDimArray }}) \n DimStack ( data :: NamedTuple ,   dims :: DimTuple ;   metadata = NoMetadata ()) \n DimStack holds multiple objects sharing some dimensions, in a  NamedTuple . Notably, their behaviour lies somewhere between a  DimArray  and a  NamedTuple : indexing with a  Symbol  as in  dimstack[:symbol]  returns a  DimArray  layer. iteration amd  map  are apply over array layers, as indexed with a  Symbol . getindex  or  view  with  Int ,  Dimension s or  Selector s that resolve to  Int  will   return a  NamedTuple  of values from each layer in the stack.   This has very good performace, and avoids the need to always use  map . getindex  or  view  with a  Vector  or  Colon  will return another  DimStack  where   all data layers have been sliced. setindex!  must pass a  Tuple  or  NamedTuple  maching the layers. many base and  Statistics  methods ( sum ,  mean  etc) will work as for a  DimArray    again removing the need to use  map . For example, here we take the mean over the time dimension for all layers : mean ( mydimstack ;   dims = Ti ) \n And this equivalent to: map ( A   ->   mean ( A ;   dims = Ti ),   mydimstack ) \n This design gives succinct code when working with many-layered, mixed-dimension objects.  But it may be jarring initially - the most surprising outcome is that  dimstack[1]  will return a  NamedTuple  of values for the first index in all layers, while  first(dimstack)  will return the first value of the iterator - the  DimArray  for the first layer. DimStack  can be constructed from multiple  AbstractDimArray  or a  NamedTuple  of  AbstractArray  and a matching  dims  tuple. Most  Base  and  Statistics  methods that apply to  AbstractArray  can be used on all layers of the stack simulataneously. The result is a  DimStack , or a  NamedTuple  if methods like  mean  are used without  dims  arguments, and return a single non-array value. Example julia>   using   DimensionalData \n\n julia>   A   =   [ 1.0   2.0   3.0 ;   4.0   5.0   6.0 ]; \n\n julia>   dimz   =   ( X ([ :a ,   :b ]),   Y ( 10.0 : 10.0 : 30.0 )) \n X Symbol[:a, :b], \n Y 10.0:10.0:30.0 \n\n julia>   da1   =   DimArray ( 1 A ,   dimz ;   name = :one ); \n\n julia>   da2   =   DimArray ( 2 A ,   dimz ;   name = :two ); \n\n julia>   da3   =   DimArray ( 3 A ,   dimz ;   name = :three ); \n\n julia>   s   =   DimStack ( da1 ,   da2 ,   da3 ); \n\n julia>   s [ At ( :b ),   At ( 10.0 )] \n (one = 4.0, two = 8.0, three = 12.0) \n\n julia>   s [ X ( At ( :a ))]   isa   DimStack \n true \n source"},{"id":248,"pagetitle":"Reference - DimensionalData.jl","title":"Dimension indices generators ¤","ref":"/DimensionalData/stable/reference/#dimension-indices-generators","content":" Dimension indices generators ¤ # DimensionalData.DimIndices  —  Type . DimIndices   <:   AbstractArray \n\n DimIndices ( x ) \n DimIndices ( dims :: Tuple ) \n DimIndices ( dims :: Dimension ) \n Like  CartesianIndices , but for  Dimension s. Behaves as an  Array  of  Tuple  of  Dimension(i)  for all combinations of the axis indices of  dims . This can be used to view/index into arbitrary dimensions over an array, and is especially useful when combined with  otherdims , to iterate over the indices of unknown dimension. source # DimensionalData.DimKeys  —  Type . DimKeys   <:   AbstractArray \n\n DimKeys ( x ) \n DimKeys ( dims :: Tuple ) \n DimKeys ( dims :: Dimension ) \n Like  CartesianIndices , but for the lookup values of Dimensions. Behaves as an  Array  of  Tuple  of  Dimension(At(lookupvalue))  for all combinations of the lookup values of  dims . source # DimensionalData.DimPoints  —  Type . DimPoints   <:   AbstractArray \n\n DimPoints ( x ;   order ) \n DimPoints ( dims :: Tuple ;   order ) \n DimPoints ( dims :: Dimension ;   order ) \n Like  CartesianIndices , but for the point values of the dimension index.  Behaves as an  Array  of  Tuple  lookup values (whatever they are) for all combinations of the lookup values of  dims . Either a  Dimension , a  Tuple  of  Dimension  or an object that defines a  dims  method can be passed in. Keywords order : determines the order of the points, the same as the order of  dims  by default. source"},{"id":249,"pagetitle":"Reference - DimensionalData.jl","title":"Tables.jl/TableTraits.jl interface ¤","ref":"/DimensionalData/stable/reference/#tablesjltabletraitsjl-interface","content":" Tables.jl/TableTraits.jl interface ¤ # DimensionalData.AbstractDimTable  —  Type . AbstractDimTable   <:   Tables . AbstractColumns \n Abstract supertype for dim tables source # DimensionalData.DimTable  —  Type . DimTable   <:   AbstractDimTable \n\n DimTable ( A :: AbstractDimArray ) \n Construct a Tables.jl/TableTraits.jl compatible object out of an  AbstractDimArray . This table will have a column for the array data and columns for each  Dimension  index, as a [ DimColumn ]. These are lazy, and generated as required. Column names are converted from the dimension types using  DimensionalData.dim2key . This means type  Ti  becomes the column name  :Ti , and  Dim{:custom}  becomes  :custom . To get dimension columns, you can index with  Dimension  ( X() ) or  Dimension  type ( X ) as well as the regular  Int  or  Symbol . source # DimensionalData.DimColumn  —  Type . DimColumn { T , D <: Dimension }   <:   AbstractVector { T } \n\n DimColumn ( dim :: Dimension ,   dims :: Tuple { Vararg { DimTuple }}) \n DimColumn ( dim :: DimColumn ,   length :: Int ,   dimstride :: Int ) \n A table column based on a  Dimension  and it's relationship with other  Dimension s in  dims . length  is the product of all dim lengths (usually the length of the corresponding array data), while stride is the product of the preceding dimension lengths, which may or may not be the real stride of the corresponding array depending on the data type. For  A isa Array , the  dimstride  will match the  stride . When the second argument is a  Tuple  of  Dimension , the  length  and  dimstride  fields are calculated from the dimensions, relative to the column dimension  dim . This object will be returned as a column of  DimTable . source"},{"id":250,"pagetitle":"Reference - DimensionalData.jl","title":"Common methods ¤","ref":"/DimensionalData/stable/reference/#common-methods","content":" Common methods ¤ Common functions for obtaining information from objects: # DimensionalData.Dimensions.dims  —  Function . dims ( x ,   [ dims :: Tuple ])   =>   Tuple { Vararg { Dimension }} \n dims ( x ,   dim )   =>   Dimension \n Return a tuple of  Dimension s for an object, in the order that matches the axes or columns of the underlying data. dims  can be  Dimension ,  Dimension  types, or  Symbols  for  Dim{Symbol} . The default is to return  nothing . source # DimensionalData.Dimensions.refdims  —  Function . refdims ( x ,   [ dims :: Tuple ])   =>   Tuple { Vararg { Dimension }} \n refdims ( x ,   dim )   =>   Dimension \n Reference dimensions for an array that is a slice or view of another array with more dimensions. slicedims(a, dims)  returns a tuple containing the current new dimensions and the new reference dimensions. Refdims can be stored in a field or disgarded, as it is mostly to give context to plots. Ignoring refdims will simply leave some captions empty. The default is to return an empty  Tuple () . source # DimensionalData.Dimensions.LookupArrays.metadata  —  Function . metadata ( x )   =>   ( object   metadata ) \n metadata ( x ,   dims :: Tuple )    =>   Tuple   ( Dimension   metadata ) \n metadata ( xs :: Tuple )   =>   Tuple \n Returns the metadata for an object or for the specified dimension(s) Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.name  —  Function . name ( x )   =>   Symbol \n name ( xs : Tuple )   =>   NTuple { N , Symbol } \n name ( x ,   dims :: Tuple )   =>   NTuple { N , Symbol } \n name ( x ,   dim )   =>   Symbol \n Get the name of an array or Dimension, or a tuple of of either as a Symbol. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source Utility methods for transforming DimensionalData objects: # DimensionalData.Dimensions.LookupArrays.set  —  Function . set ( x ,   val ) \n set ( x ,   args :: Pairs ... )   =>   x   with   updated   field / s \n set ( x ,   args ... ;   kw ... )   =>   x   with   updated   field / s \n set ( x ,   args :: Tuple { Vararg { Dimension }};   kw ... )   =>   x   with   updated   field / s \n\n set ( dim :: Dimension ,   index :: AbstractArray )   =>   Dimension \n set ( dim :: Dimension ,   lookup :: LookupArray )   =>   Dimension \n set ( dim :: Dimension ,   lookupcomponent :: LookupArrayTrait )   =>   Dimension \n set ( dim :: Dimension ,   metadata :: AbstractMetadata )   =>   Dimension \n Set the properties of an object, its internal data or the traits of its dimensions and lookup index. As DimensionalData is so strongly typed you do not need to specify what field of a  LookupArray  to  set  - there is no ambiguity. To set fields of a  LookupArray  you need to specify the dimension. This can be done using  X => val  pairs,  X = val  keyword arguments, or  X(val)  wrapped arguments. When a  Dimension  or  LookupArray  is passed to  set  to replace the existing ones, fields that are not set will keep their original values. Notes: Changing a lookup index range/vector will also update the step size and order where applicable. Setting the  Order  like  ForwardOrdered  will  not  reverse the array or dimension to match. Use  reverse  and  reorder  to do this. Examples julia>   using   DimensionalData ;   const   DD   =   DimensionalData \n DimensionalData \n\n julia>   da   =   DimArray ( zeros ( 3 ,   4 ),   ( custom = 10.0 : 010.0 : 30.0 ,   Z =- 20 : 010.0 : 10.0 )); \n\n julia>   set ( da ,   ones ( 3 ,   4 )) \n 3×4 DimArray{Float64,2} with dimensions: \n   Dim{:custom} Sampled{Float64} 10.0:10.0:30.0 ForwardOrdered Regular Points, \n   Z Sampled{Float64} -20.0:10.0:10.0 ForwardOrdered Regular Points \n        -20.0  -10.0  0.0  10.0 \n  10.0    1.0    1.0  1.0   1.0 \n  20.0    1.0    1.0  1.0   1.0 \n  30.0    1.0    1.0  1.0   1.0  \n Change the  Dimension  wrapper type: julia>   set ( da ,   :Z   =>   Ti ,   :custom   =>   Z ) \n 3×4 DimArray{Float64,2} with dimensions: \n   Z Sampled{Float64} 10.0:10.0:30.0 ForwardOrdered Regular Points, \n   Ti Sampled{Float64} -20.0:10.0:10.0 ForwardOrdered Regular Points \n        -20.0  -10.0  0.0  10.0 \n  10.0    0.0    0.0  0.0   0.0 \n  20.0    0.0    0.0  0.0   0.0 \n  30.0    0.0    0.0  0.0   0.0  \n Change the lookup  Vector : julia>   set ( da ,   Z   =>   [ :a ,   :b ,   :c ,   :d ],   :custom   =>   [ 4 ,   5 ,   6 ]) \n 3×4 DimArray{Float64,2} with dimensions: \n   Dim{:custom} Sampled{Int64} Int64[4, 5, 6] ForwardOrdered Regular Points, \n   Z Sampled{Symbol} Symbol[:a, :b, :c, :d] ForwardOrdered Regular Points \n      :a   :b   :c   :d \n  4  0.0  0.0  0.0  0.0 \n  5  0.0  0.0  0.0  0.0 \n  6  0.0  0.0  0.0  0.0 \n Change the  LookupArray  type: julia>   set ( da ,   Z = DD . NoLookup (),   custom = DD . Sampled ()) \n 3×4 DimArray{Float64,2} with dimensions: \n   Dim{:custom} Sampled{Float64} 10.0:10.0:30.0 ForwardOrdered Regular Points, \n   Z \n  10.0  0.0  0.0  0.0  0.0 \n  20.0  0.0  0.0  0.0  0.0 \n  30.0  0.0  0.0  0.0  0.0 \n Change the  Sampling  trait: julia>   set ( da ,   :custom   =>   DD . Irregular ( 10 ,   12 ),   Z   =>   DD . Regular ( 9.9 )) \n 3×4 DimArray{Float64,2} with dimensions: \n   Dim{:custom} Sampled{Float64} 10.0:10.0:30.0 ForwardOrdered Irregular Points, \n   Z Sampled{Float64} -20.0:10.0:10.0 ForwardOrdered Regular Points \n        -20.0  -10.0  0.0  10.0 \n  10.0    0.0    0.0  0.0   0.0 \n  20.0    0.0    0.0  0.0   0.0 \n  30.0    0.0    0.0  0.0   0.0 \n source # DimensionalData.Dimensions.LookupArrays.rebuild  —  Function . rebuild ( x ,   args ... ) \n rebuild ( x ;   kw ... ) \n Rebuild an object struct with updated field values. x  can be a  AbstractDimArray , a  Dimension ,  LookupArray  or other custom types. This is an abstraction that alows inbuilt and custom types to be rebuilt to update their fields, as most objects in DimensionalData.jl are immutable. The arguments version can be concise but depends on a fixed order defined for some DimensionalData objects. It should be defined based on the object type in DimensionalData, adding the fields specific to your object. The keyword version ignores order, and is mostly automated  using  ConstructionBase.setproperties . It should only be defined if your object has  missing fields or fields with different names to DimensionalData objects. The arguments required are defined for the abstract type that has a  rebuild  method. source # DimensionalData.modify  —  Function . modify ( f ,   A :: AbstractDimArray )   =>   AbstractDimArray \n modify ( f ,   s :: AbstractDimStack )   =>   AbstractDimStack \n modify ( f ,   dim :: Dimension )   =>   Dimension \n modify ( f ,   x ,   lookupdim :: Dimension )   =>   typeof ( x ) \n Modify the parent data, rebuilding the object wrapper without change.  f  must return a  AbstractArray  of the same size as the original. This method is mostly useful as a way of swapping the parent array type of an object. Example If we have a previously-defined  DimArray , we can copy it to an Nvidia GPU with: A   =   DimArray ( rand ( 100 ,   100 ),   ( X ,   Y )) \n modify ( CuArray ,   A ) \n This also works for all the data layers in a  DimStack . source # DimensionalData.broadcast_dims  —  Function . broadcast_dims ( f ,   sources :: AbstractDimArray ... )   =>   AbstractDimArray \n Broadcast function  f  over the  AbstractDimArray s in  sources , permuting and reshaping dimensions to match where required. The result will contain all the dimensions in  all passed in arrays in the order in which they are found. Arguments sources :  AbstractDimArrays  to broadcast over with  f . This is like broadcasting over every slice of  A  if it is sliced by the dimensions of  B . source # DimensionalData.broadcast_dims!  —  Function . broadcast_dims! ( f ,   dest :: AbstractDimArray ,   sources :: AbstractDimArray ... )   =>   dest \n Broadcast function  f  over the  AbstractDimArray s in  sources , writing to  dest .   sources  are permuting and reshaping dimensions to match where required. The result will contain all the dimensions in all passed in arrays, in the order in which they are found. Arguments dest :  AbstractDimArray  to update. sources :  AbstractDimArrays  to broadcast over with  f . source # DimensionalData.reorder  —  Function . reorder ( A :: AbstractDimArray ,   order :: Pair )   =>   AbstractDimArray \n reorder ( A :: Dimension ,   order :: Order )   =>   AbstractDimArray \n Reorder every dims index/array to  order , or reorder index for the the given dimension(s) to the  Order  they wrap. order  can be an  Order , or  Dimeension => Order  pairs. If no axis reversal is required the same objects will be returned, without allocation. source # Base.cat  —  Function . Base . cat ( stacks :: AbstractDimStack ... ;   [ keys = keys ( stacks [ 1 ])],   dims ) \n Concatenate all or a subset of layers for all passed in stacks. Keywords keys :  Tuple  of  Symbol  for the stack keys to concatenate. dims : Dimension of child array to concatenate on. Example Concatenate the :sea surface temp and :humidity layers in the time dimension: cat ( stacks ... ;   keys = ( :sea_surface_temp ,   :humidity ),   dims = Ti ) \n source # Base.map  —  Function . Base . map ( f ,   stacks :: AbstractDimStack ... ) \n Apply function  f  to each layer of the  stacks . If  f  returns  DimArray s the result will be another  DimStack . Other values will be returned in a  NamedTuple . source # Base.copy!  —  Function . Base . copy! ( dst :: AbstractArray ,   src :: AbstractGimStack ,   key :: Key ) \n Copy the stack layer  key  to  dst , which can be any  AbstractArray . Example Copy the  :humidity  layer from  stack  to  array . copy! ( array ,   stack ,   :humidity ) \n source Base.copy!(dst::AbstractDimStack, src::AbstractDimStack, [keys=keys(dst)])\n Copy all or a subset of layers from one stack to another. Example Copy just the  :sea_surface_temp  and  :humidity  layers from  src  to  dst . copy! ( dst :: AbstractDimStack ,   src :: AbstractDimStack ,   keys = ( :sea_surface_temp ,   :humidity )) \n source # Base.eachslice  —  Function . Base . eachslice ( stack :: AbstractDimStack ;   dims ) \n Create a generator that iterates over dimensions  dims  of  stack , returning stacks that select all the data from the other dimensions in  stack  using views. The generator has  size  and  axes  equivalent to those of the provided  dims . Examples julia >   ds   =   DimStack (( \n             x = DimArray ( randn ( 2 ,   3 ,   4 ),   ( X ([ :x1 ,   :x2 ]),   Y ( 1 : 3 ),   Z )), \n             y = DimArray ( randn ( 2 ,   3 ,   5 ),   ( X ([ :x1 ,   :x2 ]),   Y ( 1 : 3 ),   Ti )) \n         )); \n\n julia >   slices   =   eachslice ( ds ;   dims = ( Z ,   X )); \n\n julia >   size ( slices ) \n ( 4 ,   2 ) \n\n julia >   map ( dims ,   axes ( slices )) \n Z , \n X   Categorical { Symbol }   Symbol [ x1 ,   x2 ]   ForwardOrdered \n\n julia >   first ( slices ) \n DimStack   with   dimensions : \n    Y   Sampled { Int64 }   1 : 3   ForwardOrdered   Regular   Points , \n    Ti \n and   2   layers : \n    :x   Float64   dims :   Y   ( 3 ) \n    :y   Float64   dims :   Y ,   Ti   ( 3 × 5 ) \n source Most base methods work as expected, using  Dimension  wherever a  dims  keyword is used. They are not allspecifically documented here. Shorthand constructors: # Base.fill  —  Function . Base . fill ( x ,   dims :: Dimension ... ;   kw ... )   =>   DimArray \n Base . fill ( x ,   dims :: Tuple { Vararg { Dimension }};   kw ... )   =>   DimArray \n Create a  DimArray  with a fill value of  x . There are two kinds of  Dimension  value acepted: A  Dimension  holding an  AbstractVector  will set the dimension index to  that  AbstractVector , and detect the dimension lookup. A  Dimension  holding an  Integer  will set the length of the axis, and set the dimension lookup to  NoLookup . Keywords are the same as for  DimArray . Example ```@doctest\njulia> using DimensionalData julia> rand(Bool, X(2), Y(4))\n2×4 DimArray{Bool,2} with dimensions: X, Y\n 1  0  0  1\n 1  0  1  1\n <a target='_blank' href='https://github.com/rafaqz/DimensionalData.jl/blob/1006c83dcde704a6aa9f741ba953fa970c1acb81/src/array/array.jl#L373-L396' class='documenter-source'>source</a><br>\n\n<a id='Base.rand' href='#Base.rand'>#</a>\n**`Base.rand`** &mdash; *Function*.\n\n\n\n```julia\nBase.rand(x, dims::Dimension...; kw...) => DimArray\nBase.rand(x, dims::Tuple{Vararg{Dimension}}; kw...) => DimArray\nBase.rand(r::AbstractRNG, x, dims::Tuple{Vararg{Dimension}}; kw...) => DimArray\nBase.rand(r::AbstractRNG, x, dims::Dimension...; kw...) => DimArray\n Create a  DimArray  of random values. There are two kinds of  Dimension  value acepted: A  Dimension  holding an  AbstractVector  will set the dimension index to  that  AbstractVector , and detect the dimension lookup. A  Dimension  holding an  Integer  will set the length of the axis, and set the dimension lookup to  NoLookup . Keywords are the same as for  DimArray . Example julia >   using   DimensionalData \n\n julia >   rand ( Bool ,   X ( 2 ),   Y ( 4 )) \n 2 × 4   DimArray { Bool , 2 }   with   dimensions :   X ,   Y \n   1    0    0    1 \n   1    0    1    1 \n\n julia >   rand ( X ([ :a ,   :b ,   :c ]),   Y ( 100.0 : 50 : 200.0 )) \n 3 × 3   DimArray { Float64 , 2 }   with   dimensions : \n    X :   Symbol [ a ,   b ,   c ]   Categorical :   Unordered , \n    Y :   100.0 : 50.0 : 200.0   Sampled :   Ordered   Regular   Points \n   0.43204     0.835111    0.624231 \n   0.752868    0.471638    0.193652 \n   0.484558    0.846559    0.455256 \n source # Base.zeros  —  Function . Base . zeros ( x ,   dims :: Dimension ... ;   kw ... )   =>   DimArray \n Base . zeros ( x ,   dims :: Tuple { Vararg { Dimension }};   kw ... )   =>   DimArray \n Create a  DimArray  of zeros. There are two kinds of  Dimension  value acepted: A  Dimension  holding an  AbstractVector  will set the dimension index to  that  AbstractVector , and detect the dimension lookup. A  Dimension  holding an  Integer  will set the length of the axis, and set the dimension lookup to  NoLookup . Keywords are the same as for  DimArray . Example ```@doctest\njulia> using DimensionalData julia> zeros(Bool, X(2), Y(4))\n2×4 DimArray{Bool,2} with dimensions: X, Y\n 0  0  0  0\n 0  0  0  0 julia> zeros(X([:a, :b, :c]), Y(100.0:50:200.0))\n3×3 DimArray{Float64,2} with dimensions:\n  X: Symbol[a, b, c] Categorical: Unordered,\n  Y: 100.0:50.0:200.0 Sampled: Ordered Regular Points\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n <a target='_blank' href='https://github.com/rafaqz/DimensionalData.jl/blob/1006c83dcde704a6aa9f741ba953fa970c1acb81/src/array/array.jl#L435-L466' class='documenter-source'>source</a><br>\n\n<a id='Base.ones' href='#Base.ones'>#</a>\n**`Base.ones`** &mdash; *Function*.\n\n\n\n```julia\nBase.ones(x, dims::Dimension...; kw...) => DimArray\nBase.ones(x, dims::Tuple{Vararg{Dimension}}; kw...) => DimArray\n Create a  DimArray  of ones. There are two kinds of  Dimension  value acepted: A  Dimension  holding an  AbstractVector  will set the dimension index to  that  AbstractVector , and detect the dimension lookup. A  Dimension  holding an  Integer  will set the length of the axis, and set the dimension lookup to  NoLookup . Keywords are the same as for  DimArray . Example ```@doctest\njulia> using DimensionalData julia> ones(Bool, X(2), Y(4))\n2×4 DimArray{Bool,2} with dimensions: X, Y\n 1  1  1  1\n 1  1  1  1 julia> ones(X([:a, :b, :c]), Y(100.0:50:200.0))\n3×3 DimArray{Float64,2} with dimensions:\n  X: Symbol[a, b, c] Categorical: Unordered,\n  Y: 100.0:50.0:200.0 Sampled: Ordered Regular Points\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n <a target='_blank' href='https://github.com/rafaqz/DimensionalData.jl/blob/1006c83dcde704a6aa9f741ba953fa970c1acb81/src/array/array.jl#L469-L500' class='documenter-source'>source</a><br>\n\n\n<a id='Dimensions'></a>\n\n<a id='Dimensions-1'></a>\n\n# Dimensions\n\n\nHandling of Dimensions is kept in a sub-module `Dimensions`.\n\n<a id='DimensionalData.Dimensions' href='#DimensionalData.Dimensions'>#</a>\n**`DimensionalData.Dimensions`** &mdash; *Module*.\n\n\n\nDimensions\n\nSub-module for [`Dimension`](reference.md#DimensionalData.Dimensions.Dimension)s wrappers, and operations on them used in DimensionalData.jl.\n\nTo load `Dimensions` types and methods into scope:\n\n```julia\nusing DimensionalData\nusing DimensionalData.Dimensions\n source Dimensions have a type-heirarchy that organises plotting and dimension matching. # DimensionalData.Dimensions.Dimension  —  Type . Dimension \n Abstract supertype of all dimension types. Example concrete implementations are  X ,  Y ,  Z ,  Ti  (Time), and the custom [ Dim ]@ref) dimension. Dimension s label the axes of an  AbstractDimArray , or other dimensional objects, and are used to index into the array. They may also provide an alternate index to lookup for each array axis. This may be any  AbstractVector  matching the array axis length, or a  Val  holding a tuple for compile-time index lookups. Dimension s also have  lookup  and  metadata  fields. lookup  gives more details about the dimension, such as that it is  Categorical  or  Sampled  as  Points  or  Intervals  along some transect. DimensionalData will attempt to guess the lookup from the passed-in index value. Example: using   DimensionalData ,   Dates \n\n x   =   X ( 2 : 2 : 10 ) \n y   =   Y ([ 'a' ,   'b' ,   'c' ]) \n ti   =   Ti ( DateTime ( 2021 ,   1 ) : Month ( 1 ) : DateTime ( 2021 ,   12 )) \n\n A   =   DimArray ( zeros ( 3 ,   5 ,   12 ),   ( y ,   x ,   ti )) \n\n # output \n\n 3 × 5 × 12   DimArray { Float64 , 3 }   with   dimensions : \n    Y   Categorical { Char }   Char [ 'a' ,   'b' ,   'c' ]   ForwardOrdered , \n    X   Sampled { Int64 }   2 : 2 : 10   ForwardOrdered   Regular   Points , \n    Ti   Sampled { DateTime }   DateTime ( \"2021-01-01T00:00:00\" ) : Month ( 1 ) : DateTime ( \"2021-12-01T00:00:00\" )   ForwardOrdered   Regular   Points \n [ : ,   : ,   1 ] \n         2      4      6      8      10 \n    'a'    0.0    0.0    0.0    0.0     0.0 \n    'b'    0.0    0.0    0.0    0.0     0.0 \n    'c'    0.0    0.0    0.0    0.0     0.0 \n [ and   11   more   slices ... ] \n For simplicity, the same  Dimension  types are also used as wrappers in  getindex , like: x   =   A [ X ( 2 ),   Y ( 3 )] \n\n # output \n\n 12 - element   DimArray { Float64 , 1 }   with   dimensions : \n    Ti   Sampled { DateTime }   DateTime ( \"2021-01-01T00:00:00\" ) : Month ( 1 ) : DateTime ( \"2021-12-01T00:00:00\" )   ForwardOrdered   Regular   Points \n and   reference   dimensions : \n    Y   Categorical { Char }   Char [ 'c' ]   ForwardOrdered , \n    X   Sampled { Int64 }   4 : 2 : 4   ForwardOrdered   Regular   Points \n   2021 - 01 - 01 T00 : 00 : 00    0.0 \n   2021 - 02 - 01 T00 : 00 : 00    0.0 \n   2021 - 03 - 01 T00 : 00 : 00    0.0 \n   2021 - 04 - 01 T00 : 00 : 00    0.0 \n   ⋮ \n   2021 - 10 - 01 T00 : 00 : 00    0.0 \n   2021 - 11 - 01 T00 : 00 : 00    0.0 \n   2021 - 12 - 01 T00 : 00 : 00    0.0 \n A  Dimension  can also wrap  Selector . x   =   A [ X ( Between ( 3 ,   4 )),   Y ( At ( 'b' ))] \n\n # output \n\n 1 × 12   DimArray { Float64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   4 : 2 : 4   ForwardOrdered   Regular   Points , \n    Ti   Sampled { DateTime }   DateTime ( \"2021-01-01T00:00:00\" ) : Month ( 1 ) : DateTime ( \"2021-12-01T00:00:00\" )   ForwardOrdered   Regular   Points \n and   reference   dimensions : \n    Y   Categorical { Char }   Char [ 'b' ]   ForwardOrdered \n       2021 - 01 - 01 T00 : 00 : 00    …     2021 - 12 - 01 T00 : 00 : 00 \n   4    0.0                                    0.0 \n Dimension  objects may have  lookup  and  metadata  fields to track additional information about the data and the index, and their relationship. source # DimensionalData.Dimensions.DependentDim  —  Type . DependentDim   <:   Dimension \n Abstract supertype for Dependent dimensions. These will plot on the Y axis. source # DimensionalData.Dimensions.IndependentDim  —  Type . IndependentDim   <:   Dimension \n Abstract supertype for independent dimensions. Thise will plot on the X axis. source # DimensionalData.Dimensions.XDim  —  Type . XDim   <:   IndependentDim \n Abstract supertype for all X dimensions. source # DimensionalData.Dimensions.YDim  —  Type . YDim   <:   DependentDim \n Abstract supertype for all Y dimensions. source # DimensionalData.Dimensions.ZDim  —  Type . ZDim   <:   DependentDim \n Abstract supertype for all Z dimensions. source # DimensionalData.Dimensions.TimeDim  —  Type . TimeDim   <:   IndependentDim \n Abstract supertype for all time dimensions. In a  TimeDime  with  Interval  sampling the locus will automatically be set to  Start() . Dates and times generally refer to the start of a month, hour, second etc., not the central point as is more common with spatial data. ` source # DimensionalData.Dimensions.X  —  Type . X   <:   XDim \n\n X ( val =: ) \n X  Dimension .  X <: XDim <: IndependentDim Example: xdim   =   X ( 2 : 2 : 10 ) \n # Or \n val   =   A [ X ( 1 )] \n # Or \n mean ( A ;   dims = X ) \n source # DimensionalData.Dimensions.Y  —  Type . Y   <:   YDim \n\n Y ( val =: ) \n Y  Dimension .  Y <: YDim <: DependentDim Example: ydim   =   Y ([ 'a' ,   'b' ,   'c' ]) \n # Or \n val   =   A [ Y ( 1 )] \n # Or \n mean ( A ;   dims = Y ) \n source # DimensionalData.Dimensions.Z  —  Type . Z   <:   ZDim \n\n Z ( val =: ) \n Z  Dimension .  Z <: ZDim <: Dimension Example: zdim   =   Z ( 10 : 10 : 100 ) \n # Or \n val   =   A [ Z ( 1 )] \n # Or \n mean ( A ;   dims = Z ) \n source # DimensionalData.Dimensions.Ti  —  Type . m     Ti <: TimeDim Ti(val=:)\n Time  Dimension .  Ti <: TimeDim <: IndependentDim Time  is already used by Dates, and  T  is a common type parameter, We use  Ti  to avoid clashes. Example: timedim   =   Ti ( DateTime ( 2021 ,   1 ) : Month ( 1 ) : DateTime ( 2021 ,   12 )) \n # Or \n val   =   A [ Ti ( 1 )] \n # Or \n mean ( A ;   dims = Ti ) \n source # DimensionalData.Dimensions.Dim  —  Type . Dim { S }( val =: ) \n A generic dimension. For use when custom dims are required when loading data from a file. Can be used as keyword arguments for indexing. Dimension types take precedence over same named  Dim  types when indexing with symbols, or e.g. creating Tables.jl keys. using   DimensionalData \n\n dim   =   Dim { :custom }([ 'a' ,   'b' ,   'c' ]) \n\n # output \n\n Dim { :custom }   Char [ 'a' ,   'b' ,   'c' ] \n source # DimensionalData.Dimensions.Coord  —  Type . Coord   <:   Dimension \n A coordinate dimension itself holds dimensions. This allows combining point data with other dimensions, such as time. Example julia >   using   DimensionalData \n\n julia >   dim   =   Coord ([( 1.0 , 1.0 , 1.0 ),   ( 1.0 , 2.0 , 2.0 ),   ( 3.0 , 4.0 , 4.0 ),   ( 1.0 , 3.0 , 4.0 )],   ( X (),   Y (),   Z ())) \n Coord   :: \n    val :   Tuple { Float64 ,   Float64 ,   Float64 }[( 1.0 ,   1.0 ,   1.0 ),   ( 1.0 ,   2.0 ,   2.0 ),   ( 3.0 ,   4.0 ,   4.0 ),   ( 1.0 ,   3.0 , \n 4.0 )] \n    lookup :   MergedLookup \n Coord { Vector { Tuple { Float64 ,   Float64 ,   Float64 }},   DimensionalData . MergedLookup { Tuple { X { Colon ,   AutoLookup { Auto \n Order },   NoMetadata },   Y { Colon ,   AutoLookup { AutoOrder },   NoMetadata },   Z { Colon ,   AutoLookup { AutoOrder },   NoMetada \n ta }}},   NoMetadata } \n\n julia >   da   =   DimArray ( 0.1 : 0.1 : 0.4 ,   dim ) \n 4 - element   DimArray { Float64 , 1 }   with   dimensions : \n    Coord   () :   Tuple { Float64 ,   Float64 ,   Float64 }[( 1.0 ,   1.0 ,   1.0 ),   ( 1.0 ,   2.0 ,   2.0 ),   ( 3.0 ,   4.0 ,   4.0 ),   ( 1.0 , \n 3.0 ,   4.0 )] \n      MergedLookup \n   0.1 \n   0.2 \n   0.3 \n   0.4 \n\n julia >   da [ Coord ( Z ( At ( 1.0 )),   Y ( Between ( 1 ,   3 )))] \n 1 - element   DimArray { Float64 , 1 }   with   dimensions : \n    Coord   () :   Tuple { Float64 ,   Float64 ,   Float64 }[( 1.0 ,   1.0 ,   1.0 )]   MergedLookup \n   0.1 \n\n julia >   da [ Coord ( 4 )]   ==   0.4 \n true \n\n julia >   da [ Coord ( Between ( 1 ,   5 ),   : ,   At ( 4.0 ))] \n 2 - element   DimArray { Float64 , 1 }   with   dimensions : \n    Coord   () :   Tuple { Float64 ,   Float64 ,   Float64 }[( 3.0 ,   4.0 ,   4.0 ),   ( 1.0 ,   3.0 ,   4.0 )]   MergedLookup \n   0.3 \n   0.4 \n source # DimensionalData.Dimensions.AnonDim  —  Type . AnonDim   <:   Dimension \n\n AnonDim () \n Anonymous dimension. Used when extra dimensions are created, such as during transpose of a vector. source # DimensionalData.Dimensions.@dim  —  Macro . @dim   typ   [ supertype = Dimension ]   [ name :: String = string ( typ )] \n Macro to easily define new dimensions. The supertype will be inserted into the type of the dim. The default is simply  YourDim <: Dimension . Making a Dimesion inherit from  XDim ,  YDim ,  ZDim  or  TimeDim  will affect automatic plot layout and other methods that dispatch on these types.  <: YDim  are plotted on the Y axis,  <: XDim  on the X axis, etc. Example: using   DimensionalData \n using   DimensionalData :   @dim ,   YDim ,   XDim \n @dim   Lat   YDim   \"latitude\" \n @dim   Lon   XDim   \"Longitude\" \n # output \n source"},{"id":251,"pagetitle":"Reference - DimensionalData.jl","title":"Exported methods ¤","ref":"/DimensionalData/stable/reference/#exported-methods","content":" Exported methods ¤ # DimensionalData.Dimensions.hasdim  —  Function . hasdim ([ f ],   x ,   query :: Tuple )   =>   NTUple { Bool } \n hasdim ([ f ],   x ,   query ... )   =>   NTUple { Bool } \n hasdim ([ f ],   x ,   query )   =>   Bool \n Check if an object  x  has dimensions that match or inherit from the  query  dimensions. Arguments x : any object with a  dims  method, a  Tuple  of  Dimension  or a single  Dimension . query : Tuple or single  Dimension  or dimension  Type . f :  <:  by default, but can be  >:  to match abstract types to concrete types. Check if an object or tuple contains an  Dimension , or a tuple of dimensions. Example julia>   using   DimensionalData \n\n julia>   A   =   DimArray ( ones ( 10 ,   10 ,   10 ),   ( X ,   Y ,   Z )); \n\n julia>   hasdim ( A ,   X ) \n true \n\n julia>   hasdim ( A ,   ( Z ,   X ,   Y )) \n (true, true, true) \n\n julia>   hasdim ( A ,   Ti ) \n false \n source # DimensionalData.Dimensions.dimnum  —  Function . dimnum ( x ,   query :: Tuple )   =>   NTuple { Int } \n dimnum ( x ,   query )   =>   Int \n Get the number(s) of  Dimension (s) as ordered in the dimensions of an object. Arguments x : any object with a  dims  method, a  Tuple  of  Dimension  or a single  Dimension . query : Tuple, Array or single  Dimension  or dimension  Type . The return type will be a Tuple of  Int  or a single  Int , depending on wether  query  is a  Tuple  or single  Dimension . Example julia>   using   DimensionalData \n\n julia>   A   =   DimArray ( ones ( 10 ,   10 ,   10 ),   ( X ,   Y ,   Z )); \n\n julia>   dimnum ( A ,   ( Z ,   X ,   Y )) \n (3, 1, 2) \n\n julia>   dimnum ( A ,   Y ) \n 2 \n source"},{"id":252,"pagetitle":"Reference - DimensionalData.jl","title":"Non-exported methods ¤","ref":"/DimensionalData/stable/reference/#non-exported-methods","content":" Non-exported methods ¤ # DimensionalData.Dimensions.lookup  —  Function . lookup ( x :: Dimension )   =>   LookupArray \n lookup ( x ,   [ dims :: Tuple ])   =>   Tuple { Vararg { LookupArray }} \n lookup ( x :: Tuple )   =>   Tuple { Vararg { LookupArray }} \n lookup ( x ,   dim )   =>   LookupArray \n Returns the  LookupArray  of a dimension. This dictates properties of the dimension such as array axis and index order, and sampling properties. dims  can be a  Dimension , a dimension type, or a tuple of either. source # DimensionalData.Dimensions.label  —  Function . label ( x )   =>   String \n label ( x ,   dims :: Tuple )   =>   NTuple { N , String } \n label ( x ,   dim )   =>   String \n label ( xs :: Tuple )   =>   NTuple { N , String } \n Get a plot label for data or a dimension. This will include the name and units if they exist, and anything else that should be shown on a plot. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.dim2key  —  Function . dim2key ( dim :: Dimension )   =>   Symbol \n dim2key ( dims :: Type { <: Dimension })   =>   Symbol \n dim2key ( dims :: Tuple )   =>   Tuple { Symbol , Vararg } \n Convert a dimension object to a simbol.  X() ,  Y() ,  Ti()  etc will be converted. to  :X ,  :Y ,  :Ti , as with any other dims generated with the  @dim  macro. All other  Dim{S}()  dimensions will generate  Symbol s  S . source # DimensionalData.Dimensions.key2dim  —  Function . key2dim ( s :: Symbol )   =>   Dimension \n key2dim ( dims ... )   =>   Tuple { Dimension , Vararg } \n key2dim ( dims :: Tuple )   =>   Tuple { Dimension , Vararg } \n Convert a symbol to a dimension object.  :X ,  :Y ,  :Ti  etc will be converted. to  X() ,  Y() ,  Ti() , as with any other dims generated with the  @dim  macro. All other  Symbol s  S  will generate  Dim{S}()  dimensions. source # DimensionalData.Dimensions.dims2indices  —  Function . dims2indices ( dim :: Dimension ,   I )   =>   NTuple { Union { Colon , AbstractArray , Int }} \n Convert a  Dimension  or  Selector I  to indices of  Int ,  AbstractArray  or  Colon . source # DimensionalData.Dimensions.LookupArrays.selectindices  —  Function . selectindices ( lookups ,   selectors ) \n Converts  Selector  to regular indices. source # DimensionalData.Dimensions.format  —  Function . format ( dims ,   x )   =>   Tuple { Vararg { Dimension , N }} \n Format the passed-in dimension(s)  dims  to match the object  x . Errors are thrown if dims don't match the array dims or size,  and any fields holding  Auto-  objects are filled with guessed objects. If a  LookupArray  hasn't been specified, a lookup is chosen based on the type and element type of the index. source # DimensionalData.Dimensions.reducedims  —  Function . reducedims ( x ,   dimstoreduce )   =>   Tuple { Vararg { Dimension }} \n Replace the specified dimensions with an index of length 1. This is usually to match a new array size where an axis has been reduced with a method like  mean  or  reduce  to a length of 1, but the number of dimensions has not changed. LookupArray  traits are also updated to correspond to the change in cell step, sampling type and order. source # DimensionalData.Dimensions.swapdims  —  Function . swapdims ( x :: T ,   newdims )   =>   T \n swapdims ( dims :: Tuple ,   newdims )   =>   Tuple { Vararg { Dimension }} \n Swap dimensions for the passed in dimensions, in the order passed. Passing in the  Dimension  types rewraps the dimension index, keeping the index values and metadata, while constructed  Dimension  objectes replace the original dimension.  nothing  leaves the original dimension as-is. Arguments x : any object with a  dims  method or a  Tuple  of  Dimension . newdim : Tuple of  Dimension  or dimension  Type . Example using   DimensionalData \n A   =   ones ( X ( 2 ),   Y ( 4 ),   Z ( 2 )) \n Dimensions . swapdims ( A ,   ( Dim { :a },   Dim { :b },   Dim { :c })) \n\n # output \n 2 × 4 × 2   DimArray { Float64 , 3 }   with   dimensions :   Dim { :a },   Dim { :b },   Dim { :c } \n [ : ,   : ,   1 ] \n   1.0    1.0    1.0    1.0 \n   1.0    1.0    1.0    1.0 \n [ and   1   more   slices ... ] \n source # DimensionalData.Dimensions.slicedims  —  Function . slicedims ( x ,   I )   =>   Tuple { Tuple , Tuple } \n slicedims ( f ,   x ,   I )   =>   Tuple { Tuple , Tuple } \n Slice the dimensions to match the axis values of the new array. All methods return a tuple conatining two tuples: the new dimensions, and the reference dimensions. The ref dimensions are no longer used in the new struct but are useful to give context to plots. Called at the array level the returned tuple will also include the previous reference dims attached to the array. Arguments f : a function  getindex ,   view  or  dotview . This will be used for slicing    getindex  is the default if  f  is not included. x : An  AbstractDimArray ,  Tuple  of  Dimension , or  Dimension I : A tuple of  Integer ,  Colon  or  AbstractArray source # DimensionalData.Dimensions.comparedims  —  Function . comparedims ( A :: AbstractDimArray ... ;   kw ... ) \n comparedims ( A :: Tuple ... ;   kw ... ) \n comparedims ( a ,   b ;   kw ... ) \n Check that dimensions or tuples of dimensions are the same, and return the first valid dimension. If  AbstractDimArray s are passed as arguments their dimensions are compared. Empty tuples and  nothing  dimension values are ignored, returning the  Dimension  value if it exists. Keywords These are all  Bool  flags: type : compare dimension type,  true  by default. valtype : compare wrapped value type,  false  by default. val : compare wrapped values,  false  by default. length : compare lengths,  true  by default. ignore_length_one : ignore length  1  in comparisons, and return whichever   dimension is not length 1, if any. This is useful in e.g. broadcasting comparisons.    false  by default. source # DimensionalData.Dimensions.combinedims  —  Function . combinedims ( xs ;   check = true ) \n Combine the dimensions of each object in  xs , in the order they are found. source # DimensionalData.Dimensions.otherdims  —  Function . otherdims ( x ,   query )   =>   Tuple { Vararg { Dimension , N }} \n Get the dimensions of an object  not  in  query . Arguments x : any object with a  dims  method, a  Tuple  of  Dimension . query : Tuple or single  Dimension  or dimension  Type . f :  <:  by default, but can be  >:  to match abstract types to concrete types. A tuple holding the unmatched dimensions is always returned. Example julia>   using   DimensionalData ,   DimensionalData . Dimensions \n\n julia>   A   =   DimArray ( ones ( 10 ,   10 ,   10 ),   ( X ,   Y ,   Z )); \n\n julia>   otherdims ( A ,   X ) \n Y, Z \n\n julia>   otherdims ( A ,   ( Y ,   Z )) \n X \n source # DimensionalData.Dimensions.commondims  —  Function . commondims ([ f ],   x ,   query )   =>   Tuple { Vararg { Dimension }} \n This is basically  dims(x, query)  where the order of the original is kept, unlike  dims  where the query tuple determines the order Also unlike  dims , commondims  always returns a  Tuple , no matter the input. No errors are thrown if dims are absent from either  x  or  query . f  is  <:  by default, but can be  >:  to sort abstract types by concrete types. julia>   using   DimensionalData ,   . Dimensions \n\n julia>   A   =   DimArray ( ones ( 10 ,   10 ,   10 ),   ( X ,   Y ,   Z )); \n\n julia>   commondims ( A ,   X ) \n X \n\n julia>   commondims ( A ,   ( X ,   Z )) \n X, Z \n\n julia>   commondims ( A ,   Ti ) \n () \n source # DimensionalData.Dimensions.sortdims  —  Function . sortdims ([ f ],   tosort ,   order )   =>   Tuple \n Sort dimensions  tosort  by  order . Dimensions in  order  but missing from  tosort  are replaced with  nothing . tosort  and  order  can be  Tuple s or  Vector s or Dimension or dimension type. Abstract supertypes like  TimeDim  can be used in  order . f  is  <:  by default, but can be  >:  to sort abstract types by concrete types. source # DimensionalData.Dimensions.LookupArrays.basetypeof  —  Function . basetypeof ( x )   =>   Type \n Get the \"base\" type of an object - the minimum required to define the object without it's fields. By default this is the full  UnionAll  for the type. But custom  basetypeof  methods can be defined for types with free type parameters. In DimensionalData this is primariliy used for comparing  Dimension s, where  Dim{:x}  is different from  Dim{:y} . source # DimensionalData.Dimensions.setdims  —  Function . setdims ( X ,   newdims )   =>   AbstractArray \n setdims ( :: Tuple ,   newdims )   =>   Tuple { Vararg { Dimension , N }} \n Replaces the first dim matching  <: basetypeof(newdim)  with newdim, and returns a new object or tuple with the dimension updated. Arguments x : any object with a  dims  method, a  Tuple  of  Dimension  or a single  Dimension . newdim : Tuple or single  Dimension ,  Type  or  Symbol . Example using   DimensionalData ,   DimensionalData . Dimensions ,   DimensionalData . LookupArrays \n A   =   ones ( X ( 10 ),   Y ( 10 : 10 : 100 )) \n B   =   setdims ( A ,   Y ( Categorical ( 'a' : 'j' ;   order = ForwardOrdered ()))) \n lookup ( B ,   Y ) \n # output \n Categorical { Char }   ForwardOrdered \n wrapping :   'a' : 1 : 'j' \n source # DimensionalData.Dimensions.dimsmatch  —  Function . dimsmatch ([ f ],   dim ,   query )   =>   Bool \n dimsmatch ([ f ],   dims :: Tuple ,   query :: Tuple )   =>   Bool \n Compare 2 dimensions or  Tuple  of  Dimension  are of the same base type, or are at least rotations/transformations of the same type. f  is  <:  by default, but can be  >:  to match abstract types to concrete types. source # DimensionalData.Dimensions.dimstride  —  Function . dimstride ( x ,   dim )   =>   Int \n Get the stride of the dimension relative to the other dimensions. This may or may not be equal to the stride of the related array, although it will be for  Array . Arguments x  is any object with a  dims  method, or a  Tuple  of  Dimension . dim  is a  Dimension ,  Dimension  type, or and  Int . Using an  Int  is not type-stable. source # DimensionalData.refdims_title  —  Function . refdims_title ( A :: AbstractDimArray ) \n refdims_title ( refdims :: Tuple ) \n refdims_title ( refdim :: Dimension ) \n Generate a title string based on reference dimension values. source # DimensionalData.rebuild_from_arrays  —  Function . rebuild_from_arrays ( s :: AbstractDimStack ,   das :: NamedTuple { <: Any , <: Tuple { Vararg { AbstractDimArray }}};   kw ... ) \n Rebuild an  AbstractDimStack  from a  Tuple  or  NamedTuple  of  AbstractDimArray  and an existing stack. Keywords Keywords are simply the fields of the stack object: data dims refdims metadata layerdims layermetadata source"},{"id":253,"pagetitle":"Reference - DimensionalData.jl","title":"LookupArrays ¤","ref":"/DimensionalData/stable/reference/#lookuparrays","content":" LookupArrays ¤ # DimensionalData.Dimensions.LookupArrays  —  Module . LookupArrays \n Module for  LookupArrays  and [ Selector ]s used in DimensionalData.jl LookupArrays  defines traits and  AbstractArray  wrappers that give specific behaviours for a lookup index when indexed with  Selector . For example, these allow tracking over array order so fast indexing works evne when  the array is reversed. To load LookupArrays types and methods into scope: using   DimensionalData \n using   DimensionalData . LookupArrays \n source"},{"id":254,"pagetitle":"Reference - DimensionalData.jl","title":"Selectors ¤","ref":"/DimensionalData/stable/reference/#selectors","content":" Selectors ¤ # DimensionalData.Dimensions.LookupArrays.Selector  —  Type . Selector \n Abstract supertype for all selectors. Selectors are wrappers that indicate that passed values are not the array indices, but values to be selected from the dimension index, such as  DateTime  objects for a  Ti  dimension. Selectors provided in DimensionalData are: At Between Touches Near Where Contains source # DimensionalData.Dimensions.LookupArrays.IntSelector  —  Type . IntSelector   <:   Selector \n Abstract supertype for  Selector s that return a single  Int  index. source # DimensionalData.Dimensions.LookupArrays.ArraySelector  —  Type . ArraySelector   <:   Selector \n Abstract supertype for  Selector s that return an  AbstractArray . source # DimensionalData.Dimensions.LookupArrays.At  —  Type . At   <:   IntSelector \n\n At ( x ,   atol ,   rtol ) \n At ( x ;   atol = nothing ,   rtol = nothing ) \n Selector that exactly matches the value on the passed-in dimensions, or throws an error. For ranges and arrays, every intermediate value must match an existing value - not just the end points. x  can be any value or  Vector  of values. atol  and  rtol  are passed to  isapprox . For  Number rtol  will be set to  Base.rtoldefault , otherwise  nothing , and wont be used. Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 5 : 7 ))) \n A [ X ( At ( 20 )),   Y ( At ( 6 ))] \n\n # output \n\n 5 \n source # DimensionalData.Dimensions.LookupArrays.Near  —  Type . Near   <:   IntSelector \n\n Near ( x ) \n Selector that selects the nearest index to  x . With  Points  this is simply the index values nearest to the  x , however with  Intervals  it is the interval  center  nearest to  x . This will be offset from the index value for  Start  and  End  loci. Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 5 : 7 ))) \n A [ X ( Near ( 23 )),   Y ( Near ( 5.1 ))] \n\n # output \n 4 \n source # DimensionalData.Dimensions.LookupArrays.Between  —  Type . Between   <:   ArraySelector \n\n Between ( a ,   b ) \n Depreciated: use  a..b  instead of  Between(a, b) . Other  Interval  objects from IntervalSets.jl, like `OpenInterval(a, b) will also work, giving the correct open/closed boundaries. Between  will e removed in furture to avoid clashes with  DataFrames.Between . Selector that retreive all indices located between 2 values, evaluated with  >=  for the lower value, and  <  for the upper value. This means the same value will not be counted twice in 2 adjacent  Between  selections. For  Intervals  the whole interval must be lie between the values. For  Points  the points must fall between the values. Different  Sampling  types may give different results with the same input - this is the intended behaviour. Between  for  Irregular  intervals is a little complicated. The interval is the distance between a value and the next (for  Start  locus) or previous (for  End  locus) value. For  Center , we take the mid point between two index values as the start and end of each interval. This may or may not make sense for the values in your indes, so use  Between  with  Irregular Intervals(Center())  with caution. Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 5 : 7 ))) \n A [ X ( Between ( 15 ,   25 )),   Y ( Between ( 4 ,   6.5 ))] \n\n # output \n\n 1 × 2   DimArray { Int64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   20 : 10 : 20   ForwardOrdered   Regular   Points , \n    Y   Sampled { Int64 }   5 : 6   ForwardOrdered   Regular   Points \n       5    6 \n   20    4    5 \n source # DimensionalData.Dimensions.LookupArrays.Touches  —  Type . Touches   <:   ArraySelector \n\n Touches ( a ,   b ) \n Selector that retreives all indices touching the closed interval 2 values, for the maximum possible area that could interact with the supplied range. This can be better than  ..  when e.g. subsetting an area to rasterize, as you may wish to include pixels that just touch the area, rather than those that fall within it. Touches is different to using closed intervals when the lookups also contain intervals - if any of the intervals touch, they are included. With  ..  they are discarded unless the whole cell interval falls inside the selector interval. Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 5 : 7 ))) \n A [ X ( Touches ( 15 ,   25 )),   Y ( Touches ( 4 ,   6.5 ))] \n\n # output \n 1 × 2   DimArray { Int64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   20 : 10 : 20   ForwardOrdered   Regular   Points , \n    Y   Sampled { Int64 }   5 : 6   ForwardOrdered   Regular   Points \n       5    6 \n   20    4    5 \n source # DimensionalData.Dimensions.LookupArrays.Contains  —  Type . Contains   <:   IntSelector \n\n Contains ( x ) \n Selector that selects the interval the value is contained by. If the interval is not present in the index, an error will be thrown. Can only be used for  Intervals  or  Categorical . Example using   DimensionalData ;   const   DD   =   DimensionalData \n dims_   =   X ( 10 : 10 : 20 ;   sampling = DD . Intervals ( DD . Center ())), \n          Y ( 5 : 7 ;   sampling = DD . Intervals ( DD . Center ())) \n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   dims_ ) \n A [ X ( Contains ( 8 )),   Y ( Contains ( 6.8 ))] \n\n # output \n 3 \n source # DimensionalData.Dimensions.LookupArrays.Where  —  Type . Where   <:   ArraySelector \n\n Where ( f :: Function ) \n Selector that filters a dimension lookup by any function that accepts a single value and returns a  Bool . Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 19 : 21 ))) \n A [ X ( Where ( x   ->   x   >   15 )),   Y ( Where ( x   ->   x   in   ( 19 ,   21 )))] \n\n # output \n\n 1 × 2   DimArray { Int64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   Int64 [ 20 ]   ForwardOrdered   Regular   Points , \n    Y   Sampled { Int64 }   Int64 [ 19 ,   21 ]   ForwardOrdered   Regular   Points \n       19    21 \n   20     4     6 \n source # DimensionalData.Dimensions.LookupArrays.All  —  Type . All   <:   Selector \n\n All ( selectors :: Selector ... ) \n Selector that combines the results of other selectors.  The indices used will be the union of all result sorted in ascending order. Example using   DimensionalData ,   Unitful \n\n dimz   =   X ( 10.0 : 20 : 200.0 ),   Ti ( 1 u \"s\" : 5 u \"s\" : 100 u \"s\" ) \n A   =   DimArray (( 1 : 10 )   *   ( 1 : 20 ) ' ,   dimz ) \n A [ X = All ( At ( 10.0 ),   At ( 50.0 )),   Ti = All ( 1 u \"s\" .. 10 u \"s\" ,   90 u \"s\" .. 100 u \"s\" )] \n\n # output \n\n 2 × 4   DimArray { Int64 , 2 }   with   dimensions : \n    X   Sampled { Float64 }   Float64 [ 10.0 ,   50.0 ]   ForwardOrdered   Regular   Points , \n    Ti   Sampled { Quantity { Int64 ,   𝐓 ,   Unitful . FreeUnits {( s ,),   𝐓 ,   nothing }}}   Quantity { Int64 ,   𝐓 ,   Unitful . FreeUnits {( s ,),   𝐓 ,   nothing }}[ 1   s ,   6   s ,   91   s ,   96   s ]   ForwardOrdered   Regular   Points \n         1   s    6   s    91   s    96   s \n   10.0      1      2      19      20 \n   50.0      3      6      57      60 \n source Lookup properties: # DimensionalData.Dimensions.LookupArrays.bounds  —  Function . bounds ( xs ,   [ dims :: Tuple ])   =>   Tuple { Vararg { Tuple { T , T }}} \n bounds ( xs :: Tuple )   =>   Tuple { Vararg { Tuple { T , T }}} \n bounds ( x ,   dim )   =>   Tuple { T , T } \n bounds ( dim :: Union { Dimension , LookupArray })   =>   Tuple { T , T } \n Return the bounds of all dimensions of an object, of a specific dimension, or of a tuple of dimensions. dims  can be a  Dimension , a dimension type, or a tuple of either. source # DimensionalData.Dimensions.LookupArrays.val  —  Function . val ( x ) \n val ( dims :: Tuple )   =>   Tuple \n Return the contained value of a wrapper object. dims  can be  Dimension ,  Dimension  types, or  Symbols  for  Dim{Symbol} . Objects that don't define a  val  method are returned unaltered. source # DimensionalData.Dimensions.LookupArrays.LookupArray  —  Type . LookupArray \n Types defining the behaviour of a lookup index, how it is plotted and how  Selector s like  Between  work. A  LookupArray  may be  NoLookup  indicating that the index is just the underlying array axis,  Categorical  for ordered or unordered categories,  or a  Sampled  index for  Points  or  Intervals . source # DimensionalData.Dimensions.LookupArrays.Aligned  —  Type . Aligned   <:   LookupArray \n Abstract supertype for  LookupArray s where the index is aligned with the array axes. This is by far the most common supertype for  LookupArray . source # DimensionalData.Dimensions.LookupArrays.AbstractSampled  —  Type . AbstractSampled   <:   Aligned \n Abstract supertype for  LookupArray s where the index is aligned with the array, and is independent of other dimensions.  Sampled  is provided by this package. AbstractSampled  must have   order ,  span  and  sampling  fields, or a  rebuild  method that accpts them as keyword arguments. source # DimensionalData.Dimensions.LookupArrays.Sampled  —  Type . Sampled   <:   AbstractSampled \n\n Sampled ( data :: AbstractVector ,   order :: Order ,   span :: Span ,   sampling :: Sampling ,   metadata ) \n Sampled (;   data = AutoIndex (),   order = AutoOrder (),   span = AutoSpan (),   sampling = Points (),   metadata = NoMetadata ()) \n A concrete implementation of the  LookupArray AbstractSampled . It can be used to represent  Points  or  Intervals . Sampled  is capable of representing gridded data from a wide range of sources, allowing correct  bounds  and  Selector s for points or intervals of regular, irregular, forward and reverse indexes. On  AbstractDimArray  construction,  Sampled  lookup is assigned for all lookups of   AbstractRange  not assigned to  Categorical . Arguments data : An  AbstractVector  of index values, matching the length of the curresponding   array axis. order :  Order ) indicating the order of the index,    AutoOrder  by default, detected from the order of  data    to be  ForwardOrdered ,  ReverseOrdered  or  Unordered .   These can be provided explicitly if they are known and performance is important. span : indicates the size of intervals or distance between points, and will be set to    Regular  for  AbstractRange  and  Irregular  for  AbstractArray ,   unless assigned manually. sampling : is assigned to  Points , unless set to  Intervals  manually.    Using  Intervals  will change the behaviour of  bounds  and  Selectors s   to take account for the full size of the interval, rather than the point alone. metadata : a  Dict  or  Metadata  wrapper that holds any metadata object adding more   information about the array axis - useful for extending DimensionalData for specific   contexts, like geospatial data in GeoData.jl. By default it is  NoMetadata() . Example Create an array with [ Interval ] sampling, and  Regular  span for a vector with known spacing. We set the  Locus  of the  Intervals  to  Start  specifying that the index values are for the positions at the start of each interval. using   DimensionalData ,   DimensionalData . LookupArrays \n\n x   =   X ( Sampled ( 100 :- 20 : 10 ;   sampling = Intervals ( Start ()))) \n y   =   Y ( Sampled ([ 1 ,   4 ,   7 ,   10 ];   span = Regular ( 3 ),   sampling = Intervals ( Start ()))) \n A   =   ones ( x ,   y ) \n\n # output \n 5 × 4   DimArray { Float64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   100 :- 20 : 20   ReverseOrdered   Regular   Intervals , \n    Y   Sampled { Int64 }   Int64 [ 1 ,   4 ,   7 ,   10 ]   ForwardOrdered   Regular   Intervals \n        1      4      7      10 \n   100    1.0    1.0    1.0     1.0 \n    80    1.0    1.0    1.0     1.0 \n    60    1.0    1.0    1.0     1.0 \n    40    1.0    1.0    1.0     1.0 \n    20    1.0    1.0    1.0     1.0 \n source # DimensionalData.Dimensions.LookupArrays.AbstractCategorical  —  Type . AbstractCategorical   <:   Aligned \n LookupArray s where the values are categories. Categorical  is the provided concrete implementation.  but this can easily be extended - all methods are defined for  AbstractCategorical . All  AbstractCategorical  must provide a  rebuild  method with  data ,  order  and  metadata  keyword arguments. source # DimensionalData.Dimensions.LookupArrays.Categorical  —  Type . Categorical   <:   AbstractCategorical \n\n Categorical ( o :: Order ) \n Categorical (;   order = Unordered ()) \n An LookupArray where the values are categories. This will be automatically assigned if the index contains  AbstractString ,  Symbol  or  Char . Otherwise it can be assigned manually. Order  will be determined automatically where possible. Arguments data : An  AbstractVector  of index values, matching the length of the curresponding   array axis. order :  Order ) indicating the order of the index,    AutoOrder  by default, detected from the order of  data    to be  ForwardOrdered ,  ReverseOrdered  or  Unordered .   Can be provided if this is known and performance is important. metadata : a  Dict  or  Metadata  wrapper that holds any metadata object adding more   information about the array axis - useful for extending DimensionalData for specific   contexts, like geospatial data in GeoData.jl. By default it is  NoMetadata() . Example Create an array with [ Interval ] sampling. using   DimensionalData \n\n ds   =   X ([ \"one\" ,   \"two\" ,   \"three\" ]),   Y ([ :a ,   :b ,   :c ,   :d ]) \n A   =   DimArray ( rand ( 3 ,   4 ),   ds ) \n Dimensions . lookup ( A ) \n\n # output \n\n Categorical { String }   String [ \"one\" ,   \"two\" ,   \"three\" ]   Unordered , \n Categorical { Symbol }   Symbol [ :a ,   :b ,   :c ,   :d ]   ForwardOrdered \n source # DimensionalData.Dimensions.LookupArrays.Unaligned  —  Type . Unaligned   <:   LookupArray \n Abstract supertype for  LookupArray  where the index is not aligned to the grid. Indexing an  Unaligned  with  Selector s must provide all other  Unaligned  dimensions. source # DimensionalData.Dimensions.LookupArrays.Transformed  —  Type . Transformed   <:   Unaligned \n\n Transformed ( f ,   dim :: Dimension ;   metadata = NoMetadata ()) \n LookupArray  that uses an affine transformation to convert dimensions from  dims(lookup)  to  dims(array) . This can be useful when the dimensions are e.g. rotated from a more commonly used axis. Any function can be used to do the transformation, but transformations from CoordinateTransformations.jl may be useful. Arguments f : transformation function dim : a dimension to transform to. Keyword Arguments metdata : Example using   DimensionalData ,   DimensionalData . LookupArrays ,   CoordinateTransformations \n\n m   =   LinearMap ([ 0.5   0.0 ;   0.0   0.5 ]) \n A   =   [ 1   2    3    4 \n       5   6    7    8 \n       9   10   11   12 ]; \n da   =   DimArray ( A ,   ( t1 = Transformed ( m ,   X ),   t2 = Transformed ( m ,   Y ))) \n\n da [ X ( At ( 6 )),   Y ( At ( 2 ))] \n\n # output \n 9 \n source # DimensionalData.Dimensions.MergedLookup  —  Type . MergedLookup   <:   LookupArray \n\n MergedLookup ( data ,   dims ;   [ metadata ]) \n A  LookupArray  that holds multiple combined dimensions. MergedLookup  can be indexed with  Selector s like  At ,   Between , and  Where  although  Near  has undefined meaning. Arguments data : A  Vector  of  Tuple . dims : A  Tuple  of  Dimension  indicating the dimensions in the tuples in  data . Keywords metadata : a  Dict  or  Metadata  object to attach dimension metadata. source # DimensionalData.Dimensions.LookupArrays.NoLookup  —  Type . NoLookup   <:   LookupArray \n\n NoLookup () \n A  LookupArray  that is identical to the array axis.   Selector s can't be used on this lookup. Example Defining a  DimArray  without passing an index to the dimensions, it will be assigned  NoLookup : using   DimensionalData \n\n A   =   DimArray ( rand ( 3 ,   3 ),   ( X ,   Y )) \n Dimensions . lookup ( A ) \n\n # output \n\n NoLookup ,   NoLookup \n Which is identical to: using   . LookupArrays \n A   =   DimArray ( rand ( 3 ,   3 ),   ( X ( NoLookup ()),   Y ( NoLookup ()))) \n Dimensions . lookup ( A ) \n\n # output \n\n NoLookup ,   NoLookup \n source # DimensionalData.Dimensions.LookupArrays.AutoLookup  —  Type . AutoLookup   <:   LookupArray \n\n AutoLookup () \n AutoLookup ( index = AutoIndex ();   kw ... ) \n Automatic  LookupArray , the default lookup. It will be converted automatically to another  LookupArray  when it is possible to detect it from the index. Keywords will be used in the detected  LookupArray  constructor. source # DimensionalData.Dimensions.LookupArrays.AutoIndex  —  Type . AutoIndex \n Detect a  LookupArray  index from the context. This is used in  NoLookup  to simply use the array axis as the index when the array is constructed, and in  set  to change the  LookupArray  type without changing the index values. source"},{"id":255,"pagetitle":"Reference - DimensionalData.jl","title":"Metadata ¤","ref":"/DimensionalData/stable/reference/#metadata","content":" Metadata ¤ # DimensionalData.Dimensions.LookupArrays.AbstractMetadata  —  Type . AbstractMetadata { X , T } \n Abstract supertype for all metadata wrappers. Metadata wrappers allow tracking the contents and origin of metadata. This can  facilitate conversion between metadata types (for saving a file to a differenet format) or simply saving data back to the same file type with identical metadata. Using a wrapper instead of  Dict  or  NamedTuple  also lets us pass metadata  objects to  set  without ambiguity about where to put them. source # DimensionalData.Dimensions.LookupArrays.Metadata  —  Type . Metadata   <:   AbstractMetadata \n\n Metadata { X }( val :: Union { Dict , NamedTuple }) \n Metadata { X }( pairs :: Pair ... )   =>   Metadata { Dict } \n Metadata { X }(;   kw ... )   =>   Metadata { NamedTuple } \n General  Metadata  object. The  X  type parameter categorises the metadata for method dispatch, if required.  source # DimensionalData.Dimensions.LookupArrays.NoMetadata  —  Type . NoMetadata   <:   AbstractMetadata \n\n NoMetadata () \n Indicates an object has no metadata. But unlike using  nothing ,   get ,  keys  and  haskey  will still work on it,  get  always returning the fallback argument.  keys  returns  ()  while  haskey  always returns  false . source"},{"id":256,"pagetitle":"Reference - DimensionalData.jl","title":"LookupArray traits ¤","ref":"/DimensionalData/stable/reference/#lookuparray-traits","content":" LookupArray traits ¤ # DimensionalData.Dimensions.LookupArrays.LookupArrayTrait  —  Type . LookupArrayTrait \n Abstract supertype of all traits of a  LookupArray . These modify the behaviour of the lookup index. The term \"Trait\" is used loosely - these may be fields of an object of traits hard-coded to specific types. source"},{"id":257,"pagetitle":"Reference - DimensionalData.jl","title":"Order ¤","ref":"/DimensionalData/stable/reference/#order","content":" Order ¤ # DimensionalData.Dimensions.LookupArrays.Order  —  Type . Order   <:   LookupArrayTrait \n Traits for the order of a  LookupArray . These determine how  searchsorted  finds values in the index, and how objects are plotted. source # DimensionalData.Dimensions.LookupArrays.Ordered  —  Type . Ordered   <:   Order \n Supertype for the order of an ordered  LookupArray , including  ForwardOrdered  and  ReverseOrdered . source # DimensionalData.Dimensions.LookupArrays.ForwardOrdered  —  Type . ForwardOrdered   <:   Ordered \n\n ForwardOrdered () \n Indicates that the  LookupArray  index is in the normal forward order. source # DimensionalData.Dimensions.LookupArrays.ReverseOrdered  —  Type . ReverseOrdered   <:   Ordered \n\n ReverseOrdered () \n Indicates that the  LookupArray  index is in the reverse order. source # DimensionalData.Dimensions.LookupArrays.Unordered  —  Type . Unordered   <:   Order \n\n Unordered () \n Indicates that  LookupArray  is unordered. This means the index cannot be searched with  searchsortedfirst  or similar optimised methods - instead it will use  findfirst . source # DimensionalData.Dimensions.LookupArrays.AutoOrder  —  Type . AutoOrder   <:   Order \n\n AutoOrder () \n Specifies that the  Order  of a  LookupArray  will be found automatically where possible. source"},{"id":258,"pagetitle":"Reference - DimensionalData.jl","title":"Span ¤","ref":"/DimensionalData/stable/reference/#span","content":" Span ¤ # DimensionalData.Dimensions.LookupArrays.Span  —  Type . Span   <:   LookupArrayTrait \n Defines the type of span used in a  Sampling  index. These are  Regular  or  Irregular . source # DimensionalData.Dimensions.LookupArrays.Regular  —  Type . Regular   <:   Span \n\n Regular ( step = AutoStep ()) \n Points  or  Intervals  that have a fixed, regular step. source # DimensionalData.Dimensions.LookupArrays.Irregular  —  Type . Irregular   <:   Span \n\n Irregular ( bounds :: Tuple ) \n Irregular ( lowerbound ,   upperbound ) \n Points  or  Intervals  that have an  Irrigular  step size. To enable bounds tracking and accuract selectors, the starting bounds are provided as a 2 tuple, or 2 arguments.  (nothing, nothing)  is acceptable input, the bounds will be guessed from the index, but may be innaccurate. source # DimensionalData.Dimensions.LookupArrays.Explicit  —  Type . Explicit ( bounds :: AbstractMatix ) \n Intervals where the span is explicitly listed for every interval. This uses a matrix where with length 2 columns for each index value, holding the lower and upper bounds for that specific index. source # DimensionalData.Dimensions.LookupArrays.AutoSpan  —  Type . AutoSpan   <:   Span \n\n AutoSpan () \n The span will be guessed and replaced in  format  or  set . source"},{"id":259,"pagetitle":"Reference - DimensionalData.jl","title":"Sampling ¤","ref":"/DimensionalData/stable/reference/#sampling","content":" Sampling ¤ # DimensionalData.Dimensions.LookupArrays.Sampling  —  Type . Sampling   <:   LookupArrayTrait \n Indicates the sampling method used by the index:  Points  or  Intervals . source # DimensionalData.Dimensions.LookupArrays.Points  —  Type . Points   <:   Sampling \n\n Points () \n Sampling  lookup where single samples at exact points. These are always plotted at the center of array cells. source # DimensionalData.Dimensions.LookupArrays.Intervals  —  Type . Intervals   <:   Sampling \n\n Intervals ( locus :: Locus ) \n Sampling  specifying that sampled values are the mean (or similar) value over an  interval , rather than at one specific point. Intervals require a  Locus  of  Start ,  Center  or  End  to define the location in the interval that the index values refer to. source"},{"id":260,"pagetitle":"Reference - DimensionalData.jl","title":"Loci ¤","ref":"/DimensionalData/stable/reference/#loci","content":" Loci ¤ # DimensionalData.Dimensions.LookupArrays.Locus  —  Type . Locus <: LookupArrayTrait Abstract supertype of types that indicate the position of index values  where they represent  Intervals . These allow for values array cells to align with the  Start ,  Center , or  End  of values in the lookup index. This means they can be plotted with correct axis markers, and allows automatic converrsions to between formats with different standards (such as NetCDF and GeoTiff). source # DimensionalData.Dimensions.LookupArrays.Center  —  Type . Center   <:   Locus \n\n Center () \n Indicates a lookup value is for the center of its corresponding array cell. source # DimensionalData.Dimensions.LookupArrays.Start  —  Type . Start   <:   Locus \n\n Start () \n Indicates a lookup value is for the start of its corresponding array cell, in the direction of the lookup index order. source # DimensionalData.Dimensions.LookupArrays.End  —  Type . End   <:   Locus \n\n End () \n Indicates a lookup value is for the end of its corresponding array cell, in the direction of the lookup index order. source # DimensionalData.Dimensions.LookupArrays.AutoLocus  —  Type . AutoLocus   <:   Locus \n\n AutoLocus () \n Indicates a interval where the index position is not yet known. This will be filled with a default value on object construction. source"},{"id":261,"pagetitle":"Reference - DimensionalData.jl","title":"LookupArrays methods ¤","ref":"/DimensionalData/stable/reference/#lookuparrays-methods","content":" LookupArrays methods ¤ # DimensionalData.Dimensions.LookupArrays.hasselection  —  Function . hasselection ( x ,   selector )   =>   Bool \n hasselection ( x ,   selectors :: Tuple )   =>   Bool \n Check if indexing into x with  selectors  can be performed, where x is some object with a  dims  method, and  selectors  is a  Selector  or  Dimension  or a tuple of either. source # DimensionalData.Dimensions.LookupArrays.shiftlocus  —  Function . shiftlocus ( locus :: Locus ,   x ) \n Shift the index of  x  from the current locus to the new locus. We only shift  Sampled ,  Regular  or  Explicit ,  Intervals .  source # DimensionalData.Dimensions.LookupArrays.sampling  —  Function . sampling ( x ,   [ dims :: Tuple ])   =>   Tuple \n sampling ( x ,   dim )   =>   Sampling \n sampling ( xs :: Tuple )   =>   Tuple { Vararg { Sampling }} \n sampling ( x : Union { Dimension , LookupArray })   =>   Sampling \n Return the  Sampling  for each dimension. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.LookupArrays.span  —  Function . span ( x ,   [ dims :: Tuple ])   =>   Tuple \n span ( x ,   dim )   =>   Span \n span ( xs :: Tuple )   =>   Tuple { Vararg { Span , N }} \n span ( x :: Union { Dimension , LookupArray })   =>   Span \n Return the  Span  for each dimension. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.LookupArrays.order  —  Function . order ( x ,   [ dims :: Tuple ])   =>   Tuple \n order ( xs :: Tuple )   =>   Tuple \n order ( x :: Union { Dimension , LookupArray })   =>   Order \n Return the  Ordering  of the dimension index for each dimension:  ForwardOrdered ,  ReverseOrdered , or  Unordered Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.LookupArrays.index  —  Function . index ( x )   =>   Tuple { Vararg { AbstractArray }} \n index ( x ,   dims :: Tuple )   =>   Tuple { Vararg { AbstractArray }} \n index ( dims :: Tuple )   =>   Tuple { Vararg { AbstractArray }} } \n index ( x ,   dim )   =>   AbstractArray \n index ( dim :: Dimension )   =>   AbstractArray \n Return the contained index of a  Dimension . Only valid when a  Dimension  contains an  AbstractArray  or a Val tuple like  Val{(:a, :b)}() . The  Val  is unwrapped to return just the  Tuple dims  can be a  Dimension , or a tuple of  Dimension . source # DimensionalData.Dimensions.LookupArrays.locus  —  Function . locus ( x ,   [ dims :: Tuple ])   =>   Tuple \n locus ( x ,   dim )   =>   Locus \n locus ( xs :: Tuple )   =>   Tuple { Vararg { Locus , N }} \n locus ( x :: Union { Dimension , LookupArray })   =>   Locus \n Return the  Locus  for each dimension. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.LookupArrays.units  —  Function . units ( x )   =>   Union { Nothing , Any } \n units ( xs : Tuple )   =>   Tuple \n unit ( A :: AbstractDimArray ,   dims :: Tuple )   =>   Tuple \n unit ( A :: AbstractDimArray ,   dim )   =>   Union { Nothing , Any } \n Get the units of an array or  Dimension , or a tuple of of either. Units do not have a set field, and may or may not be included in  metadata . This method is to facilitate use in labels and plots when units are available, not a guarantee that they will be. If not available,  nothing  is returned. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source"},{"id":262,"pagetitle":"Reference - DimensionalData.jl","title":"Name ¤","ref":"/DimensionalData/stable/reference/#name","content":" Name ¤ # DimensionalData.AbstractName  —  Type . AbstractName \n Abstract supertype for name wrappers. source # DimensionalData.Name  —  Type . Name   <:   AbstractName \n\n Name ( name :: Union { Symbol , Name )   =>   Name \n Name ( name :: NoName )   =>   NoName \n Name wrapper. This lets arrays keep symbol names when the array wrapper neeeds to be `isbits, like for use on GPUs. It makes the name a property of the type. It's not necessary to use in normal use, a symbol is probably easier. source # DimensionalData.NoName  —  Type . NoName   <:   AbstractName \n\n NoName () \n NoName specifies an array is not named, and is the default  name  value for all  AAbstractDimArray s. source"}]