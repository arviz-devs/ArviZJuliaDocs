[{"id":3,"pagetitle":"Home","title":"ArviZ.jl: Exploratory analysis of Bayesian models in Julia","ref":"/ArviZ/stable/#arvizjl","content":" ArviZ.jl: Exploratory analysis of Bayesian models in Julia ArviZ.jl is a Julia meta-package for exploratory analysis of Bayesian models. It is part of the  ArviZ project , which also includes a related  Python package . ArviZ consists of and re-exports the following subpackages, along with extensions integrating them with InferenceObjects: InferenceObjects.jl : a base package implementing the  InferenceData  type with utilities for building, saving, and working with it MCMCDiagnosticTools.jl : diagnostics for Markov Chain Monte Carlo methods PSIS.jl : Pareto-smoothed importance sampling PosteriorStats.jl : common statistical analyses for the Bayesian workflow Additional functionality can be loaded with the following packages: ArviZExampleData.jl : example  InferenceData  objects, useful for demonstration and testing ArviZPythonPlots.jl : Python ArviZ's library of plotting functions for Julia types See the navigation bar for more useful packages."},{"id":4,"pagetitle":"Home","title":"Installation","ref":"/ArviZ/stable/#installation","content":" Installation From the Julia REPL, type  ]  to enter the Pkg REPL mode and run pkg> add ArviZ"},{"id":5,"pagetitle":"Home","title":"Usage","ref":"/ArviZ/stable/#usage","content":" Usage See the  Quickstart  for example usage and the  API Overview  for description of functions."},{"id":6,"pagetitle":"Home","title":"Extending ArviZ.jl","ref":"/ArviZ/stable/#extendingarviz","content":" Extending ArviZ.jl To use a custom data type with ArviZ.jl, simply overload  InferenceObjects.convert_to_inference_data  to convert your input(s) to an  InferenceObjects.InferenceData ."},{"id":9,"pagetitle":"API Overview","title":"API Overview","ref":"/ArviZ/stable/api/#api","content":" API Overview Data Dataset Diagnostics InferenceData Stats"},{"id":12,"pagetitle":"Data","title":"Data","ref":"/ArviZ/stable/api/data/#data-api","content":" Data ArviZ.from_mcmcchains ArviZ.from_samplechains InferenceObjects.from_netcdf InferenceObjects.to_netcdf"},{"id":13,"pagetitle":"Data","title":"Inference library converters","ref":"/ArviZ/stable/api/data/#Inference-library-converters","content":" Inference library converters"},{"id":14,"pagetitle":"Data","title":"ArviZ.from_mcmcchains","ref":"/ArviZ/stable/api/data/#ArviZ.from_mcmcchains","content":" ArviZ.from_mcmcchains  —  Function from_mcmcchains(posterior::MCMCChains.Chains; kwargs...) -> InferenceData\nfrom_mcmcchains(; kwargs...) -> InferenceData\nfrom_mcmcchains(\n    posterior::MCMCChains.Chains,\n    posterior_predictive,\n    predictions,\n    log_likelihood;\n    kwargs...\n) -> InferenceData Convert data in an  MCMCChains.Chains  format into an  InferenceData . Any keyword argument below without an an explicitly annotated type above is allowed, so long as it can be passed to  convert_to_inference_data . Arguments posterior::MCMCChains.Chains : Draws from the posterior Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution or   name(s) of predictive variables in  posterior predictions : Out-of-sample predictions for the posterior. prior : Draws from the prior prior_predictive : Draws from the prior predictive distribution or name(s) of predictive   variables in  prior observed_data : Observed data on which the  posterior  is conditional. It should only   contain data which is modeled as a random variable. Keys are parameter names and values. constant_data : Model constants, data included in the model that are not modeled as   random variables. Keys are parameter names. predictions_constant_data : Constants relevant to the model predictions (i.e. new  x    values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this   argument as a named tuple whose keys are observed variable names and whose values are log   likelihood arrays. Alternatively, provide the name of variable in  posterior  containing   log likelihoods. library=MCMCChains : Name of library that generated the chains coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions eltypes : Map from variable names to eltypes. This is primarily used to assign discrete   eltypes to discrete variables that were stored in  Chains  as floats. Returns InferenceData : The data with groups corresponding to the provided data source"},{"id":15,"pagetitle":"Data","title":"ArviZ.from_samplechains","ref":"/ArviZ/stable/api/data/#ArviZ.from_samplechains","content":" ArviZ.from_samplechains  —  Function from_samplechains(\n    posterior=nothing;\n    prior=nothing,\n    library=SampleChains,\n    kwargs...,\n) -> InferenceData Convert SampleChains samples to an  InferenceData . Either  posterior  or  prior  may be a  SampleChains.AbstractChain  or  SampleChains.MultiChain  object. For descriptions of remaining  kwargs , see  from_namedtuple . source"},{"id":16,"pagetitle":"Data","title":"IO / Conversion","ref":"/ArviZ/stable/api/data/#IO-/-Conversion","content":" IO / Conversion"},{"id":17,"pagetitle":"Data","title":"InferenceObjects.from_netcdf","ref":"/ArviZ/stable/api/data/#InferenceObjects.from_netcdf","content":" InferenceObjects.from_netcdf  —  Function from_netcdf(path::AbstractString; kwargs...) -> InferenceData Load an  InferenceData  from an unopened NetCDF file. Remaining  kwargs  are passed to  NCDatasets.NCDataset . This method loads data eagerly. To instead load data lazily, pass an opened  NCDataset  to  from_netcdf . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_netcdf(\"centered_eight.nc\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data from_netcdf(ds::NCDatasets.NCDataset; load_mode) -> InferenceData Load an  InferenceData  from an opened NetCDF file. load_mode  defaults to  :lazy , which avoids reading variables into memory. Operations on these arrays will be slow.  load_mode  can also be  :eager , which copies all variables into memory. It is then safe to close  ds . If  load_mode  is  :lazy  and  ds  is closed after constructing  InferenceData , using the variable arrays will have undefined behavior. Examples Here is how we might load an  InferenceData  from an  InferenceData  lazily from a web-hosted NetCDF file. julia> using HTTP, InferenceObjects, NCDatasets\n\njulia> resp = HTTP.get(\"https://github.com/arviz-devs/arviz_example_data/blob/main/data/centered_eight.nc?raw=true\");\n\njulia> ds = NCDataset(\"centered_eight\", \"r\"; memory = resp.body);\n\njulia> idata = from_netcdf(ds)\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\njulia> idata_copy = copy(idata); # disconnect from the loaded dataset\n\njulia> close(ds);"},{"id":18,"pagetitle":"Data","title":"InferenceObjects.to_netcdf","ref":"/ArviZ/stable/api/data/#InferenceObjects.to_netcdf","content":" InferenceObjects.to_netcdf  —  Function to_netcdf(data, dest::AbstractString; group::Symbol=:posterior, kwargs...)\nto_netcdf(data, dest::NCDatasets.NCDataset; group::Symbol=:posterior) Write  data  to a NetCDF file. data  is any type that can be converted to an  InferenceData  using  convert_to_inference_data . If not an  InferenceData , then  group  specifies which group the data represents. dest  specifies either the path to the NetCDF file or an opened NetCDF file. If  dest  is a path, remaining  kwargs  are passed to  NCDatasets.NCDataset . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_namedtuple((; x = randn(4, 100, 3), z = randn(4, 100)))\nInferenceData with groups:\n  > posterior\n\njulia> to_netcdf(idata, \"data.nc\")\n\"data.nc\""},{"id":21,"pagetitle":"Dataset","title":"Dataset","ref":"/ArviZ/stable/api/dataset/#dataset-api","content":" Dataset InferenceObjects.Dataset InferenceObjects.convert_to_dataset InferenceObjects.namedtuple_to_dataset"},{"id":22,"pagetitle":"Dataset","title":"Type definition","ref":"/ArviZ/stable/api/dataset/#Type-definition","content":" Type definition"},{"id":23,"pagetitle":"Dataset","title":"InferenceObjects.Dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.Dataset","content":" InferenceObjects.Dataset  —  Type Dataset{L} <: DimensionalData.AbstractDimStack{L} Container of dimensional arrays sharing some dimensions. This type is an  DimensionalData.AbstractDimStack  that implements the same interface as  DimensionalData.DimStack  and has identical usage. When a  Dataset  is passed to Python, it is converted to an  xarray.Dataset  without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an  xarray.Dataset  is passed to Julia, its data must be copied. Constructors Dataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n) In most cases, use  convert_to_dataset  to create a  Dataset  instead of directly using a constructor."},{"id":24,"pagetitle":"Dataset","title":"General conversion","ref":"/ArviZ/stable/api/dataset/#General-conversion","content":" General conversion"},{"id":25,"pagetitle":"Dataset","title":"InferenceObjects.convert_to_dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.convert_to_dataset","content":" InferenceObjects.convert_to_dataset  —  Function convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset Convert a supported object to a  Dataset . In most cases, this function calls  convert_to_inference_data  and returns the corresponding  group ."},{"id":26,"pagetitle":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.namedtuple_to_dataset","content":" InferenceObjects.namedtuple_to_dataset  —  Function namedtuple_to_dataset(data; kwargs...) -> Dataset Convert  NamedTuple  mapping variable names to arrays to a  Dataset . Any non-array values will be converted to a 0-dimensional array. Keywords attrs::AbstractDict{<:AbstractString} : a collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable. library::Union{String,Module} : library used for performing inference. Will be attached to the  attrs  metadata. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated."},{"id":27,"pagetitle":"Dataset","title":"DimensionalData","ref":"/ArviZ/stable/api/dataset/#DimensionalData","content":" DimensionalData As a  DimensionalData.AbstractDimStack ,  Dataset  also implements the  AbstractDimStack  API and can be used like a  DimStack . See  DimensionalData's documentation  for example usage."},{"id":28,"pagetitle":"Dataset","title":"Tables inteface","ref":"/ArviZ/stable/api/dataset/#Tables-inteface","content":" Tables inteface Dataset  implements the  Tables  interface. This allows  Dataset s to be used as sources for any function that can accept a table. For example, it's straightforward to: write to CSV with CSV.jl flatten to a DataFrame with DataFrames.jl plot with StatsPlots.jl plot with AlgebraOfGraphics.jl"},{"id":31,"pagetitle":"Diagnostics","title":"Diagnostics","ref":"/ArviZ/stable/api/diagnostics/#diagnostics-api","content":" Diagnostics MCMCDiagnosticTools.AutocovMethod MCMCDiagnosticTools.BDAAutocovMethod MCMCDiagnosticTools.FFTAutocovMethod MCMCDiagnosticTools.bfmi MCMCDiagnosticTools.ess MCMCDiagnosticTools.ess_rhat MCMCDiagnosticTools.mcse MCMCDiagnosticTools.rhat MCMCDiagnosticTools.rstar"},{"id":32,"pagetitle":"Diagnostics","title":"Bayesian fraction of missing information","ref":"/ArviZ/stable/api/diagnostics/#bfmi","content":" Bayesian fraction of missing information"},{"id":33,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.bfmi","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.bfmi","content":" MCMCDiagnosticTools.bfmi  —  Function bfmi(energy::AbstractVector{<:Real}) -> Real\nbfmi(energy::AbstractMatrix{<:Real}; dims::Int=1) -> AbstractVector{<:Real} Calculate the estimated Bayesian fraction of missing information (BFMI). When sampling with Hamiltonian Monte Carlo (HMC), BFMI quantifies how well momentum resampling matches the marginal energy distribution. The current advice is that values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may change. A BFMI value below the threshold often indicates poor adaptation of sampling parameters or that the target distribution has heavy tails that were not well explored by the Markov chain. For more information, see Section 6.1 of  [Betancourt2018]  or  [Betancourt2016]  for a complete account. energy  is either a vector of Hamiltonian energies of draws or a matrix of energies of draws for multiple chains.  dims  indicates the dimension in  energy  that contains the draws. The default  dims=1  assumes  energy  has the shape  draws  or  (draws, chains) . If a different shape is provided,  dims  must be set accordingly. If  energy  is a vector, a single BFMI value is returned. Otherwise, a vector of BFMI values for each chain is returned."},{"id":34,"pagetitle":"Diagnostics","title":"Effective sample size and  $\\widehat{R}$  diagnostic","ref":"/ArviZ/stable/api/diagnostics/#ess_rhat","content":" Effective sample size and  $\\widehat{R}$  diagnostic"},{"id":35,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.ess","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.ess","content":" MCMCDiagnosticTools.ess  —  Function ess(data::InferenceData; kwargs...) -> Dataset\ness(data::Dataset; kwargs...) -> Dataset Calculate the effective sample size (ESS) for each parameter in the data. ess(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind=:bulk,\n    relative::Bool=false,\n    autocov_method=AutocovMethod(),\n    split_chains::Int=2,\n    maxlag::Int=250,\n    kwargs...\n) Estimate the effective sample size (ESS) of the  samples  of shape  (draws, [chains[, parameters...]])  with the  autocov_method . Optionally, the  kind  of ESS estimate to be computed can be specified (see below). Some  kind s accept additional  kwargs . If  relative  is  true , the relative ESS is returned, i.e.  ess / (draws * chains) . split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. There must be at least 3 draws in each chain after splitting. maxlag  indicates the maximum lag for which autocovariance is computed and must be greater than 0. For a given estimand, it is recommended that the ESS is at least  100 * chains  and that  $\\widehat{R} < 1.01$ . [VehtariGelman2021] See also:  AutocovMethod ,  FFTAutocovMethod ,  BDAAutocovMethod ,  rhat ,  ess_rhat ,  mcse Kinds of ESS estimates If  kind  isa a  Symbol , it may take one of the following values: :bulk : basic ESS computed on rank-normalized draws. This kind diagnoses poor convergence   in the bulk of the distribution due to trends or different locations of the chains. :tail : minimum of the quantile-ESS for the symmetric quantiles where    tail_prob=0.1  is the probability in the tails. This kind diagnoses poor convergence in   the tails of the distribution. If this kind is chosen,  kwargs  may contain a    tail_prob  keyword. :basic : basic ESS, equivalent to specifying  kind=Statistics.mean . Note While Bulk-ESS is conceptually related to basic ESS, it is well-defined even if the chains do not have finite variance. [VehtariGelman2021]  For each parameter, rank-normalization proceeds by first ranking the inputs using \"tied ranking\" and then transforming the ranks to normal quantiles so that the result is standard normally distributed. This transform is monotonic. Otherwise,  kind  specifies one of the following estimators, whose ESS is to be estimated: Statistics.mean Statistics.median Statistics.std StatsBase.mad Base.Fix2(Statistics.quantile, p::Real)"},{"id":36,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.rhat","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.rhat","content":" MCMCDiagnosticTools.rhat  —  Function rhat(data::InferenceData; kwargs...) -> Dataset\nrhat(data::Dataset; kwargs...) -> Dataset Calculate the  $\\widehat{R}$  diagnostic for each parameter in the data. rhat(samples::AbstractArray{Union{Real,Missing}}; kind::Symbol=:rank, split_chains=2) Compute the  $\\widehat{R}$  diagnostics for each parameter in  samples  of shape  (draws, [chains[, parameters...]]) . [VehtariGelman2021] kind  indicates the kind of  $\\widehat{R}$  to compute (see below). split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. See also  ess ,  ess_rhat ,  rstar Kinds of  $\\widehat{R}$ The following  kind s are supported: :rank : maximum of  $\\widehat{R}$  with  kind=:bulk  and  kind=:tail . :bulk : basic  $\\widehat{R}$  computed on rank-normalized draws. This kind diagnoses   poor convergence in the bulk of the distribution due to trends or different locations of   the chains. :tail :  $\\widehat{R}$  computed on draws folded around the median and then   rank-normalized. This kind diagnoses poor convergence in the tails of the distribution   due to different scales of the chains. :basic : Classic  $\\widehat{R}$ ."},{"id":37,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.ess_rhat","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.ess_rhat","content":" MCMCDiagnosticTools.ess_rhat  —  Function ess_rhat(data::InferenceData; kwargs...) -> Dataset\ness_rhat(data::Dataset; kwargs...) -> Dataset Calculate the effective sample size (ESS) and  $\\widehat{R}$  diagnostic for each parameter in the data. ess_rhat(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind::Symbol=:rank,\n    kwargs...,\n) -> NamedTuple{(:ess, :rhat)} Estimate the effective sample size and  $\\widehat{R}$  of the  samples  of shape  (draws, [chains[, parameters...]]) . When both ESS and  $\\widehat{R}$  are needed, this method is often more efficient than calling  ess  and  rhat  separately. See  rhat  for a description of supported  kind s and  ess  for a description of  kwargs . The following autocovariance methods are supported:"},{"id":38,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.AutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.AutocovMethod","content":" MCMCDiagnosticTools.AutocovMethod  —  Type AutocovMethod <: AbstractAutocovMethod The  AutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021]  and uses the biased estimator of the autocovariance, as discussed by  [Geyer1992] ."},{"id":39,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.FFTAutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.FFTAutocovMethod","content":" MCMCDiagnosticTools.FFTAutocovMethod  —  Type FFTAutocovMethod <: AbstractAutocovMethod The  FFTAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. The algorithm is the same as the one of  AutocovMethod  but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation. Info To be able to use this method, you have to load a package that implements the  AbstractFFTs.jl  interface such as  FFTW.jl  or  FastTransforms.jl ."},{"id":40,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.BDAAutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.BDAAutocovMethod","content":" MCMCDiagnosticTools.BDAAutocovMethod  —  Type BDAAutocovMethod <: AbstractAutocovMethod The  BDAAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021] . and uses the variogram estimator of the autocorrelation function discussed by  [BDA3] ."},{"id":41,"pagetitle":"Diagnostics","title":"Monte Carlo standard error","ref":"/ArviZ/stable/api/diagnostics/#mcse","content":" Monte Carlo standard error"},{"id":42,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.mcse","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.mcse","content":" MCMCDiagnosticTools.mcse  —  Function mcse(data::InferenceData; kwargs...) -> Dataset\nmcse(data::Dataset; kwargs...) -> Dataset Calculate the Monte Carlo standard error (MCSE) for each parameter in the data. mcse(samples::AbstractArray{<:Union{Missing,Real}}; kind=Statistics.mean, kwargs...) Estimate the Monte Carlo standard errors (MCSE) of the estimator  kind  applied to  samples  of shape  (draws, [chains[, parameters...]]) . See also:  ess Kinds of MCSE estimates The estimator whose MCSE should be estimated is specified with  kind .  kind  must accept a vector of the same  eltype  as  samples  and return a real estimate. For the following estimators, the effective sample size  ess  and an estimate of the asymptotic variance are used to compute the MCSE, and  kwargs  are forwarded to  ess : Statistics.mean Statistics.median Statistics.std Base.Fix2(Statistics.quantile, p::Real) For other estimators, the subsampling bootstrap method (SBM) [FlegalJones2011] [Flegal2012]  is used as a fallback, and the only accepted  kwargs  are  batch_size , which indicates the size of the overlapping batches used to estimate the MCSE, defaulting to  floor(Int, sqrt(draws * chains)) . Note that SBM tends to underestimate the MCSE, especially for highly autocorrelated chains. One should verify that autocorrelation is low by checking the bulk- and tail-ESS values."},{"id":43,"pagetitle":"Diagnostics","title":"$R^*$  diagnostic","ref":"/ArviZ/stable/api/diagnostics/#rstar","content":" $R^*$  diagnostic"},{"id":44,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.rstar","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.rstar","content":" MCMCDiagnosticTools.rstar  —  Function rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    data::Union{InferenceData,Dataset};\n    kwargs...,\n) Calculate the  $R^*$  diagnostic for the data. rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples,\n    chain_indices::AbstractVector{Int};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the table  samples  with the  classifier . samples  must be either an  AbstractMatrix , an  AbstractVector , or a table (i.e. implements the Tables.jl interface) whose rows are draws and whose columns are parameters. chain_indices  indicates the chain ids of each row of  samples . This method supports ragged chains, i.e. chains of nonequal lengths. rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples::AbstractArray{<:Real};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the  samples  with the  classifier . samples  is an array of draws with the shape  (draws, [chains[, parameters...]]) .` This implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari. The  classifier  has to be a supervised classifier of the MLJ framework (see the  MLJ documentation  for a list of supported models). It is trained with a  subset  of the samples from each chain. Each chain is split into  split_chains  separate chains to additionally check for within-chain convergence. The training of the classifier can be inspected by adjusting the  verbosity  level. If the classifier is deterministic, i.e., if it predicts a class, the value of the  $R^*$  statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the  $R^*$  statistic is returned (algorithm 2). Note The correctness of the statistic depends on the convergence of the  classifier  used internally in the statistic. Examples julia> using MLJBase, MLJIteration, EvoTrees, Statistics\n\njulia> samples = fill(4.0, 100, 3, 2); One can compute the distribution of the  $R^*$  statistic (algorithm 2) with a probabilistic classifier. For instance, we can use a gradient-boosted trees model with  nrounds = 100  sequentially stacked trees and learning rate  eta = 0.05 : julia> model = EvoTreeClassifier(; nrounds=100, eta=0.05);\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 Note, however, that it is recommended to determine  nrounds  based on early-stopping. With the MLJ framework, this can be achieved in the following way (see the  MLJ documentation  for additional explanations): julia> model = IteratedModel(;\n           model=EvoTreeClassifier(; eta=0.05),\n           iteration_parameter=:nrounds,\n           resampling=Holdout(),\n           measures=log_loss,\n           controls=[Step(5), Patience(2), NumberLimit(100)],\n           retrain=true,\n       );\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 For deterministic classifiers, a single  $R^*$  statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models. julia> evotree_deterministic = Pipeline(model; operation=predict_mode);\n\njulia> value = rstar(evotree_deterministic, samples);\n\njulia> round(value; digits=2)\n1.0 References Lambert, B., & Vehtari, A. (2020).  $R^*$ : A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers. Betancourt2018 Betancourt M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo.  arXiv:1701.02434v2  [stat.ME] Betancourt2016 Betancourt M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.  arXiv:1604.00695v1  [stat.ME] VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 Geyer1992 Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483. VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 BDA3 Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press. FlegalJones2011 Flegal JM, Jones GL. (2011) Implementing MCMC: estimating with confidence.                 Handbook of Markov Chain Monte Carlo. pp. 175-97.                  pdf Flegal2012 Flegal JM. (2012) Applicability of subsampling bootstrap methods in Markov chain Monte Carlo.            Monte Carlo and Quasi-Monte Carlo Methods 2010. pp. 363-72.            doi:  10.1007/978-3-642-27440-4_18"},{"id":47,"pagetitle":"InferenceData","title":"InferenceData","ref":"/ArviZ/stable/api/inference_data/#inferencedata-api","content":" InferenceData InferenceObjects.InferenceData Base.cat Base.getindex Base.getproperty Base.merge Base.propertynames Base.setindex InferenceObjects.convert_to_inference_data InferenceObjects.from_dict InferenceObjects.from_namedtuple"},{"id":48,"pagetitle":"InferenceData","title":"Type definition","ref":"/ArviZ/stable/api/inference_data/#Type-definition","content":" Type definition"},{"id":49,"pagetitle":"InferenceData","title":"InferenceObjects.InferenceData","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.InferenceData","content":" InferenceObjects.InferenceData  —  Type InferenceData{group_names,group_types} Container for inference data storage using DimensionalData. This object implements the  InferenceData schema . Internally, groups are stored in a  NamedTuple , which can be accessed using  parent(::InferenceData) . Constructors InferenceData(groups::NamedTuple)\nInferenceData(; groups...) Construct an inference data from either a  NamedTuple  or keyword arguments of groups. Groups must be  Dataset  objects. Instead of directly creating an  InferenceData , use the exported  from_xyz  functions or  convert_to_inference_data ."},{"id":50,"pagetitle":"InferenceData","title":"Property interface","ref":"/ArviZ/stable/api/inference_data/#Property-interface","content":" Property interface"},{"id":51,"pagetitle":"InferenceData","title":"Base.getproperty","ref":"/ArviZ/stable/api/inference_data/#Base.getproperty","content":" Base.getproperty  —  Function getproperty(data::InferenceData, name::Symbol) -> Dataset Get group with the specified  name ."},{"id":52,"pagetitle":"InferenceData","title":"Base.propertynames","ref":"/ArviZ/stable/api/inference_data/#Base.propertynames","content":" Base.propertynames  —  Function propertynames(data::InferenceData) -> Tuple{Symbol} Get names of groups"},{"id":53,"pagetitle":"InferenceData","title":"Indexing interface","ref":"/ArviZ/stable/api/inference_data/#Indexing-interface","content":" Indexing interface"},{"id":54,"pagetitle":"InferenceData","title":"Base.getindex","ref":"/ArviZ/stable/api/inference_data/#Base.getindex","content":" Base.getindex  —  Function Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData Return a new  InferenceData  containing the specified groups sliced to the specified coords. coords  specifies a dimension name mapping to an index, a  DimensionalData.Selector , or an  IntervalSets.AbstractInterval . If one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised. Examples Select data from all groups for just the specified id values. julia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\" Select data from just the posterior, returning a  Dataset  if the indices index more than one element from any of the variables: julia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:19:25.982\" Note that if a single index is provided, the behavior is still to slice so that the dimension is preserved."},{"id":55,"pagetitle":"InferenceData","title":"Base.setindex","ref":"/ArviZ/stable/api/inference_data/#Base.setindex","content":" Base.setindex  —  Function Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData Create a new  InferenceData  containing the  group  with the specified  name . If a group with  name  is already in  data , it is replaced."},{"id":56,"pagetitle":"InferenceData","title":"Iteration interface","ref":"/ArviZ/stable/api/inference_data/#Iteration-interface","content":" Iteration interface InferenceData  also implements the same iteration interface as its underlying  NamedTuple . That is, iterating over an  InferenceData  iterates over its groups."},{"id":57,"pagetitle":"InferenceData","title":"General conversion","ref":"/ArviZ/stable/api/inference_data/#General-conversion","content":" General conversion"},{"id":58,"pagetitle":"InferenceData","title":"InferenceObjects.convert_to_inference_data","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.convert_to_inference_data","content":" InferenceObjects.convert_to_inference_data  —  Function convert_to_inference_data(obj; group, kwargs...) -> InferenceData Convert a supported object to an  InferenceData  object. If  obj  converts to a single dataset,  group  specifies which dataset in the resulting  InferenceData  that is. See  convert_to_dataset Arguments obj  can be many objects. Basic supported types are: InferenceData : return unchanged Dataset / DimensionalData.AbstractDimStack : add to  InferenceData  as the only group NamedTuple / AbstractDict : create a  Dataset  as the only group AbstractArray{<:Real} : create a  Dataset  as the only group, given an arbitrary name, if the name is not set More specific types may be documented separately. Keywords group::Symbol = :posterior : If  obj  converts to a single dataset, assign the resulting dataset to this group. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. kwargs : remaining keywords forwarded to converter functions"},{"id":59,"pagetitle":"InferenceData","title":"InferenceObjects.from_dict","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.from_dict","content":" InferenceObjects.from_dict  —  Function from_dict(posterior::AbstractDict; kwargs...) -> InferenceData Convert a  Dict  to an  InferenceData . Arguments posterior : The data to be converted. Its strings must be  Symbol  or  AbstractString , and its values must be arrays. Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior::Dict=nothing : Draws from the prior prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata = Dict(\n    :x => rand(ndraws, nchains),\n    :y => randn(2, ndraws, nchains),\n    :z => randn(3, 2, ndraws, nchains),\n)\nidata = from_dict(data)"},{"id":60,"pagetitle":"InferenceData","title":"InferenceObjects.from_namedtuple","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.from_namedtuple","content":" InferenceObjects.from_namedtuple  —  Function from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData Convert a  NamedTuple  or container of  NamedTuple s to an  InferenceData . If containers are passed, they are flattened into a single  NamedTuple  with array elements whose first dimensions correspond to the dimensions of the containers. Arguments posterior : The data to be converted. It may be of the following types: ::NamedTuple : The keys are the variable names and the values are arrays with dimensions  (ndraws, nchains[, sizes...]) . ::Vector{Vector{<:NamedTuple}} : A vector of length  nchains  whose elements have length  ndraws . Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior=nothing : Draws from the prior. Accepts the same types as  posterior . prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Note If a  NamedTuple  is provided for  observed_data ,  constant_data , or predictions constant data`, any non-array values (e.g. integers) are converted to 0-dimensional arrays. Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata1 = (\n    x=rand(ndraws, nchains), y=randn(ndraws, nchains, 2), z=randn(ndraws, nchains, 3, 2)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata2 = from_namedtuple(data2)"},{"id":61,"pagetitle":"InferenceData","title":"General functions","ref":"/ArviZ/stable/api/inference_data/#General-functions","content":" General functions"},{"id":62,"pagetitle":"InferenceData","title":"Base.cat","ref":"/ArviZ/stable/api/inference_data/#Base.cat","content":" Base.cat  —  Function cat(data::InferenceData...; [groups=keys(data[1]),] dims) -> InferenceData Concatenate  InferenceData  objects along the specified dimension  dims . Only the groups in  groups  are concatenated. Remaining groups are  merge d into the new  InferenceData  object. Examples Here is how we can concatenate all groups of two  InferenceData  objects along the existing  chain  dimension: julia> coords = (; a_dim=[\"x\", \"y\", \"z\"]);\n\njulia> dims = dims=(; a=[:a_dim]);\n\njulia> data = Dict(:a => randn(100, 4, 3), :b => randn(100, 4));\n\njulia> idata = from_dict(data; coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1 = cat(idata, idata; dims=:chain)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim} (100×8×3)\n  :b Float64 dims: Dim{:draw}, Dim{:chain} (100×8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\" Alternatively, we can concatenate along a new  run  dimension, which will be created. julia> idata_cat2 = cat(idata, idata; dims=:run)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat2.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\" We can also concatenate only a subset of groups and merge the rest, which is useful when some groups are present only in some of the  InferenceData  objects or will be identical in all of them: julia> observed_data = Dict(:y => randn(10));\n\njulia> idata2 = from_dict(data; observed_data=observed_data, coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3 = cat(idata, idata2; groups=(:posterior,), dims=:run)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\"\n\njulia> idata_cat3.observed_data\nDataset with dimensions: Dim{:y_dim_1}\nand 1 layer:\n  :y Float64 dims: Dim{:y_dim_1} (10)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-02-17T15:11:00.59\""},{"id":63,"pagetitle":"InferenceData","title":"Base.merge","ref":"/ArviZ/stable/api/inference_data/#Base.merge","content":" Base.merge  —  Function merge(data::InferenceData...) -> InferenceData Merge  InferenceData  objects. The result contains all groups in  data  and  others . If a group appears more than once, the one that occurs last is kept. See also:  cat Examples Here we merge an  InferenceData  containing only a posterior group with one containing only a prior group to create a new one containing both groups. julia> idata1 = from_dict(Dict(:a => randn(100, 4, 3), :b => randn(100, 4)))\nInferenceData with groups:\n  > posterior\n\njulia> idata2 = from_dict(; prior=Dict(:a => randn(100, 1, 3), :c => randn(100, 1)))\nInferenceData with groups:\n  > prior\n\njulia> idata_merged = merge(idata1, idata2)\nInferenceData with groups:\n  > posterior\n  > prior"},{"id":66,"pagetitle":"Stats","title":"Stats","ref":"/ArviZ/stable/api/stats/#stats-api","content":" Stats PSIS.PSISResult PosteriorStats.AbstractELPDResult PosteriorStats.AbstractModelWeightsMethod PosteriorStats.BootstrappedPseudoBMA PosteriorStats.ModelComparisonResult PosteriorStats.PSISLOOResult PosteriorStats.PseudoBMA PosteriorStats.Stacking PosteriorStats.SummaryStats PosteriorStats.WAICResult PSIS.PSISPlots.paretoshapeplot PSIS.ess_is PSIS.psis PSIS.psis! PosteriorStats.compare PosteriorStats.default_diagnostics PosteriorStats.default_stats PosteriorStats.default_summary_stats PosteriorStats.elpd_estimates PosteriorStats.hdi PosteriorStats.hdi! PosteriorStats.information_criterion PosteriorStats.loo PosteriorStats.loo_pit PosteriorStats.model_weights PosteriorStats.r2_score PosteriorStats.smooth_data PosteriorStats.summarize PosteriorStats.waic StatsBase.summarystats"},{"id":67,"pagetitle":"Stats","title":"Summary statistics","ref":"/ArviZ/stable/api/stats/#Summary-statistics","content":" Summary statistics"},{"id":68,"pagetitle":"Stats","title":"PosteriorStats.SummaryStats","ref":"/ArviZ/stable/api/stats/#PosteriorStats.SummaryStats","content":" PosteriorStats.SummaryStats  —  Type A container for a column table of values computed by  summarize . This object implements the Tables and TableTraits interfaces and has a custom  show  method. name : The name of the collection of summary statistics, used as the table title in display. data : The summary statistics for each parameter, with an optional first column  parameter  containing the parameter names."},{"id":69,"pagetitle":"Stats","title":"PosteriorStats.default_summary_stats","ref":"/ArviZ/stable/api/stats/#PosteriorStats.default_summary_stats","content":" PosteriorStats.default_summary_stats  —  Function default_summary_stats(focus=Statistics.mean; kwargs...) Combinatiton of  default_stats  and  default_diagnostics  to be used with  summarize ."},{"id":70,"pagetitle":"Stats","title":"PosteriorStats.default_stats","ref":"/ArviZ/stable/api/stats/#PosteriorStats.default_stats","content":" PosteriorStats.default_stats  —  Function default_stats(focus=Statistics.mean; prob_interval=0.94, kwargs...) Default statistics to be computed with  summarize . The value of  focus  determines the statistics to be returned: Statistics.mean :  mean ,  std ,  hdi_3% ,  hdi_97% Statistics.median :  median ,  mad ,  eti_3% ,  eti_97% If  prob_interval  is set to a different value than the default, then different HDI and ETI statistics are computed accordingly.  hdi  refers to the highest-density interval, while  eti  refers to the equal-tailed interval (i.e. the credible interval computed from symmetric quantiles). See also:  hdi"},{"id":71,"pagetitle":"Stats","title":"PosteriorStats.default_diagnostics","ref":"/ArviZ/stable/api/stats/#PosteriorStats.default_diagnostics","content":" PosteriorStats.default_diagnostics  —  Function default_diagnostics(focus=Statistics.mean; kwargs...) Default diagnostics to be computed with  summarize . The value of  focus  determines the diagnostics to be returned: Statistics.mean :  mcse_mean ,  mcse_std ,  ess_tail ,  ess_bulk ,  rhat Statistics.median :  mcse_median ,  ess_tail ,  ess_bulk ,  rhat"},{"id":72,"pagetitle":"Stats","title":"PosteriorStats.summarize","ref":"/ArviZ/stable/api/stats/#PosteriorStats.summarize","content":" PosteriorStats.summarize  —  Function summarize(data, stats_funs...; name=\"SummaryStats\", [var_names]) -> SummaryStats Compute the summary statistics in  stats_funs  on each param in  data . stats_funs  is a collection of functions that reduces a matrix with shape  (draws, chains)  to a scalar or a collection of scalars. Alternatively, an item in  stats_funs  may be a  Pair  of the form  name => fun  specifying the name to be used for the statistic or of the form  (name1, ...) => fun  when the function returns a collection. When the function returns a collection, the names in this latter format must be provided. If no stats functions are provided, then those specified in  default_summary_stats  are computed. var_names  specifies the names of the parameters in  data . If not provided, the names are inferred from  data . To support computing summary statistics from a custom object, overload this method specifying the type of  data . See also  SummaryStats ,  default_summary_stats ,  default_stats ,  default_diagnostics . Examples Compute  mean ,  std  and the Monte Carlo standard error (MCSE) of the mean estimate: julia> using Statistics, StatsBase\n\njulia> x = randn(1000, 4, 3) .+ reshape(0:10:20, 1, 1, :);\n\njulia> summarize(x, mean, std, :mcse_mean => sem; name=\"Mean/Std\")\nMean/Std\n       mean    std  mcse_mean\n 1   0.0003  0.990      0.016\n 2  10.02    0.988      0.016\n 3  19.98    0.988      0.016 Avoid recomputing the mean by using  mean_and_std , and provide parameter names: julia> summarize(x, (:mean, :std) => mean_and_std, mad; var_names=[:a, :b, :c])\nSummaryStats\n         mean    std    mad\n a   0.000305  0.990  0.978\n b  10.0       0.988  0.995\n c  20.0       0.988  0.979 Note that when an estimator and its MCSE are both computed, the MCSE is used to determine the number of significant digits that will be displayed. julia> summarize(x; var_names=[:a, :b, :c])\nSummaryStats\n       mean   std  hdi_3%  hdi_97%  mcse_mean  mcse_std  ess_tail  ess_bulk  r ⋯\n a   0.0003  0.99   -1.92     1.78      0.016     0.012      3567      3663  1 ⋯\n b  10.02    0.99    8.17    11.9       0.016     0.011      3841      3906  1 ⋯\n c  19.98    0.99   18.1     21.9       0.016     0.012      3892      3749  1 ⋯\n                                                                1 column omitted Compute just the statistics with an 89% HDI on all parameters, and provide the parameter names: julia> summarize(x, default_stats(; prob_interval=0.89)...; var_names=[:a, :b, :c])\nSummaryStats\n         mean    std  hdi_5.5%  hdi_94.5%\n a   0.000305  0.990     -1.63       1.52\n b  10.0       0.988      8.53      11.6\n c  20.0       0.988     18.5       21.6 Compute the summary stats focusing on  Statistics.median : julia> summarize(x, default_summary_stats(median)...; var_names=[:a, :b, :c])\nSummaryStats\n    median    mad  eti_3%  eti_97%  mcse_median  ess_tail  ess_median  rhat\n a   0.004  0.978   -1.83     1.89        0.020      3567        3336  1.00\n b  10.02   0.995    8.17    11.9         0.023      3841        3787  1.00\n c  19.99   0.979   18.1     21.9         0.020      3892        3829  1.00"},{"id":73,"pagetitle":"Stats","title":"StatsBase.summarystats","ref":"/ArviZ/stable/api/stats/#StatsBase.summarystats","content":" StatsBase.summarystats  —  Function summarystats(data::InferenceData; group=:posterior, kwargs...) -> SummaryStats\nsummarystats(data::Dataset; kwargs...) -> SummaryStats Compute default summary statistics for the data using  summarize ."},{"id":74,"pagetitle":"Stats","title":"General statistics","ref":"/ArviZ/stable/api/stats/#General-statistics","content":" General statistics"},{"id":75,"pagetitle":"Stats","title":"PosteriorStats.hdi","ref":"/ArviZ/stable/api/stats/#PosteriorStats.hdi","content":" PosteriorStats.hdi  —  Function hdi(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) Estimate the unimodal highest density interval (HDI) of  samples  for the probability  prob . The HDI is the minimum width Bayesian credible interval (BCI). That is, it is the smallest possible interval containing  (100*prob) % of the probability mass. [Hyndman1996] samples  is an array of shape  (draws[, chains[, params...]]) . If multiple parameters are present, then  lower  and  upper  are arrays with the shape  (params...,) , computed separately for each marginal. This implementation uses the algorithm of  [ChenShao1999] . Note Any default value of  prob  is arbitrary. The default value of  prob=0.94  instead of a more common default like  prob=0.95  is chosen to reminder the user of this arbitrariness. Examples Here we calculate the 83% HDI for a normal random variable: julia> x = randn(2_000);\n\njulia> hdi(x; prob=0.83) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => -1.38266\n  :upper => 1.25982 We can also calculate the HDI for a 3-dimensional array of samples: julia> x = randn(1_000, 1, 1) .+ reshape(0:5:10, 1, 1, :);\n\njulia> hdi(x) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => [-1.9674, 3.0326, 8.0326]\n  :upper => [1.90028, 6.90028, 11.9003] hdi(data::InferenceData; kwargs...) -> Dataset\nhdi(data::Dataset; kwargs...) -> Dataset Calculate the highest density interval (HDI) for each parameter in the data."},{"id":76,"pagetitle":"Stats","title":"PosteriorStats.hdi!","ref":"/ArviZ/stable/api/stats/#PosteriorStats.hdi!","content":" PosteriorStats.hdi!  —  Function hdi!(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) A version of  hdi  that sorts  samples  in-place while computing the HDI."},{"id":77,"pagetitle":"Stats","title":"PosteriorStats.r2_score","ref":"/ArviZ/stable/api/stats/#PosteriorStats.r2_score","content":" PosteriorStats.r2_score  —  Function r2_score(y_true::AbstractVector, y_pred::AbstractVecOrMat) -> (; r2, r2_std) $R²$  for linear Bayesian regression models. [GelmanGoodrich2019] Arguments y_true : Observed data of length  noutputs y_pred : Predicted data with size  (ndraws[, nchains], noutputs) Examples julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"regression1d\");\n\njulia> y_true = idata.observed_data.y;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.y, (:draw, :chain, :y_dim_0));\n\njulia> r2_score(y_true, y_pred) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.683197\n  :r2_std => 0.0368838 r2_score(idata::InferenceData; y_name, y_pred_name) -> (; r2, r2_std) Compute  $R²$  from  idata , automatically formatting the predictions to the correct shape. Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. Examples julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"regression10d\");\n\njulia> r2_score(idata) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.998385\n  :r2_std => 0.000100621"},{"id":78,"pagetitle":"Stats","title":"Pareto-smoothed importance sampling","ref":"/ArviZ/stable/api/stats/#Pareto-smoothed-importance-sampling","content":" Pareto-smoothed importance sampling"},{"id":79,"pagetitle":"Stats","title":"PSIS.PSISResult","ref":"/ArviZ/stable/api/stats/#PSIS.PSISResult","content":" PSIS.PSISResult  —  Type PSISResult Result of Pareto-smoothed importance sampling (PSIS) using  psis . Properties log_weights : un-normalized Pareto-smoothed log weights weights : normalized Pareto-smoothed weights (allocates a copy) pareto_shape : Pareto  $k=ξ$  shape parameter nparams : number of parameters in  log_weights ndraws : number of draws in  log_weights nchains : number of chains in  log_weights reff : the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size. ess : estimated effective sample size of estimate of mean using smoothed importance samples (see  ess_is ) tail_length : length of the upper tail of  log_weights  that was smoothed tail_dist : the generalized Pareto distribution that was fit to the tail of  log_weights . Note that the tail weights are scaled to have a maximum of 1, so  tail_dist * exp(maximum(log_ratios))  is the corresponding fit directly to the tail of  log_ratios . normalized::Bool :indicates whether  log_weights  are log-normalized along the sample dimensions. Diagnostic The  pareto_shape  parameter  $k=ξ$  of the generalized Pareto distribution  tail_dist  can be used to diagnose reliability and convergence of estimates using the importance weights  [VehtariSimpson2021] . if  $k < \\frac{1}{3}$ , importance sampling is stable, and importance sampling (IS) and PSIS both are reliable. if  $k ≤ \\frac{1}{2}$ , then the importance ratio distributon has finite variance, and the central limit theorem holds. As  $k$  approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE. if  $\\frac{1}{2} < k ≤ 0.7$ , then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime. if  $0.7 < k ≤ 1$ , then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended. if  $k > 1$ , then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes. See  PSISPlots.paretoshapeplot  for a diagnostic plot."},{"id":80,"pagetitle":"Stats","title":"PSIS.ess_is","ref":"/ArviZ/stable/api/stats/#PSIS.ess_is","content":" PSIS.ess_is  —  Function ess_is(weights; reff=1) Estimate effective sample size (ESS) for importance sampling over the sample dimensions. Given normalized weights  $w_{1:n}$ , the ESS is estimated using the L2-norm of the weights: \\[\\mathrm{ESS}(w_{1:n}) = \\frac{r_{\\mathrm{eff}}}{\\sum_{i=1}^n w_i^2}\\] where  $r_{\\mathrm{eff}}$  is the relative efficiency of the  log_weights . ess_is(result::PSISResult; bad_shape_nan=true) Estimate ESS for Pareto-smoothed importance sampling. Note ESS estimates for Pareto shape values  $k > 0.7$ , which are unreliable and misleadingly high, are set to  NaN . To avoid this, set  bad_shape_nan=false ."},{"id":81,"pagetitle":"Stats","title":"PSIS.PSISPlots.paretoshapeplot","ref":"/ArviZ/stable/api/stats/#PSIS.PSISPlots.paretoshapeplot","content":" PSIS.PSISPlots.paretoshapeplot  —  Function paretoshapeplot(values; showlines=false, ...)\nparetoshapeplot!(values; showlines=false, kwargs...) Plot shape parameters of fitted Pareto tail distributions for diagnosing convergence. values  may be either a vector of Pareto shape parameters or a  PSIS.PSISResult . If  showlines==true , horizontal lines indicating relevant Pareto shape thresholds are drawn. See  PSIS.PSISResult  for an explanation of the thresholds. All remaining  kwargs  are forwarded to the plotting function. See  psis ,  PSISResult . Examples using PSIS, Distributions, Plots\nproposal = Normal()\ntarget = TDist(7)\nx = rand(proposal, 1_000, 100)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios)\nparetoshapeplot(result) We can also plot the Pareto shape parameters directly: paretoshapeplot(result.pareto_shape) We can also use  plot  directly: plot(result.pareto_shape; showlines=true)"},{"id":82,"pagetitle":"Stats","title":"PSIS.psis","ref":"/ArviZ/stable/api/stats/#PSIS.psis","content":" PSIS.psis  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot."},{"id":83,"pagetitle":"Stats","title":"PSIS.psis!","ref":"/ArviZ/stable/api/stats/#PSIS.psis!","content":" PSIS.psis!  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot."},{"id":84,"pagetitle":"Stats","title":"LOO and WAIC","ref":"/ArviZ/stable/api/stats/#LOO-and-WAIC","content":" LOO and WAIC"},{"id":85,"pagetitle":"Stats","title":"PosteriorStats.AbstractELPDResult","ref":"/ArviZ/stable/api/stats/#PosteriorStats.AbstractELPDResult","content":" PosteriorStats.AbstractELPDResult  —  Type abstract type AbstractELPDResult An abstract type representing the result of an ELPD computation. Every subtype stores estimates of both the expected log predictive density ( elpd ) and the effective number of parameters  p , as well as standard errors and pointwise estimates of each, from which other relevant estimates can be computed. Subtypes implement the following functions: elpd_estimates information_criterion"},{"id":86,"pagetitle":"Stats","title":"PosteriorStats.PSISLOOResult","ref":"/ArviZ/stable/api/stats/#PosteriorStats.PSISLOOResult","content":" PosteriorStats.PSISLOOResult  —  Type Results of Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO). See also:  loo ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates psis_result : Pareto-smoothed importance sampling (PSIS) results"},{"id":87,"pagetitle":"Stats","title":"PosteriorStats.WAICResult","ref":"/ArviZ/stable/api/stats/#PosteriorStats.WAICResult","content":" PosteriorStats.WAICResult  —  Type Results of computing the widely applicable information criterion (WAIC). See also:  waic ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates"},{"id":88,"pagetitle":"Stats","title":"PosteriorStats.elpd_estimates","ref":"/ArviZ/stable/api/stats/#PosteriorStats.elpd_estimates","content":" PosteriorStats.elpd_estimates  —  Function elpd_estimates(result::AbstractELPDResult; pointwise=false) -> (; elpd, elpd_mcse, lpd) Return the (E)LPD estimates from the  result ."},{"id":89,"pagetitle":"Stats","title":"PosteriorStats.information_criterion","ref":"/ArviZ/stable/api/stats/#PosteriorStats.information_criterion","content":" PosteriorStats.information_criterion  —  Function information_criterion(elpd, scale::Symbol) Compute the information criterion for the given  scale  from the  elpd  estimate. scale  must be one of  (:deviance, :log, :negative_log) . See also:  loo ,  waic information_criterion(result::AbstractELPDResult, scale::Symbol; pointwise=false) Compute information criterion for the given  scale  from the existing ELPD  result . scale  must be one of  (:deviance, :log, :negative_log) . If  pointwise=true , then pointwise estimates are returned."},{"id":90,"pagetitle":"Stats","title":"PosteriorStats.loo","ref":"/ArviZ/stable/api/stats/#PosteriorStats.loo","content":" PosteriorStats.loo  —  Function loo(log_likelihood; reff=nothing, kwargs...) -> PSISLOOResult{<:NamedTuple,<:NamedTuple} Compute the Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO).  [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . Keywords reff::Union{Real,AbstractArray{<:Real}} : The relative effective sample size(s) of the  likelihood  values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using  MCMCDiagnosticTools.ess . kwargs : Remaining keywords are forwarded to [ PSIS.psis ]. See also:  PSISLOOResult ,  waic Examples Manually compute  $R_\\mathrm{eff}$  and calculate PSIS-LOO of a model: julia> using ArviZExampleData, MCMCDiagnosticTools\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> reff = ess(log_like; kind=:basic, split_chains=1, relative=true);\n\njulia> loo(log_like; reff)\nPSISLOOResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.34\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  7 (87.5%)  151\n  (0.5, 0.7]  okay  1 (12.5%)  446 loo(data::Dataset; [var_name::Symbol,] kwargs...) -> PSISLOOResult{<:NamedTuple,<:Dataset}\nloo(data::InferenceData; [var_name::Symbol,] kwargs...) -> PSISLOOResult{<:NamedTuple,<:Dataset} Compute PSIS-LOO from log-likelihood values in  data . If more than one log-likelihood variable is present, then  var_name  must be provided. Examples Calculate PSIS-LOO of a model: julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo(idata)\nPSISLOOResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.34\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  6 (75.0%)  135\n  (0.5, 0.7]  okay  2 (25.0%)  421"},{"id":91,"pagetitle":"Stats","title":"PosteriorStats.waic","ref":"/ArviZ/stable/api/stats/#PosteriorStats.waic","content":" PosteriorStats.waic  —  Function waic(log_likelihood::AbstractArray) -> WAICResult{<:NamedTuple,<:NamedTuple} Compute the widely applicable information criterion (WAIC). [Watanabe2010] [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . See also:  WAICResult ,  loo Examples Calculate WAIC of a model: julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> waic(log_like)\nWAICResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.33 waic(data::Dataset; [var_name::Symbol]) -> WAICResult{<:NamedTuple,<:Dataset}\nwaic(data::InferenceData; [var_name::Symbol]) -> WAICResult{<:NamedTuple,<:Dataset} Compute WAIC from log-likelihood values in  data . If more than one log-likelihood variable is present, then  var_name  must be provided. Examples Calculate WAIC of a model: julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> waic(idata)\nWAICResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.33"},{"id":92,"pagetitle":"Stats","title":"Model comparison","ref":"/ArviZ/stable/api/stats/#Model-comparison","content":" Model comparison"},{"id":93,"pagetitle":"Stats","title":"PosteriorStats.ModelComparisonResult","ref":"/ArviZ/stable/api/stats/#PosteriorStats.ModelComparisonResult","content":" PosteriorStats.ModelComparisonResult  —  Type ModelComparisonResult Result of model comparison using ELPD. This struct implements the Tables and TableTraits interfaces. Each field returns a collection of the corresponding entry for each model: name : Names of the models, if provided. rank : Ranks of the models (ordered by decreasing ELPD) elpd_diff : ELPD of a model subtracted from the largest ELPD of any model elpd_diff_mcse : Monte Carlo standard error of the ELPD difference weight : Model weights computed with  weights_method elpd_result :  AbstactELPDResult s for each model, which can be used to access useful stats like ELPD estimates, pointwise estimates, and Pareto shape values for PSIS-LOO weights_method : Method used to compute model weights with  model_weights"},{"id":94,"pagetitle":"Stats","title":"PosteriorStats.compare","ref":"/ArviZ/stable/api/stats/#PosteriorStats.compare","content":" PosteriorStats.compare  —  Function compare(models; kwargs...) -> ModelComparisonResult Compare models based on their expected log pointwise predictive density (ELPD). The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353 Arguments models : a  Tuple ,  NamedTuple , or  AbstractVector  whose values are either  AbstractELPDResult  entries or any argument to  elpd_method . Keywords weights_method::AbstractModelWeightsMethod=Stacking() : the method to be used to weight the models. See  model_weights  for details elpd_method=loo : a method that computes an  AbstractELPDResult  from an argument in  models . sort::Bool=true : Whether to sort models by decreasing ELPD. Returns ModelComparisonResult : A container for the model comparison results. The fields contain a similar collection to  models . Examples Compare the centered and non centered models of the eight school problem using the defaults:  loo  and  Stacking  weights. A custom  myloo  method formates the inputs as expected by  loo . julia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> function myloo(idata)\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n\njulia> mc = compare(models; elpd_method=myloo)\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\nModelComparisonResult with Stacking weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0       1.0  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067     0.0  0.9   ⋯\n                                                                1 column omitted\njulia> mc.weight |> pairs\npairs(::NamedTuple) with 2 entries:\n  :non_centered => 1.0\n  :centered     => 5.34175e-19 Compare the same models from pre-computed PSIS-LOO results and computing  BootstrappedPseudoBMA  weights: julia> elpd_results = mc.elpd_result;\n\njulia> compare(elpd_results; weights_method=BootstrappedPseudoBMA())\nModelComparisonResult with BootstrappedPseudoBMA weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0      0.52  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067    0.48  0.9   ⋯\n                                                                1 column omitted"},{"id":95,"pagetitle":"Stats","title":"PosteriorStats.model_weights","ref":"/ArviZ/stable/api/stats/#PosteriorStats.model_weights","content":" PosteriorStats.model_weights  —  Function model_weights(elpd_results; method=Stacking())\nmodel_weights(method::AbstractModelWeightsMethod, elpd_results) Compute weights for each model in  elpd_results  using  method . elpd_results  is a  Tuple ,  NamedTuple , or  AbstractVector  with  AbstractELPDResult  entries. The weights are returned in the same type of collection. Stacking  is the recommended approach, as it performs well even when the true data generating process is not included among the candidate models. See  [YaoVehtari2018]  for details. See also:  AbstractModelWeightsMethod ,  compare Examples Compute  Stacking  weights for two models: julia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> elpd_results = map(models) do idata\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\n\njulia> model_weights(elpd_results; method=Stacking()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 5.34175e-19\n  :non_centered => 1.0 Now we compute  BootstrappedPseudoBMA  weights for the same models: julia> model_weights(elpd_results; method=BootstrappedPseudoBMA()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 0.483723\n  :non_centered => 0.516277 The following model weighting methods are available"},{"id":96,"pagetitle":"Stats","title":"PosteriorStats.AbstractModelWeightsMethod","ref":"/ArviZ/stable/api/stats/#PosteriorStats.AbstractModelWeightsMethod","content":" PosteriorStats.AbstractModelWeightsMethod  —  Type abstract type AbstractModelWeightsMethod An abstract type representing methods for computing model weights. Subtypes implement  model_weights (method, elpd_results) ."},{"id":97,"pagetitle":"Stats","title":"PosteriorStats.BootstrappedPseudoBMA","ref":"/ArviZ/stable/api/stats/#PosteriorStats.BootstrappedPseudoBMA","content":" PosteriorStats.BootstrappedPseudoBMA  —  Type struct BootstrappedPseudoBMA{R<:Random.AbstractRNG, T<:Real} <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging using Akaike-type weighting with the Bayesian bootstrap (pseudo-BMA+) [YaoVehtari2018] . The Bayesian bootstrap stabilizes the model weights. BootstrappedPseudoBMA(; rng=Random.default_rng(), samples=1_000, alpha=1)\nBootstrappedPseudoBMA(rng, samples, alpha) Construct the method. rng::Random.AbstractRNG : The random number generator to use for the Bayesian bootstrap samples::Int64 : The number of samples to draw for bootstrapping alpha::Real : The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. The default (1) corresponds to a uniform distribution on the simplex. See also:  Stacking"},{"id":98,"pagetitle":"Stats","title":"PosteriorStats.PseudoBMA","ref":"/ArviZ/stable/api/stats/#PosteriorStats.PseudoBMA","content":" PosteriorStats.PseudoBMA  —  Type struct PseudoBMA <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging (pseudo-BMA) and Akaike-type weighting. PseudoBMA(; regularize=false)\nPseudoBMA(regularize) Construct the method with optional regularization of the weights using the standard error of the ELPD estimate. Note This approach is not recommended, as it produces unstable weight estimates. It is recommended to instead use  BootstrappedPseudoBMA  to stabilize the weights or  Stacking . For details, see  [YaoVehtari2018] . See also:  Stacking"},{"id":99,"pagetitle":"Stats","title":"PosteriorStats.Stacking","ref":"/ArviZ/stable/api/stats/#PosteriorStats.Stacking","content":" PosteriorStats.Stacking  —  Type struct Stacking{O<:Optim.AbstractOptimizer} <: AbstractModelWeightsMethod Model weighting using stacking of predictive distributions [YaoVehtari2018] . Stacking(; optimizer=Optim.LBFGS(), options=Optim.Options()\nStacking(optimizer[, options]) Construct the method, optionally customizing the optimization. optimizer::Optim.AbstractOptimizer : The optimizer to use for the optimization of the weights. The optimizer must support projected gradient optimization via a  manifold  field. options::Optim.Options : The Optim options to use for the optimization of the weights. See also:  BootstrappedPseudoBMA"},{"id":100,"pagetitle":"Stats","title":"Predictive checks","ref":"/ArviZ/stable/api/stats/#Predictive-checks","content":" Predictive checks"},{"id":101,"pagetitle":"Stats","title":"PosteriorStats.loo_pit","ref":"/ArviZ/stable/api/stats/#PosteriorStats.loo_pit","content":" PosteriorStats.loo_pit  —  Function loo_pit(y, y_pred, log_weights; kwargs...) -> Union{Real,AbstractArray} Compute leave-one-out probability integral transform (LOO-PIT) checks. Arguments y : array of observations with shape  (params...,) y_pred : array of posterior predictive samples with shape  (draws, chains, params...) . log_weights : array of normalized log LOO importance weights with shape  (draws, chains, params...) . Keywords is_discrete : If not provided, then it is set to  true  iff elements of  y  and  y_pred  are all integer-valued. If  true , then data are smoothed using  smooth_data  to make them non-discrete before estimating LOO-PIT values. kwargs : Remaining keywords are forwarded to  smooth_data  if data is discrete. Returns pitvals : LOO-PIT values with same size as  y . If  y  is a scalar, then  pitvals  is a scalar. LOO-PIT is a marginal posterior predictive check. If  $y_{-i}$  is the array  $y$  of observations with the  $i$ th observation left out, and  $y_i^*$  is a posterior prediction of the  $i$ th observation, then the LOO-PIT value for the  $i$ th observation is defined as \\[P(y_i^* \\le y_i \\mid y_{-i}) = \\int_{-\\infty}^{y_i} p(y_i^* \\mid y_{-i}) \\mathrm{d} y_i^*\\] The LOO posterior predictions and the corresponding observations should have similar distributions, so if conditional predictive distributions are well-calibrated, then all LOO-PIT values should be approximately uniformly distributed on  $[0, 1]$ . [Gabry2019] Examples Calculate LOO-PIT values using as test quantity the observed values themselves. julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> y = idata.observed_data.obs;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.obs, (:draw, :chain, :school));\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> log_weights = loo(log_like).psis_result.log_weights;\n\njulia> loo_pit(y, y_pred, log_weights)\n8-element DimArray{Float64,1} with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 Calculate LOO-PIT values using as test quantity the square of the difference between each observation and  mu . julia> using Statistics\n\njulia> mu = idata.posterior.mu;\n\njulia> T = y .- median(mu);\n\njulia> T_pred = y_pred .- mu;\n\njulia> loo_pit(T .^ 2, T_pred .^ 2, log_weights)\n8-element DimArray{Float64,1} with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.873577\n \"Deerfield\"         0.243686\n \"Phillips Andover\"  0.357563\n \"Phillips Exeter\"   0.149908\n \"Hotchkiss\"         0.435094\n \"Lawrenceville\"     0.220627\n \"St. Paul's\"        0.775086\n \"Mt. Hermon\"        0.296706 loo_pit(idata::InferenceData, log_weights; kwargs...) -> DimArray Compute LOO-PIT values using existing normalized log LOO importance weights. Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. kwargs : Remaining keywords are forwarded to the base method of  loo_pit . Examples Calculate LOO-PIT values using already computed log weights. julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo_result = loo(idata; var_name=:obs);\n\njulia> loo_pit(idata, loo_result.psis_result.log_weights; y_name=:obs)\n8-element DimArray{Float64,1} loo_pit_obs with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 loo_pit(idata::InferenceData; kwargs...) -> DimArray Compute LOO-PIT from groups in  idata  using PSIS-LOO. Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. log_likelihood_name : Name of log-likelihood variable in  idata.log_likelihood . If not provided, then  y_name  is used if  idata  has a  log_likelihood  group, otherwise the only variable is used. reff::Union{Real,AbstractArray{<:Real}} : The relative effective sample size(s) of the  likelihood  values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using  ess . kwargs : Remaining keywords are forwarded to the base method of  loo_pit . Examples Calculate LOO-PIT values using as test quantity the observed values themselves. julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo_pit(idata; y_name=:obs)\n8-element DimArray{Float64,1} loo_pit_obs with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275"},{"id":102,"pagetitle":"Stats","title":"Utilities","ref":"/ArviZ/stable/api/stats/#Utilities","content":" Utilities"},{"id":103,"pagetitle":"Stats","title":"PosteriorStats.smooth_data","ref":"/ArviZ/stable/api/stats/#PosteriorStats.smooth_data","content":" PosteriorStats.smooth_data  —  Function smooth_data(y; dims=:, interp_method=CubicSpline, offset_frac=0.01) Smooth  y  along  dims  using  interp_method . interp_method  is a 2-argument callabale that takes the arguments  y  and  x  and returns a DataInterpolations.jl interpolation method, defaulting to a cubic spline interpolator. offset_frac  is the fraction of the length of  y  to use as an offset when interpolating. Hyndman1996 Rob J. Hyndman (1996) Computing and Graphing Highest Density Regions,             Amer. Stat., 50(2): 120-6.             DOI:  10.1080/00031305.1996.10474359 jstor . ChenShao1999 Ming-Hui Chen & Qi-Man Shao (1999)              Monte Carlo Estimation of Bayesian Credible and HPD Intervals,              J Comput. Graph. Stat., 8:1, 69-92.              DOI:  10.1080/10618600.1999.10474802 jstor . GelmanGoodrich2019 Andrew Gelman, Ben Goodrich, Jonah Gabry & Aki Vehtari (2019) R-squared for Bayesian Regression Models, The American Statistician, 73:3, 307-9, DOI:  10.1080/00031305.2018.1549100 . VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html Watanabe2010 Watanabe, S. Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. 11(116):3571−3594, 2010. https://jmlr.csail.mit.edu/papers/v11/watanabe10a.html Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 Gabry2019 Gabry, J., Simpson, D., Vehtari, A., Betancourt, M. & Gelman, A. Visualization in Bayesian Workflow. J. R. Stat. Soc. Ser. A Stat. Soc. 182, 389–402 (2019). doi:  10.1111/rssa.12378  arXiv:  1709.01449"},{"id":106,"pagetitle":"Creating custom plots","title":"Environment","ref":"/ArviZ/stable/creating_custom_plots/#Environment","content":" Environment using Pkg, InteractiveUtils using PlutoUI with_terminal(Pkg.status; color=false) Status `~/work/ArviZ.jl/ArviZ.jl/docs/Project.toml`\n  [cbdf2221] AlgebraOfGraphics v0.6.16\n  [131c737c] ArviZ v0.10.2 `~/work/ArviZ.jl/ArviZ.jl`\n  [2f96bb34] ArviZExampleData v0.1.5\n  [4a6e88f0] ArviZPythonPlots v0.1.0\n  [13f3f980] CairoMakie v0.10.7\n  [a93c6f00] DataFrames v1.6.1\n  [0703355e] DimensionalData v0.24.14\n  [31c24e10] Distributions v0.25.100\n  [e30172f5] Documenter v0.27.25\n  [f6006082] EvoTrees v0.15.2\n  [b5cf5a8d] InferenceObjects v0.3.11\n  [be115224] MCMCDiagnosticTools v0.3.5\n  [a7f614a8] MLJBase v0.21.13\n  [614be32b] MLJIteration v0.5.1\n  [ce719bf2] PSIS v0.9.2\n  [359b1769] PlutoStaticHTML v6.0.14\n  [7f904dfe] PlutoUI v0.7.52\n  [7f36be82] PosteriorStats v0.1.2\n  [c1514b29] StanSample v7.4.2\n  [2913bbd2] StatsBase v0.34.0\n  [fce5fe82] Turing v0.28.1\n  [f43a241f] Downloads v1.6.0\n  [37e2e46d] LinearAlgebra\n  [10745b16] Statistics v1.9.0\n with_terminal(versioninfo) Julia Version 1.9.2\nCommit e4ee485e909 (2023-07-05 09:39 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 2 × Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, skylake-avx512)\n  Threads: 2 on 2 virtual cores\nEnvironment:\n  JULIA_PKG_SERVER_REGISTRY_PREFERENCE = eager\n  JULIA_PROJECT = /home/runner/work/ArviZ.jl/ArviZ.jl/docs/\n  JULIA_DEPOT_PATH = /home/runner/.julia:/opt/hostedtoolcache/julia/1.9.2/x64/local/share/julia:/opt/hostedtoolcache/julia/1.9.2/x64/share/julia\n  JULIA_NUM_THREADS = 2\n  JULIA_LOAD_PATH = @:@v#.#:@stdlib\n  JULIA_CMDSTAN_HOME = /home/runner/work/ArviZ.jl/ArviZ.jl/.cmdstan//cmdstan-2.25.0/\n  JULIA_REVISE_WORKER_ONLY = 1\n"},{"id":113,"pagetitle":"Working with InferenceData","title":"Working with  InferenceData","ref":"/ArviZ/stable/working_with_inference_data/#working-with-inference-data","content":" Working with  InferenceData using ArviZ, ArviZExampleData, DimensionalData, Statistics Here we present a collection of common manipulations you can use while working with  InferenceData . Let's load one of ArviZ's example datasets.  posterior ,  posterior_predictive , etc are the groups stored in  idata , and they are stored as  Dataset s. In this HTML view, you can click a group name to expand a summary of the group. idata = load_example_data(\"centered_eight\") InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" Info Dataset s are  DimensionalData.AbstractDimStack s and can be used identically.   The variables a  Dataset  contains are called \"layers\", and dimensions of the same name that appear in more than one layer within a  Dataset  must have the same indices. InferenceData  behaves like a  NamedTuple  and can be used similarly. Note that unlike a  NamedTuple , the groups always appear in a specific order. length(idata) # number of groups 8 keys(idata) # group names (:posterior, :posterior_predictive, :log_likelihood, :sample_stats, :prior, :prior_predictive, :observed_data, :constant_data)"},{"id":114,"pagetitle":"Working with InferenceData","title":"Get the dataset corresponding to a single group","ref":"/ArviZ/stable/working_with_inference_data/#Get-the-dataset-corresponding-to-a-single-group","content":" Get the dataset corresponding to a single group Group datasets can be accessed both as properties or as indexed items. post = idata.posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\" post  is the dataset itself, so this is a non-allocating operation. idata[:posterior] === post true InferenceData  supports a more advanced indexing syntax, which we'll see later."},{"id":115,"pagetitle":"Working with InferenceData","title":"Getting a new  InferenceData  with a subset of groups","ref":"/ArviZ/stable/working_with_inference_data/#Getting-a-new-InferenceData-with-a-subset-of-groups","content":" Getting a new  InferenceData  with a subset of groups We can index by a collection of group names to get a new  InferenceData  with just those groups. This is also non-allocating. idata_sub = idata[(:posterior, :posterior_predictive)] InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\""},{"id":116,"pagetitle":"Working with InferenceData","title":"Adding groups to an  InferenceData","ref":"/ArviZ/stable/working_with_inference_data/#Adding-groups-to-an-InferenceData","content":" Adding groups to an  InferenceData InferenceData  is immutable, so to add or replace groups we use  merge  to create a new object. merge(idata_sub, idata[(:observed_data, :prior)]) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" We can also use  Base.setindex  to out-of-place add or replace a single group. Base.setindex(idata_sub, idata.prior, :prior) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\""},{"id":117,"pagetitle":"Working with InferenceData","title":"Add a new variable","ref":"/ArviZ/stable/working_with_inference_data/#Add-a-new-variable","content":" Add a new variable Dataset  is also immutable. So while the values within the underlying data arrays can be mutated, layers cannot be added or removed from  Dataset s, and groups cannot be added/removed from  InferenceData . Instead, we do this out-of-place also using  merge . merge(post, (log_tau=log.(post[:tau]),)) Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 4 layers:\n  :mu      Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta   Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :log_tau Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\""},{"id":118,"pagetitle":"Working with InferenceData","title":"Obtain an array for a given parameter","ref":"/ArviZ/stable/working_with_inference_data/#Obtain-an-array-for-a-given-parameter","content":" Obtain an array for a given parameter Let’s say we want to get the values for  mu  as an array. Parameters can be accessed with either property or index syntax. post.tau 500×4 DimArray{Float64,2} tau with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\n      0        1         2        3\n   0  4.72574  1.97083   3.50128  6.07326\n   1  3.90899  2.04903   2.89324  3.77187\n   2  4.84403  2.12376   4.27329  3.17054\n   3  1.8567   3.39183  11.8965   6.00193\n   4  4.74841  4.84368   7.11325  3.28632\n   ⋮                              \n 494  8.15827  1.61268   4.96249  3.13966\n 495  7.56498  1.61268   3.56495  2.78607\n 496  2.24702  1.84816   2.55959  4.28196\n 497  1.89384  2.17459   4.08978  2.74061\n 498  5.92006  1.32755   2.72017  2.93238\n 499  4.3259   1.21199   1.91701  4.46125 post[:tau] === post.tau true To remove the dimensions, just use  parent  to retrieve the underlying array. parent(post.tau) 500×4 Matrix{Float64}:\n 4.72574   1.97083   3.50128  6.07326\n 3.90899   2.04903   2.89324  3.77187\n 4.84403   2.12376   4.27329  3.17054\n 1.8567    3.39183  11.8965   6.00193\n 4.74841   4.84368   7.11325  3.28632\n 3.51387  10.8872    7.18892  2.16314\n 4.20898   4.01889   9.0977   7.68505\n 2.6834    4.28584   7.84286  4.08612\n 1.16889   3.70403  17.1548   5.1157\n 1.21052   3.15829  16.7573   4.86939\n ⋮                            \n 2.05742   1.09087  10.8168   5.08507\n 2.72536   1.09087   2.16788  6.1552\n 5.97049   1.67101   5.19169  8.23756\n 8.15827   1.61268   4.96249  3.13966\n 7.56498   1.61268   3.56495  2.78607\n 2.24702   1.84816   2.55959  4.28196\n 1.89384   2.17459   4.08978  2.74061\n 5.92006   1.32755   2.72017  2.93238\n 4.3259    1.21199   1.91701  4.46125"},{"id":119,"pagetitle":"Working with InferenceData","title":"Get the dimension lengths","ref":"/ArviZ/stable/working_with_inference_data/#Get-the-dimension-lengths","content":" Get the dimension lengths Let’s check how many groups are in our hierarchical model. size(idata.observed_data, :school) 8"},{"id":120,"pagetitle":"Working with InferenceData","title":"Get coordinate/index values","ref":"/ArviZ/stable/working_with_inference_data/#Get-coordinate/index-values","content":" Get coordinate/index values What are the names of the groups in our hierarchical model? You can access them from the coordinate name  school  in this case. DimensionalData.index(idata.observed_data, :school) 8-element Vector{String}:\n \"Choate\"\n \"Deerfield\"\n \"Phillips Andover\"\n \"Phillips Exeter\"\n \"Hotchkiss\"\n \"Lawrenceville\"\n \"St. Paul's\"\n \"Mt. Hermon\""},{"id":121,"pagetitle":"Working with InferenceData","title":"Get a subset of chains","ref":"/ArviZ/stable/working_with_inference_data/#Get-a-subset-of-chains","content":" Get a subset of chains Let’s keep only chain 0 here. For the subset to take effect on all relevant  InferenceData  groups –  posterior ,  sample_stats ,  log_likelihood , and  posterior_predictive  – we will index  InferenceData  instead of  Dataset . Here we use DimensionalData's  At  selector. Its  other selectors  are also supported. idata[chain=At(0)] InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×1)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×1)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×1)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" Note that in this case,  prior  only has a chain of 0. If it also had the other chains, we could have passed  chain=At([0, 2])  to subset by chains 0 and 2. Warning If we used  idata[chain=[0, 2]]  without the  At  selector, this is equivalent to  idata[chain=DimensionalData.index(idata.posterior, :chain)[0, 2]] , that is,  [0, 2]  indexes an array of dimension indices, which here would error.   But if we had requested  idata[chain=[1, 2]]  we would not hit an error, but we would index the wrong chains.   So it's important to always use a selector to index by values of dimension indices."},{"id":122,"pagetitle":"Working with InferenceData","title":"Remove the first  $n$  draws (burn-in)","ref":"/ArviZ/stable/working_with_inference_data/#Remove-the-first-n-draws-(burn-in)","content":" Remove the first  $n$  draws (burn-in) Let’s say we want to remove the first 100 draws from all the chains and all  InferenceData  groups with draws. To do this we use the  ..  syntax from IntervalSets.jl, which is exported by DimensionalData. idata[draw=100 .. Inf] InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (400×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (400×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (400×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (400×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (400×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" If you check the object you will see that the groups  posterior ,  posterior_predictive ,  prior , and  sample_stats  have 400 draws compared to  idata , which has 500. The group  observed_data  has not been affected because it does not have the  draw  dimension. Alternatively, you can change a subset of groups by combining indexing styles with  merge . Here we use this to build a new  InferenceData  where we have discarded the first 100 draws only from  posterior . merge(idata, idata[(:posterior,), draw=100 .. Inf]) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×400×4)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (400×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\""},{"id":123,"pagetitle":"Working with InferenceData","title":"Compute posterior mean values along draw and chain dimensions","ref":"/ArviZ/stable/working_with_inference_data/#Compute-posterior-mean-values-along-draw-and-chain-dimensions","content":" Compute posterior mean values along draw and chain dimensions To compute the mean value of the posterior samples, do the following: mean(post) (mu = 4.485933103402338,\n theta = 4.911515591394205,\n tau = 4.124222787491913,) This computes the mean along all dimensions, discarding all dimensions and returning the result as a  NamedTuple . This may be what you wanted for  mu  and  tau , which have only two dimensions ( chain  and  draw ), but maybe not what you expected for  theta , which has one more dimension  school . You can specify along which dimension you want to compute the mean (or other functions), which instead returns a  Dataset . mean(post; dims=(:chain, :draw)) Dataset with dimensions: \n  Dim{:draw} Sampled{Float64} Float64[249.5] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Float64} Float64[1.5] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (1×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×1×1)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (1×1)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\" The singleton dimensions of  chain  and  draw  now contain meaningless indices, so you may want to discard them, which you can do with  dropdims . dropdims(mean(post; dims=(:chain, :draw)); dims=(:chain, :draw)) Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: \n  :theta Float64 dims: Dim{:school} (8)\n  :tau   Float64 dims: \n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\""},{"id":124,"pagetitle":"Working with InferenceData","title":"Renaming a dimension","ref":"/ArviZ/stable/working_with_inference_data/#Renaming-a-dimension","content":" Renaming a dimension We can rename a dimension in a  Dataset  using DimensionalData's  set  method: theta_bis = set(post.theta; school=:school_bis) 8×500×4 DimArray{Float64,3} theta with dimensions: \n  Dim{:school_bis} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\n[:, :, 1]\n                       0        …  497         498        499\n  \"Choate\"            12.3207       -0.213828   10.4025     6.66131\n  \"Deerfield\"          9.90537       1.35515     6.90741    7.41377\n  \"Phillips Andover\"  14.9516        6.98269    -4.96414   -9.3226\n  \"Phillips Exeter\"   11.0115        3.71681     3.13584    2.69192\n  \"Hotchkiss\"          5.5796   …    5.32446    -2.2243    -0.502331\n  \"Lawrenceville\"     16.9018        6.96589    -2.83504   -4.25487\n  \"St. Paul's\"        13.1981        4.9302      5.39106    7.56657\n  \"Mt. Hermon\"        15.0614        3.0586      6.38124    9.98762\n[and 3 more slices...] We can use this, for example, to broadcast functions across multiple arrays, automatically matching up shared dimensions, using  DimensionalData.broadcast_dims . theta_school_diff = broadcast_dims(-, post.theta, theta_bis) 8×500×4×8 DimArray{Float64,4} theta with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school_bis} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n[:, :, 1, 1]\n                       0         …  497        498        499\n  \"Choate\"             0.0            0.0        0.0        0.0\n  \"Deerfield\"         -2.41532        1.56898   -3.49509    0.752459\n  \"Phillips Andover\"   2.63093        7.19652  -15.3666   -15.9839\n  \"Phillips Exeter\"   -1.3092         3.93064   -7.26666   -3.96939\n  \"Hotchkiss\"         -6.74108   …    5.53829  -12.6268    -7.16364\n  \"Lawrenceville\"      4.58111        7.17972  -13.2375   -10.9162\n  \"St. Paul's\"         0.877374       5.14403   -5.01144    0.905263\n  \"Mt. Hermon\"         2.74068        3.27243   -4.02126    3.32631\n[and 31 more slices...]"},{"id":125,"pagetitle":"Working with InferenceData","title":"Compute and store posterior pushforward quantities","ref":"/ArviZ/stable/working_with_inference_data/#Compute-and-store-posterior-pushforward-quantities","content":" Compute and store posterior pushforward quantities We use “posterior pushfoward quantities” to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables. You can compute these pushforward operations and store them as a new variable in a copy of the posterior group. Here we'll create a new  InferenceData  with  theta_school_diff  in the posterior: idata_new = Base.setindex(idata, merge(post, (; theta_school_diff)), :posterior) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:school_bis} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 4 layers:\n  :mu                Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta             Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n  :tau               Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :theta_school_diff Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain}, Dim{:school_bis} (8×500×4×8)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" posterior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" log_likelihood Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×4)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" sample_stats Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points\nand 16 layers:\n  :max_energy_error    Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy_error        Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :lp                  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :index_in_trajectory Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :acceptance_rate     Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :diverging           Bool dims: Dim{:draw}, Dim{:chain} (500×4)\n  :process_time_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :n_steps             Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_start  Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :largest_eigval      Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :smallest_eigval     Union{Missing, Float64} dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size_bar       Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :step_size           Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :energy              Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :tree_depth          Int64 dims: Dim{:draw}, Dim{:chain} (500×4)\n  :perf_counter_diff   Float64 dims: Dim{:draw}, Dim{:chain} (500×4)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" prior_predictive Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0] ForwardOrdered Irregular Points\nand 1 layer:\n  :obs Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×1)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" constant_data Dataset with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :scores Float64 dims: Dim{:school} (8)\n\nwith metadata Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" Once you have these pushforward quantities in an  InferenceData , you’ll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them, or save and share the  InferenceData  object with the pushforward quantities included. Here we compute the  mcse  of  theta_school_diff : mcse(idata_new.posterior).theta_school_diff 8×8 DimArray{Float64,2} theta_school_diff with dimensions: \n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:school_bis} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n                         \"Choate\"  …     \"St. Paul's\"     \"Mt. Hermon\"\n  \"Choate\"            NaN               0.117476         0.219695\n  \"Deerfield\"           0.191463        0.16484          0.189386\n  \"Phillips Andover\"    0.255636        0.258001         0.160477\n  \"Phillips Exeter\"     0.162782        0.156724         0.144923\n  \"Hotchkiss\"           0.282881   …    0.283969         0.189015\n  \"Lawrenceville\"       0.259065        0.251988         0.178094\n  \"St. Paul's\"          0.117476      NaN                0.222054\n  \"Mt. Hermon\"          0.219695        0.222054       NaN"},{"id":126,"pagetitle":"Working with InferenceData","title":"Advanced subsetting","ref":"/ArviZ/stable/working_with_inference_data/#Advanced-subsetting","content":" Advanced subsetting To select the value corresponding to the difference between the Choate and Deerfield schools do: school_idx = [\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"]\nschool_bis_idx = [\"Deerfield\", \"Choate\", \"Lawrenceville\"]\ntheta_school_diff[school=At(school_idx), school_bis=At(school_bis_idx)] 3×500×4×3 DimArray{Float64,4} theta with dimensions: \n  Dim{:school} Categorical{String} String[\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"] Unordered,\n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, 2, 3] ForwardOrdered Irregular Points,\n  Dim{:school_bis} Categorical{String} String[\"Deerfield\", \"Choate\", \"Lawrenceville\"] Unordered\n[:, :, 1, 1]\n                 0         1        …  497        498         499\n  \"Choate\"       2.41532   2.1563       -1.56898    3.49509    -0.752459\n  \"Hotchkiss\"   -4.32577  -1.31781       3.96931   -9.13171    -7.9161\n  \"Mt. Hermon\"   5.156    -2.9526        1.70345   -0.526168    2.57385\n[and 11 more slices...]"},{"id":127,"pagetitle":"Working with InferenceData","title":"Add new chains using  cat","ref":"/ArviZ/stable/working_with_inference_data/#Add-new-chains-using-cat","content":" Add new chains using  cat Suppose after checking the  mcse  and realizing you need more samples, you rerun the model with two chains and obtain an  idata_rerun  object. idata_rerun = InferenceData(; posterior=set(post[chain=At([0, 1])]; chain=[4, 5])) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[4, 5] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×2)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×2)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×2)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\" You can combine the two using  cat . cat(idata[[:posterior]], idata_rerun; dims=:chain) InferenceData posterior Dataset with dimensions: \n  Dim{:draw} Sampled{Int64} Int64[0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  Dim{:chain} Sampled{Int64} Int64[0, 1, …, 4, 5] ForwardOrdered Irregular Points,\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:draw}, Dim{:chain} (500×6)\n  :theta Float64 dims: Dim{:school}, Dim{:draw}, Dim{:chain} (8×500×6)\n  :tau   Float64 dims: Dim{:draw}, Dim{:chain} (500×6)\n\nwith metadata Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\""},{"id":130,"pagetitle":"Home","title":"ArviZPythonPlots.jl","ref":"/ArviZPythonPlots/stable/#ArviZPythonPlots.jl","content":" ArviZPythonPlots.jl ArviZPythonPlots.jl provides PyPlot-compatible plotting functions for exploratory analysis of Bayesian models using  ArviZ.jl . It uses  PythonCall.jl  to provide an interface for using the plotting functions in  Python ArviZ  with Julia types. It also re-exports all methods exported by  PythonPlot.jl . For details, see the  Example Gallery  or the  API ."},{"id":131,"pagetitle":"Home","title":"Installation","ref":"/ArviZPythonPlots/stable/#installation","content":" Installation To install ArviZPythonPlots.jl, we first need to install Python ArviZ. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run pkg> add ArviZPythonPlots"},{"id":134,"pagetitle":"API Overview","title":"API Overview","ref":"/ArviZPythonPlots/stable/api/#api","content":" API Overview Plotting styles rcParams Plotting functions"},{"id":137,"pagetitle":"Plotting functions","title":"Plotting functions","ref":"/ArviZPythonPlots/stable/api/plots/#plots-api","content":" Plotting functions ArviZPythonPlots.plot_autocorr ArviZPythonPlots.plot_bf ArviZPythonPlots.plot_bpv ArviZPythonPlots.plot_compare ArviZPythonPlots.plot_density ArviZPythonPlots.plot_dist ArviZPythonPlots.plot_dist_comparison ArviZPythonPlots.plot_dot ArviZPythonPlots.plot_ecdf ArviZPythonPlots.plot_elpd ArviZPythonPlots.plot_energy ArviZPythonPlots.plot_ess ArviZPythonPlots.plot_forest ArviZPythonPlots.plot_hdi ArviZPythonPlots.plot_kde ArviZPythonPlots.plot_khat ArviZPythonPlots.plot_lm ArviZPythonPlots.plot_loo_pit ArviZPythonPlots.plot_mcse ArviZPythonPlots.plot_pair ArviZPythonPlots.plot_parallel ArviZPythonPlots.plot_posterior ArviZPythonPlots.plot_ppc ArviZPythonPlots.plot_rank ArviZPythonPlots.plot_separation ArviZPythonPlots.plot_trace ArviZPythonPlots.plot_violin"},{"id":138,"pagetitle":"Plotting functions","title":"Reference","ref":"/ArviZPythonPlots/stable/api/plots/#Reference","content":" Reference"},{"id":139,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_autocorr","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_autocorr-Tuple","content":" ArviZPythonPlots.plot_autocorr  —  Method Bar plot of the autocorrelation function (ACF) for a sequence of data.\n\n    The ACF plots are helpful as a convergence diagnostic for posteriors from MCMC\n    samples which display autocorrelation.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    max_lag : int, optional\n        Maximum lag to calculate autocorrelation. By Default, the plot displays the\n        first 100 lag or the total number of draws, whichever is smaller.\n    combined : bool, default False\n        Flag for combining multiple chains into a single chain. If False, chains will be\n        plotted separately.\n    grid : tuple, optional\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n        Note this is not used if `ax` is supplied.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_config : dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in ``rcParams``.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    autocov : Compute autocovariance estimates for every lag for the input array.\n    autocorr : Compute autocorrelation using FFT for every lag for the input array.\n\n    Examples\n    --------\n    Plot default autocorrelation\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_autocorr(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'] )\n\n\n    Combine chains by variable and select variables by excluding some with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['~thet'], filter_vars=\"like\", combined=True)\n\n\n    Specify maximum lag (x axis bound)\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'], max_lag=200, combined=True)\n    \n source"},{"id":140,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_bf","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_bf-Tuple","content":" ArviZPythonPlots.plot_bf  —  Method Approximated Bayes Factor for comparing hypothesis of two nested models.\n\n    The Bayes factor is estimated by comparing a model (H1) against a model in which the\n    parameter of interest has been restricted to be a point-null (H0). This computation\n    assumes the models are nested and thus H0 is a special case of H1.\n\n    Notes\n    -----\n    The bayes Factor is approximated pproximated as the Savage-Dickey density ratio\n    algorithm presented in [1]_.\n\n    Parameters\n    -----------\n    idata : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    var_name : str, optional\n        Name of variable we want to test.\n    prior : numpy.array, optional\n        In case we want to use different prior, for example for sensitivity analysis.\n    ref_val : int, default 0\n        Point-null for Bayes factor estimation.\n    xlim :  tuple, optional\n        Set the x limits, which might be used for visualization purposes.\n    colors : tuple, default ('C0', 'C1')\n        Tuple of valid Matplotlib colors. First element for the prior, second for the posterior.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    textsize: float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`.\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :func:`matplotlib.pyplot.plot`.\n    hist_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist`. Only works for discrete variables.\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend :{\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    dict : A dictionary with BF10 (Bayes Factor 10 (H1/H0 ratio), and BF01 (H0/H1 ratio).\n    axes : matplotlib_axes or bokeh_figure\n\n    References\n    ----------\n    .. [1] Heck, D., 2019. A caveat on the avage-Dickey density ratio:\n    The case of computing Bayes factors for regression parameters.\n\n    Examples\n    --------\n    Moderate evidence indicating that the parameter \"a\" is different from zero.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> idata = az.from_dict(posterior={\"a\":np.random.normal(1, 0.5, 5000)},\n        ...     prior={\"a\":np.random.normal(0, 1, 5000)})\n        >>> az.plot_bf(idata, var_name=\"a\", ref_val=0)\n    \n source"},{"id":141,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_bpv","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_bpv-Tuple","content":" ArviZPythonPlots.plot_bpv  —  Method Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n\n    Parameters\n    ----------\n    data : InferenceData\n        :class:`arviz.InferenceData` object containing the observed and\n        posterior/prior predictive data.\n    kind : {\"u_value\", \"p_value\", \"t_stat\"}, default \"u_value\"\n        Specify the kind of plot:\n\n        * The ``kind=\"p_value\"`` computes :math:`p := p(y* \\leq y | y)`.\n          This is the probability of the data y being larger or equal than the predicted data y*.\n          The ideal value is 0.5 (half the predictions below and half above the data).\n        * The ``kind=\"u_value\"`` argument computes :math:`p_i := p(y_i* \\leq y_i | y)`.\n          i.e. like a p_value but per observation :math:`y_i`. This is also known as marginal\n          p_value. The ideal distribution is uniform. This is similar to the LOO-PIT\n          calculation/plot, the difference is than in LOO-pit plot we compute\n          :math:`pi = p(y_i* r \\leq y_i | y_{-i} )`, where :math:`y_{-i}`,\n          is all other data except :math:`y_i`.\n        * The ``kind=\"t_stat\"`` argument computes :math:`:= p(T(y)* \\leq T(y) | y)`\n          where T is any test statistic. See ``t_stat`` argument below for details\n          of available options.\n\n    t_stat : str, float, or callable, default \"median\"\n        Test statistics to compute from the observations and predictive distributions.\n        Allowed strings are “mean”, “median” or “std”. Alternative a quantile can be passed\n        as a float (or str) in the interval (0, 1). Finally a user defined function is also\n        acepted, see examples section for details.\n    bpv : bool, default True\n        If True add the Bayesian p_value to the legend when ``kind = t_stat``.\n    plot_mean : bool, default True\n        Whether or not to plot the mean test statistic.\n    reference : {\"analytical\", \"samples\", None}, default \"analytical\"\n        How to compute the distributions used as reference for ``kind=u_values``\n        or ``kind=p_values``. Use `None` to not plot any reference.\n    mse : bool, default False\n        Show scaled mean square error between uniform distribution and marginal p_value\n        distribution.\n    n_ref : int, default 100\n        Number of reference distributions to sample when ``reference=samples``.\n    hdi_prob : float, optional\n        Probability for the highest density interval for the analytical reference distribution when\n        ``kind=u_values``. Should be in the interval (0, 1]. Defaults to the\n        rcParam ``stats.hdi_prob``. See :ref:`this section <common_hdi_prob>` for usage examples.\n    color : str, optional\n        Matplotlib color\n    grid : tuple, optional\n        Number of rows and columns. By default, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on `figsize`.\n    data_pairs : dict, optional\n        Dictionary containing relations between observed data and posterior/prior predictive data.\n        Dictionary structure:\n\n        - key = data var_name\n        - value = posterior/prior predictive var_name\n\n        For example, ``data_pairs = {'y' : 'y_hat'}``\n        If None, it will assume that the observed data and the posterior/prior\n        predictive data have the same variable name.\n    Labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    var_names : list of str, optional\n        Variables to be plotted. If `None` all variable are plotted. Prefix the variables by ``~``\n        when you want to exclude them from the plot. See the :ref:`this section <common_var_names>`\n        for usage examples. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    coords : dict, optional\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None. See :ref:`this section <common_coords>` for usage examples.\n    flatten : list, optional\n        List of dimensions to flatten in observed_data. Only flattens across the coordinates\n        specified in the coords argument. Defaults to flattening all of the dimensions.\n    flatten_pp : list, optional\n        List of dimensions to flatten in posterior_predictive/prior_predictive. Only flattens\n        across the coordinates specified in the coords argument. Defaults to flattening all\n        of the dimensions. Dimensions should match flatten excluding dimensions for data_pairs\n        parameters. If `flatten` is defined and `flatten_pp` is None, then ``flatten_pp=flatten``.\n    legend : bool, default True\n        Add legend to figure.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : str, optional\n        Select plotting backend {\"matplotlib\", \"bokeh\"}. Default \"matplotlib\".\n    plot_ref_kwargs :  dict, optional\n        Extra keyword arguments to control how reference is represented.\n        Passed to :meth:`matplotlib.axes.Axes.plot` or\n        :meth:`matplotlib.axes.Axes.axhspan` (when ``kind=u_value``\n        and ``reference=analytical``).\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    group : {\"posterior\", \"prior\"}, default \"posterior\"\n        Specifies which InferenceData group should be plotted. If \"posterior\", then the values\n        in `posterior_predictive` group are compared to the ones in `observed_data`, if \"prior\" then\n        the same comparison happens, but with the values in `prior_predictive` group.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_ppc : Plot for posterior/prior predictive checks.\n    plot_loo_pit : Plot Leave-One-Out probability integral transformation (PIT) predictive checks.\n    plot_dist_comparison : Plot to compare fitted and unfitted distributions.\n\n    References\n    ----------\n    * Gelman et al. (2013) see http://www.stat.columbia.edu/~gelman/book/ pages 151-153 for details\n\n    Examples\n    --------\n    Plot Bayesian p_values.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data(\"regression1d\")\n        >>> az.plot_bpv(data, kind=\"p_value\")\n\n    Plot custom test statistic comparison.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data(\"regression1d\")\n        >>> az.plot_bpv(data, kind=\"t_stat\", t_stat=lambda x:np.percentile(x, q=50, axis=-1))\n    \n source"},{"id":142,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_compare","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_compare-Tuple","content":" ArviZPythonPlots.plot_compare  —  Method Summary plot for model comparison.\n\n    Models are compared based on their expected log pointwise predictive density (ELPD).\n    This plot is in the style of the one used in [2]_. Chapter 6 in the first edition\n    or 7 in the second.\n\n    Notes\n    -----\n    The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend LOO in line with the work presented by [1]_.\n\n    Parameters\n    ----------\n    comp_df : pandas.DataFrame\n        Result of the :func:`arviz.compare` method.\n    insample_dev : bool, default False\n        Plot in-sample ELPD, that is the value of the information criteria without the\n        penalization given by the effective number of parameters (p_loo or p_waic).\n    plot_standard_error : bool, default True\n        Plot the standard error of the ELPD.\n    plot_ic_diff : bool, default True\n        Plot standard error of the difference in ELPD between each model\n        and the top-ranked model.\n    order_by_rank : bool, default True\n        If True ensure the best model is used as reference.\n    legend : bool, default True\n        Add legend to figure.\n    figsize : (float, float), optional\n        If `None`, size is (6, num of models) inches.\n    title : bool, default True\n        Show a tittle with a description of how to interpret the plot.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    plot_kwargs : dict, optional\n        Optional arguments for plot elements. Currently accepts 'color_ic',\n        'marker_ic', 'color_insample_dev', 'marker_insample_dev', 'color_dse',\n        'marker_dse', 'ls_min_ic' 'color_ls_min_ic',  'fontsize'\n    ax : matplotlib_axes or bokeh_figure, optional\n        Matplotlib axes or bokeh figure.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_elpd : Plot pointwise elpd differences between two or more models.\n    compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n    waic : Compute the widely applicable information criterion.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2016). Practical Bayesian model evaluation using leave-one-out\n    cross-validation and WAIC https://arxiv.org/abs/1507.04544\n\n    .. [2] McElreath R. (2022). Statistical Rethinking A Bayesian Course with Examples in\n    R and Stan, Second edition, CRC Press.\n\n    Examples\n    --------\n    Show default compare plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> model_compare = az.compare({'Centered 8 schools': az.load_arviz_data('centered_eight'),\n        >>>                  'Non-centered 8 schools': az.load_arviz_data('non_centered_eight')})\n        >>> az.plot_compare(model_compare)\n\n    Include the in-sample ELDP\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_compare(model_compare, insample_dev=True)\n\n    \n source"},{"id":143,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_density","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_density-Tuple","content":" ArviZPythonPlots.plot_density  —  Method Generate KDE plots for continuous variables and histograms for discrete ones.\n\n    Plots are truncated at their 100*(1-alpha)% highest density intervals. Plots are grouped per\n    variable and colors assigned to models.\n\n    Parameters\n    ----------\n    data : InferenceData or iterable of InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object, or an Iterator\n        returning a sequence of such objects.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    group : {\"posterior\", \"prior\"}, default \"posterior\"\n        Specifies which InferenceData group should be plotted. If \"posterior\", then the values\n        in `posterior_predictive` group are compared to the ones in `observed_data`, if \"prior\" then\n        the same comparison happens, but with the values in `prior_predictive` group.\n    data_labels : list of str, default None\n        List with names for the datasets passed as \"data.\" Useful when plotting more than one\n        dataset.  Must be the same shape as the data parameter.\n    var_names : list of str, optional\n        List of variables to plot. If multiple datasets are supplied and `var_names` is not None,\n        will print the same set of variables for each dataset. Defaults to None, which results in\n        all the variables being plotted.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    transform : callable\n        Function to transform data (defaults to `None` i.e. the identity function).\n    hdi_prob : float, default 0.94\n        Probability for the highest density interval. Should be in the interval (0, 1].\n        See :ref:`this section <common_hdi_prob>` for usage examples.\n    point_estimate : str, optional\n        Plot point estimate per variable. Values should be 'mean', 'median', 'mode' or None.\n        Defaults to 'auto' i.e. it falls back to default set in ``rcParams``.\n    colors : str or list of str, optional\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplotlib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    outline : bool, default True\n        Use a line to draw KDEs and histograms.\n    hdi_markers : str\n        A valid `matplotlib.markers` like 'v', used to indicate the limits of the highest density\n        interval. Defaults to empty string (no marker).\n    shade : float, default 0\n        Alpha blending value for the shaded area under the curve, between 0 (no shade) and 1\n        (opaque).\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when `circular` is False\n        and \"taylor\" (for now) when `circular` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    circular : bool, default False\n        If True, it interprets the values passed are from a circular variable measured in radians\n        and a circular KDE is used. Only valid for 1D KDE.\n    grid : tuple, optional\n        Number of rows and columns. Defaults to ``None``, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n\n    Examples\n    --------\n    Plot default density plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> az.plot_density([centered, non_centered])\n\n    Plot variables in a 4x5 grid\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], grid=(4, 5))\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"])\n\n    Plot a specific `az.InferenceData` group\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], group=\"prior\")\n\n    Specify highest density interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], hdi_prob=.5)\n\n    Shade plots and/or remove outlines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], outline=False, shade=.8)\n\n    Specify binwidth for kernel density estimation\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], bw=.9)\n    \n source"},{"id":144,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_dist","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_dist-Tuple","content":" ArviZPythonPlots.plot_dist  —  Method Plot distribution as histogram or kernel density estimates.\n\n    By default continuous variables are plotted using KDEs and discrete ones using histograms\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot from an unknown continuous or discrete distribution.\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE or a hexbin will be estimated.\n    color : string\n        valid matplotlib color.\n    kind : string, default \"auto\"\n        By default (\"auto\") continuous variables will use the kind defined by rcParam\n        ``plot.density_kind`` and discrete ones will use histograms.\n        To override this use \"hist\" to plot histograms and \"kde\" for KDEs.\n    cumulative : bool, default False\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE.\n    label : string\n        Text to include as part of the legend.\n    rotated : bool, default False\n        Whether to rotate the 1D KDE plot 90 degrees.\n    rug : bool, default False\n        Add a `rug plot <https://en.wikipedia.org/wiki/Rug_plot>`_ for a specific subset\n        of values. Ignored for 2D KDE.\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``is_circular`` is False\n        and \"taylor\" (for now) when ``is_circular`` is True.\n        Defaults to \"experimental\" when variable is not circular and \"taylor\" when it is.\n    quantiles : list, optional\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n    contour : bool, default True\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE.\n    fill_last : bool, default True\n        If True fill the last contour of the 2D KDE plot.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`. Not implemented for bokeh backend.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE. Passed to :func:`arviz.plot_kde` as\n        ``plot_kwargs``.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use fill_kwargs={'alpha': 0} to disable fill).\n        Ignored for 2D KDE. Passed to :func:`arviz.plot_kde` as ``fill_kwargs``.\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if ``rug=False`` or for 2D KDE\n        Use ``space`` keyword (float) to control the position of the rugplot.\n        The larger this number the lower the rugplot. Passed to\n        :func:`arviz.plot_kde` as ``rug_kwargs``.\n    contour_kwargs : dict\n        Keywords passed to the contourplot. Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to :meth:`matplotlib.axes.Axes.contourf`. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to :meth:`matplotlib.axes.Axes.pcolormesh`. Ignored for 1D KDE.\n    hist_kwargs : dict\n        Keyword arguments used to customize the histogram. Ignored when plotting a KDE.\n        They are passed to :meth:`matplotlib.axes.Axes.hist` if using matplotlib,\n        or to :meth:`bokeh.plotting.figure.quad` if using bokeh. In bokeh case,\n        the following extra keywords are also supported:\n\n        * ``color``: replaces the ``fill_color`` and ``line_color`` of the ``quad`` method\n        * ``bins``: taken from ``hist_kwargs`` and passed to :func:`numpy.histogram` instead\n        * ``density``: normalize histogram to represent a probability density function,\n          Defaults to ``True``\n\n        * ``cumulative``: plot the cumulative counts. Defaults to ``False``.\n\n    is_circular : {False, True, \"radians\", \"degrees\"}, default False\n        Select input type {\"radians\", \"degrees\"} for circular histogram or KDE plot. If True,\n        default input type is \"radians\". When this argument is present, it interprets the\n        values passed are from a circular variable measured in radians and a circular KDE is\n        used. Inputs in \"degrees\" will undergo an internal conversion to radians. Only valid\n        for 1D KDE.\n    ax : matplotlib_axes or bokeh_figure, optional\n        Matplotlib or bokeh targets on which to plot. If not supplied, Arviz will create\n        its own plot area (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs :dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figure\n\n    See Also\n    --------\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    plot_kde : 1D or 2D KDE plot taking into account boundary conditions.\n\n    Examples\n    --------\n    Plot an integer distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> a = np.random.poisson(4, 1000)\n        >>> az.plot_dist(a)\n\n    Plot a continuous distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> b = np.random.normal(0, 1, 1000)\n        >>> az.plot_dist(b)\n\n    Add a rug under the Gaussian distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True)\n\n    Segment into quantiles\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True, quantiles=[.25, .5, .75])\n\n    Plot as the cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True, quantiles=[.25, .5, .75], cumulative=True)\n    \n source"},{"id":145,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_dist_comparison","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_dist_comparison-Tuple","content":" ArviZPythonPlots.plot_dist_comparison  —  Method Plot to compare fitted and unfitted distributions.\n\n    The resulting plots will show the compared distributions both on\n    separate axes (particularly useful when one of them is substantially tighter\n    than another), and plotted together, displaying a grid of three plots per\n    distribution.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        containing the posterior/prior data. Refer to documentation of\n        :func:`arviz.convert_to_dataset` for details.\n    kind : {\"latent\", \"observed\"}, default \"latent\"\n        kind of plot to display The \"latent\" option includes {\"prior\", \"posterior\"},\n        and the \"observed\" option includes\n        {\"observed_data\", \"prior_predictive\", \"posterior_predictive\"}.\n    figsize : (float, float), optional\n        Figure size. If ``None`` it will be defined automatically.\n    textsize : float\n        Text size scaling factor for labels, titles and lines. If ``None`` it will be\n        autoscaled based on `figsize`.\n    var_names : str, list, list of lists, optional\n        if str, plot the variable. if list, plot all the variables in list\n        of all groups. if list of lists, plot the vars of groups in respective lists.\n        See :ref:`this section <common_var_names>` for usage examples.\n    coords : dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. See :ref:`this section <common_coords>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    transform : callable\n        Function to transform data (defaults to `None` i.e. the identity function).\n    legend : bool\n        Add legend to figure. By default True.\n    labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : (nvars, 3) array-like of matplotlib_axes, optional\n        Matplotlib axes: The ax argument should have shape (nvars, 3), where the\n        last column is for the combined before/after plots and columns 0 and 1 are\n        for the before and after plots, respectively.\n    prior_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for prior/predictive groups.\n    posterior_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for posterior/predictive groups.\n    observed_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for observed_data group.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes\n        Returned object will have shape (nvars, 3),\n        where the last column is the combined plot and the first columns are the single plots.\n\n    See Also\n    --------\n    plot_bpv : Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n\n    Examples\n    --------\n    Plot the prior/posterior plot for specified vars and coords.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('rugby')\n        >>> az.plot_dist_comparison(data, var_names=[\"defs\"], coords={\"team\" : [\"Italy\"]})\n\n    \n source"},{"id":146,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_dot","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_dot-Tuple","content":" ArviZPythonPlots.plot_dot  —  Method Plot distribution as dot plot or quantile dot plot.\n\n    This function uses the Wilkinson's Algorithm [1]_ to allot dots to bins.\n    The quantile dot plots was inspired from [2]_.\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot from an unknown continuous or discrete distribution.\n    binwidth : float, optional\n        Width of the bin for drawing the dot plot.\n    dotsize : float, default 1\n        The size of the dots relative to the bin width. The default makes dots be\n        just about as wide as the bin width.\n    stackratio : float, default 1\n        The distance between the center of the dots in the same stack relative to the bin height.\n        The default makes dots in the same stack just touch each other.\n    point_interval : bool, default False\n        Plots the point interval. Uses ``hdi_prob`` to plot the HDI interval\n    point_estimate : str, optional\n        Plot point estimate per variable. Values should be ``mean``, ``median``, ``mode`` or None.\n        Defaults to ``auto`` i.e. it falls back to default set in rcParams.\n    dotcolor : string, optional\n        The color of the dots. Should be a valid matplotlib color.\n    intervalcolor : string, optional\n        The color of the interval. Should be a valid matplotlib color.\n    linewidth : int, default None\n        Line width throughout. If None it will be autoscaled based on `figsize`.\n    markersize : int, default None\n        Markersize throughout. If None it will be autoscaled based on `figsize`.\n    markercolor : string, optional\n        The color of the marker when plot_interval is True. Should be a valid matplotlib color.\n    marker : string, default \"o\"\n        The shape of the marker. Valid for matplotlib backend.\n    hdi_prob : float, optional\n        Valid only when point_interval is True. Plots HDI for chosen percentage of density.\n        Defaults to ``stats.hdi_prob`` rcParam. See :ref:`this section <common_hdi_prob>`\n        for usage examples.\n    rotated : bool, default False\n        Whether to rotate the dot plot by 90 degrees.\n    nquantiles : int, default 50\n        Number of quantiles to plot, used for quantile dot plots.\n    quartiles : bool, default True\n        If True then the quartile interval will be plotted with the HDI.\n    figsize : (float,float), optional\n        Figure size. If ``None`` it will be defined automatically.\n    plot_kwargs : dict, optional\n        Keywords passed for customizing the dots. Passed to :class:`mpl:matplotlib.patches.Circle`\n        in matplotlib and :meth:`bokeh.plotting.figure.circle` in bokeh.\n    backend :{\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    ax : axes, optional\n        Matplotlib_axes or bokeh_figure.\n    show : bool, optional\n        Call backend show function.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n\n    References\n    ----------\n    .. [1] Leland Wilkinson (1999) Dot Plots, The American Statistician, 53:3, 276-281,\n        DOI: 10.1080/00031305.1999.10474474\n    .. [2] Matthew Kay, Tara Kola, Jessica R. Hullman,\n        and Sean A. Munson. 2016. When (ish) is My Bus? User-centered Visualizations of Uncertainty\n        in Everyday, Mobile Predictive Systems. DOI:https://doi.org/10.1145/2858036.2858558\n\n    Examples\n    --------\n    Plot dot plot for a set of data points\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> import numpy as np\n        >>> values = np.random.normal(0, 1, 500)\n        >>> az.plot_dot(values)\n\n    Manually adjust number of quantiles to plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dot(values, nquantiles=100)\n\n    Add a point interval under the dot plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dot(values, point_interval=True)\n\n    Rotate the dot plots by 90 degrees i.e swap x and y axis\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dot(values, point_interval=True, rotated=True)\n\n    \n source"},{"id":147,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_ecdf","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_ecdf-Tuple","content":" ArviZPythonPlots.plot_ecdf  —  Method Plot ECDF or ECDF-Difference Plot with Confidence bands.\n\n    Plots of the empirical CDF estimates of an array. When `values2` argument is provided,\n    the two empirical CDFs are overlaid with the distribution of `values` on top\n    (in a darker shade) and confidence bands in a more transparent shade. Optionally, the difference\n    between the two empirical CDFs can be computed, and the PIT for a single dataset or a comparison\n    between two samples.\n\n    Notes\n    -----\n    This plot computes the confidence bands with the simulated based algorithm presented in [1]_.\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot from an unknown continuous or discrete distribution.\n    values2 : array-like, optional\n        Values to compare to the original sample.\n    cdf : callable, optional\n        Cumulative distribution function of the distribution to compare the original sample.\n    difference : bool, default False\n        If True then plot ECDF-difference plot otherwise ECDF plot.\n    pit : bool, default False\n        If True plots the ECDF or ECDF-diff of PIT of sample.\n    confidence_bands : bool, default None\n        If True plots the simultaneous or pointwise confidence bands with `1 - fpr`\n        confidence level.\n    pointwise : bool, default False\n        If True plots pointwise confidence bands otherwise simultaneous bands.\n    npoints : int, default 100\n        This denotes the granularity size of our plot i.e the number of evaluation points\n        for the ecdf or ecdf-difference plots.\n    num_trials : int, default 500\n        The number of random ECDFs to generate for constructing simultaneous confidence bands.\n    fpr : float, default 0.05\n        The type I error rate s.t `1 - fpr` denotes the confidence level of bands.\n    figsize : (float,float), optional\n        Figure size. If `None` it will be defined automatically.\n    fill_band : bool, default True\n        If True it fills in between to mark the area inside the confidence interval. Otherwise,\n        plot the border lines.\n    plot_kwargs : dict, optional\n        Additional kwargs passed to :func:`mpl:matplotlib.pyplot.step` or\n        :meth:`bokeh.plotting.figure.step`\n    fill_kwargs : dict, optional\n        Additional kwargs passed to :func:`mpl:matplotlib.pyplot.fill_between` or\n        :meth:`bokeh:bokeh.plotting.Figure.varea`\n    plot_outline_kwargs : dict, optional\n        Additional kwargs passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.line`\n    ax :axes, optional\n        Matplotlib axes or bokeh figures.\n    show : bool, optional\n        Call backend show function.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    References\n    ----------\n    .. [1] Säilynoja, T., Bürkner, P.C. and Vehtari, A., 2021. Graphical Test for\n        Discrete Uniformity and its Applications in Goodness of Fit Evaluation and\n        Multiple Sample Comparison. arXiv preprint arXiv:2103.10522.\n\n    Examples\n    --------\n    Plot ecdf plot for a given sample\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> from scipy.stats import uniform, binom, norm\n\n        >>> sample = norm(0,1).rvs(1000)\n        >>> az.plot_ecdf(sample)\n\n    Plot ecdf plot with confidence bands for comparing a given sample w.r.t a given distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> distribution = norm(0,1)\n        >>> az.plot_ecdf(sample, cdf = distribution.cdf, confidence_bands = True)\n\n    Plot ecdf-difference plot with confidence bands for comparing a given sample\n    w.r.t a given distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ecdf(sample, cdf = distribution.cdf,\n        >>>              confidence_bands = True, difference = True)\n\n    Plot ecdf plot with confidence bands for PIT of sample for comparing a given sample\n    w.r.t a given distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ecdf(sample, cdf = distribution.cdf,\n        >>>              confidence_bands = True, pit = True)\n\n    Plot ecdf-difference plot with confidence bands for PIT of sample for comparing a given\n    sample w.r.t a given distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ecdf(sample, cdf = distribution.cdf,\n        >>>              confidence_bands = True, difference = True, pit = True)\n\n    You could also plot the above w.r.t another sample rather than a given distribution.\n    For eg: Plot ecdf-difference plot with confidence bands for PIT of sample for\n    comparing a given sample w.r.t a given sample\n\n    .. plot::\n        :context: close-figs\n\n        >>> sample2 = norm(0,1).rvs(5000)\n        >>> az.plot_ecdf(sample, sample2, confidence_bands = True, difference = True, pit = True)\n\n    \n source"},{"id":148,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_elpd","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_elpd-Tuple","content":" ArviZPythonPlots.plot_elpd  —  Method Plot pointwise elpd differences between two or more models.\n\n    Pointwise model comparison based on their expected log pointwise predictive density (ELPD).\n\n    Notes\n    -----\n    The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend LOO in line with the work presented by [1]_.\n\n    Parameters\n    ----------\n    compare_dict : mapping of {str : ELPDData or InferenceData}\n        A dictionary mapping the model name to the object containing inference data or the result\n        of :func:`arviz.loo` or :func:`arviz.waic` functions.\n        Refer to :func:`arviz.convert_to_inference_data` for details on possible dict items.\n    color : str or array_like, default \"C0\"\n        Colors of the scatter plot. If color is a str all dots will have the same color.\n        If it is the size of the observations, each dot will have the specified color.\n        Otherwise, it will be interpreted as a list of the dims to be used for the color code.\n    xlabels : bool, default False\n        Use coords as xticklabels.\n    figsize : (float, float), optional\n        If `None`, size is (8 + numvars, 8 + numvars).\n    textsize : float, optional\n        Text size for labels. If `None` it will be autoscaled based on `figsize`.\n    coords : mapping, optional\n        Coordinates of points to plot. **All** values are used for computation, but only a\n        subset can be plotted for convenience. See :ref:`this section <common_coords>`\n        for usage examples.\n    legend : bool, default False\n        Include a legend to the plot. Only taken into account when color argument is a dim name.\n    threshold : float, optional\n        If some elpd difference is larger than ``threshold * elpd.std()``, show its label. If\n        `None`, no observations will be highlighted.\n    ic : str, optional\n        Information Criterion (\"loo\" for PSIS-LOO, \"waic\" for WAIC) used to compare models.\n        Defaults to ``rcParams[\"stats.information_criterion\"]``.\n        Only taken into account when input is :class:`arviz.InferenceData`.\n    scale : str, optional\n        Scale argument passed to :func:`arviz.loo` or :func:`arviz.waic`, see their docs for\n        details. Only taken into account when values in ``compare_dict`` are\n        :class:`arviz.InferenceData`.\n    var_name : str, optional\n        Argument passed to to :func:`arviz.loo` or :func:`arviz.waic`, see their docs for\n        details. Only taken into account when values in ``compare_dict`` are\n        :class:`arviz.InferenceData`.\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter`.\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_compare : Summary plot for model comparison.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n    waic : Compute the widely applicable information criterion.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2016). Practical Bayesian model evaluation using leave-one-out\n    cross-validation and WAIC https://arxiv.org/abs/1507.04544\n\n    Examples\n    --------\n    Compare pointwise PSIS-LOO for centered and non centered models of the 8-schools problem\n    using matplotlib.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata1 = az.load_arviz_data(\"centered_eight\")\n        >>> idata2 = az.load_arviz_data(\"non_centered_eight\")\n        >>> az.plot_elpd(\n        >>>     {\"centered model\": idata1, \"non centered model\": idata2},\n        >>>     xlabels=True\n        >>> )\n\n    .. bokeh-plot::\n        :source-position: above\n\n        import arviz as az\n        idata1 = az.load_arviz_data(\"centered_eight\")\n        idata2 = az.load_arviz_data(\"non_centered_eight\")\n        az.plot_elpd(\n            {\"centered model\": idata1, \"non centered model\": idata2},\n            backend=\"bokeh\"\n        )\n\n    \n source"},{"id":149,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_energy","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_energy-Tuple","content":" ArviZPythonPlots.plot_energy  —  Method Plot energy transition distribution and marginal energy distribution in HMC algorithms.\n\n    This may help to diagnose poor exploration by gradient-based algorithms like HMC or NUTS.\n    The energy function in HMC can identify posteriors with heavy tailed distributions, that\n    in practice are challenging for sampling.\n\n    This plot is in the style of the one used in [1]_.\n\n    Parameters\n    ----------\n    data : obj\n        :class:`xarray.Dataset`, or any object that can be converted (must represent\n        ``sample_stats`` and have an ``energy`` variable).\n    kind : str, optional\n        Type of plot to display (\"kde\", \"hist\").\n    bfmi : bool, default True\n        If True add to the plot the value of the estimated Bayesian fraction of missing\n        information.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    legend : bool, default True\n        Flag for plotting legend.\n    fill_alpha : tuple, default (1, 0.75)\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque).\n    fill_color : tuple of valid matplotlib color, default ('C0', 'C5')\n        Color for Marginal energy distribution and Energy transition distribution.\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\". Defaults to \"experimental\".\n        Only works if ``kind='kde'``.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled\n        based on `figsize`.\n    fill_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_kde` (to control the shade).\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_kde` or :func:`matplotlib.pyplot.hist`\n        (if ``type='hist'``).\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    bfmi : Calculate the estimated Bayesian fraction of missing information (BFMI).\n\n    References\n    ----------\n    .. [1] Betancourt (2016). Diagnosing Suboptimal Cotangent Disintegrations in\n    Hamiltonian Monte Carlo https://arxiv.org/abs/1604.00695\n\n    Examples\n    --------\n    Plot a default energy plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_energy(data)\n\n    Represent energy plot via histograms\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_energy(data, kind='hist')\n\n    \n source"},{"id":150,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_ess","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_ess-Tuple","content":" ArviZPythonPlots.plot_ess  —  Method Generate quantile, local, or evolution ESS plots.\n\n    The local and the quantile ESS plots are recommended for checking\n    that there are enough samples for all the explored regions of the\n    parameter space. Checking local and quantile ESS is particularly\n    relevant when working with HDI intervals as opposed to ESS bulk,\n    which is suitable for point estimates.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    kind : {\"local\", \"quantile\", \"evolution\"}, default \"local\"\n        Specify the kind of plot:\n\n        * The ``kind=\"local\"`` argument generates the ESS' local efficiency for\n          estimating quantiles of a desired posterior.\n        * The ``kind=\"quantile\"`` argument generates the ESS' local efficiency\n          for estimating small-interval probability of a desired posterior.\n        * The ``kind=\"evolution\"`` argument generates the estimated ESS'\n          with incrised number of iterations of a desired posterior.\n\n    relative : bool, default False\n        Show relative ess in plot ``ress = ess / N``.\n    coords : dict, optional\n        Coordinates of `var_names` to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n        See :ref:`this section <common_coords>` for usage examples.\n    grid : tuple, optional\n        Number of rows and columns. By default, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If ``None`` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If ``None`` it will be autoscaled\n        based on `figsize`.\n    rug : bool, default False\n        Add a `rug plot <https://en.wikipedia.org/wiki/Rug_plot>`_ for a specific subset of values.\n    rug_kind : str, default \"diverging\"\n        Variable in sample stats to use as rug mask. Must be a boolean variable.\n    n_points : int, default 20\n        Number of points for which to plot their quantile/local ess or number of subsets\n        in the evolution plot.\n    extra_methods : bool, default False\n        Plot mean and sd ESS as horizontal lines. Not taken into account if ``kind = 'evolution'``.\n    min_ess : int, default 400\n        Minimum number of ESS desired. If ``relative=True`` the line is plotted at\n        ``min_ess / n_samples`` for local and quantile kinds and as a curve following\n        the ``min_ess / n`` dependency in evolution kind.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    extra_kwargs : dict, optional\n        If evolution plot, `extra_kwargs` is used to plot ess tail and differentiate it\n        from ess bulk. Otherwise, passed to extra methods lines.\n    text_kwargs : dict, optional\n        Only taken into account when ``extra_methods=True``. kwargs passed to ax.annotate\n        for extra methods lines labels. It accepts the additional\n        key ``x`` to set ``xy=(text_kwargs[\"x\"], mcse)``\n    hline_kwargs : dict, optional\n        kwargs passed to :func:`~matplotlib.axes.Axes.axhline` or to :class:`~bokeh.models.Span`\n        depending on the backend for the horizontal minimum ESS line.\n        For relative ess evolution plots the kwargs are passed to\n        :func:`~matplotlib.axes.Axes.plot` or to :class:`~bokeh.plotting.figure.line`\n    rug_kwargs : dict\n        kwargs passed to rug plot.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :meth:`mpl:matplotlib.axes.Axes.hist` or\n        :meth:`mpl:matplotlib.axes.Axes.plot` function depending on the\n        value of `kind`.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    ess : Calculate estimate of the effective sample size.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2019). Rank-normalization, folding, and\n        localization: An improved Rhat for assessing convergence of\n        MCMC https://arxiv.org/abs/1903.08008\n\n    Examples\n    --------\n    Plot local ESS.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"centered_eight\")\n        >>> coords = {\"school\": [\"Choate\", \"Lawrenceville\"]}\n        >>> az.plot_ess(\n        ...     idata, kind=\"local\", var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    Plot ESS evolution as the number of samples increase. When the model is converging properly,\n    both lines in this plot should be roughly linear.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"evolution\", var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    Customize local ESS plot to look like reference paper.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"local\", var_names=[\"mu\"], drawstyle=\"steps-mid\", color=\"k\",\n        ...     linestyle=\"-\", marker=None, rug=True, rug_kwargs={\"color\": \"r\"}\n        ... )\n\n    Customize ESS evolution plot to look like reference paper.\n\n    .. plot::\n        :context: close-figs\n\n        >>> extra_kwargs = {\"color\": \"lightsteelblue\"}\n        >>> az.plot_ess(\n        ...     idata, kind=\"evolution\", var_names=[\"mu\"],\n        ...     color=\"royalblue\", extra_kwargs=extra_kwargs\n        ... )\n\n    \n source"},{"id":151,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_forest","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_forest-Tuple","content":" ArviZPythonPlots.plot_forest  —  Method Forest plot to compare HDI intervals from a number of distributions.\n\n    Generate forest or ridge plots to compare distributions from a model or list of models.\n    Additionally, the function can display effective sample sizes (ess) and Rhats to visualize\n    convergence diagnostics alongside the distributions.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    kind : {\"foresplot\", \"ridgeplot\"}, default \"forestplot\"\n        Specify the kind of plot:\n\n        * The ``kind=\"forestplot\"`` generates credible intervals, where the central points are the\n          estimated posterior means, the thick lines are the central quartiles, and the thin lines\n          represent the :math:`100\\times`(`hdi_prob`)% highest density intervals.\n        * The ``kind=\"ridgeplot\"`` option generates density plots (kernel density estimate or\n          histograms) in the same graph. Ridge plots can be configured to have different overlap,\n          truncation bounds and quantile markers.\n\n    model_names : list of str, optional\n        List with names for the models in the list of data. Useful when plotting more that one\n        dataset.\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    transform : callable, optional\n        Function to transform data (defaults to None i.e.the identity function).\n    coords : dict, optional\n        Coordinates of ``var_names`` to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n        See :ref:`this section <common_coords>` for usage examples.\n    combined : bool, default False\n        Flag for combining multiple chains into a single chain. If False, chains will\n        be plotted separately. See :ref:`this section <common_combine>` for usage examples.\n    hdi_prob : float, default 0.94\n        Plots highest posterior density interval for chosen percentage of density.\n        See :ref:`this section <common_ hdi_prob>` for usage examples.\n    rope : tuple or dictionary of tuples\n        Lower and upper values of the Region of Practical Equivalence. If a list with one interval\n        only is provided, the ROPE will be displayed across the y-axis. If more than one\n        interval is provided the length of the list should match the number of variables.\n    quartiles : bool, default True\n        Flag for plotting the interquartile range, in addition to the ``hdi_prob`` intervals.\n    r_hat : bool, default False\n        Flag for plotting Split R-hat statistics. Requires 2 or more chains.\n    ess : bool, default False\n        Flag for plotting the effective sample size.\n    colors : list or string, optional\n        list with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically chose a color per model from the matplotlibs\n        cycle. If a single color is passed, eg 'k', 'C2', 'red' this color will be used for all\n        models. Defaults to 'cycle'.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on ``figsize``.\n    linewidth : int, optional\n        Line width throughout. If `None` it will be autoscaled based on ``figsize``.\n    markersize : int, optional\n        Markersize throughout. If `None` it will be autoscaled based on ``figsize``.\n    legend : bool, optional\n        Show a legend with the color encoded model information.\n        Defaults to True, if there are multiple models.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ridgeplot_alpha: float, optional\n        Transparency for ridgeplot fill.  If ``ridgeplot_alpha=0``, border is colored by model,\n        otherwise a `black` outline is used.\n    ridgeplot_overlap : float, default 2\n        Overlap height for ridgeplots.\n    ridgeplot_kind : string, optional\n        By default (\"auto\") continuous variables are plotted using KDEs and discrete ones using\n        histograms. To override this use \"hist\" to plot histograms and \"density\" for KDEs.\n    ridgeplot_truncate : bool, default True\n        Whether to truncate densities according to the value of ``hdi_prob``.\n    ridgeplot_quantiles : list, optional\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n    figsize : (float, float), optional\n        Figure size. If `None`, it will be defined automatically.\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_config : dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in ``rcParams``.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    1D ndarray of matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    summary : Create a data frame with summary statistics.\n\n    Examples\n    --------\n    Forestplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered_data = az.load_arviz_data('non_centered_eight')\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='forestplot',\n        >>>                            var_names=[\"^the\"],\n        >>>                            filter_vars=\"regex\",\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Forestplot with multiple datasets\n\n    .. plot::\n        :context: close-figs\n\n        >>> centered_data = az.load_arviz_data('centered_eight')\n        >>> axes = az.plot_forest([non_centered_data, centered_data],\n        >>>                            model_names = [\"non centered eight\", \"centered eight\"],\n        >>>                            kind='forestplot',\n        >>>                            var_names=[\"^the\"],\n        >>>                            filter_vars=\"regex\",\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools models')\n\n    Forestplot with ropes\n\n    .. plot::\n        :context: close-figs\n\n        >>> rope = {'theta': [{'school': 'Choate', 'rope': (2, 4)}], 'mu': [{'rope': (-2, 2)}]}\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            rope=rope,\n        >>>                            var_names='~tau',\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n\n    Ridgeplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_overlap=3,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Ridgeplot non-truncated and with quantiles\n\n    .. plot::\n        :context: close-figs\n\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_truncate=False,\n        >>>                            ridgeplot_quantiles=[.25, .5, .75],\n        >>>                            ridgeplot_overlap=0.7,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n    \n source"},{"id":152,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_hdi","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_hdi-Tuple","content":" ArviZPythonPlots.plot_hdi  —  Method \n    Plot HDI intervals for regression data.\n\n    Parameters\n    ----------\n    x : array-like\n        Values to plot.\n    y : array-like, optional\n        Values from which to compute the HDI. Assumed shape ``(chain, draw, \\*shape)``.\n        Only optional if ``hdi_data`` is present.\n    hdi_data : array_like, optional\n        Precomputed HDI values to use. Assumed shape is ``(*x.shape, 2)``.\n    hdi_prob : float, optional\n        Probability for the highest density interval. Defaults to ``stats.hdi_prob`` rcParam.\n    color : str, optional\n        Color used for the limits of the HDI and fill. Should be a valid matplotlib color.\n    circular : bool, optional\n        Whether to compute the HDI taking into account ``x`` is a circular variable\n        (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n    smooth : boolean, optional\n        If True the result will be smoothed by first computing a linear interpolation of the data\n        over a regular grid and then applying the Savitzky-Golay filter to the interpolated data.\n        Defaults to True.\n    smooth_kwargs : dict, optional\n        Additional keywords modifying the Savitzky-Golay filter. See\n        :func:`scipy:scipy.signal.savgol_filter` for details.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    fill_kwargs : dict, optional\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.fill_between`\n        (use ``fill_kwargs={'alpha': 0}`` to disable fill) or to\n        :meth:`bokeh.plotting.Figure.patch`.\n    plot_kwargs : dict, optional\n        HDI limits keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh.plotting.Figure.patch`.\n    hdi_kwargs : dict, optional\n        Keyword arguments passed to :func:`~arviz.hdi`. Ignored if ``hdi_data`` is present.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    backend : {\"matplotlib\",\"bokeh\"}, optional\n        Select plotting backend.\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh.plotting.Figure.patch`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    hdi : Calculate highest density interval (HDI) of array for given probability.\n\n    Examples\n    --------\n    Plot HDI interval of simulated regression data using `y` argument:\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> x_data = np.random.normal(0, 1, 100)\n        >>> y_data = np.random.normal(2 + x_data * 0.5, 0.5, size=(2, 50, 100))\n        >>> az.plot_hdi(x_data, y_data)\n\n    ``plot_hdi`` can also be given precalculated values with the argument ``hdi_data``. This example\n    shows how to use :func:`~arviz.hdi` to precalculate the values and pass these values to\n    ``plot_hdi``. Similarly to an example in ``hdi`` we are using the ``input_core_dims``\n    argument of :func:`~arviz.wrap_xarray_ufunc` to manually define the dimensions over which\n    to calculate the HDI.\n\n    .. plot::\n        :context: close-figs\n\n        >>> hdi_data = az.hdi(y_data, input_core_dims=[[\"draw\"]])\n        >>> ax = az.plot_hdi(x_data, hdi_data=hdi_data[0], color=\"r\", fill_kwargs={\"alpha\": .2})\n        >>> az.plot_hdi(x_data, hdi_data=hdi_data[1], color=\"k\", ax=ax, fill_kwargs={\"alpha\": .2})\n\n    ``plot_hdi`` can also be used with Inference Data objects. Here we use the posterior predictive\n    to plot the HDI interval.\n\n    .. plot::\n        :context: close-figs\n\n        >>> X = np.random.normal(0,1,100)\n        >>> Y = np.random.normal(2 + X * 0.5, 0.5, size=(2,10,100))\n        >>> idata = az.from_dict(posterior={\"y\": Y}, constant_data={\"x\":X})\n        >>> x_data = idata.constant_data.x\n        >>> y_data = idata.posterior.y\n        >>> az.plot_hdi(x_data, y_data)\n\n    \n source"},{"id":153,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_kde","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_kde-Tuple","content":" ArviZPythonPlots.plot_kde  —  Method 1D or 2D KDE plot taking into account boundary conditions.\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE will be estimated\n    cumulative : bool\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE\n    rug : bool\n        If True adds a rugplot. Defaults to False. Ignored for 2D KDE\n    label : string\n        Text to include as part of the legend\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``is_circular`` is False\n        and \"taylor\" (for now) when ``is_circular`` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    adaptive : bool, optional.\n        If True, an adaptative bandwidth is used. Only valid for 1D KDE.\n        Defaults to False.\n    quantiles : list\n        Quantiles in ascending order used to segment the KDE.\n        Use [.25, .5, .75] for quartiles. Defaults to None.\n    rotated : bool\n        Whether to rotate the 1D KDE plot 90 degrees.\n    contour : bool\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE.\n        Defaults to True.\n    hdi_probs : list\n        Plots highest density credibility regions for the provided probabilities for a 2D KDE.\n        Defaults to matplotlib chosen levels with no fixed probability associated.\n    fill_last : bool\n        If True fill the last contour of the 2D KDE plot. Defaults to False.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``. Not implemented for bokeh backend.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE. See :meth:`mpl:matplotlib.axes.Axes.plot`\n        or :meth:`bokeh:bokeh.plotting.Figure.line` for a description of accepted values.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use ``fill_kwargs={'alpha': 0}``\n        to disable fill). Ignored for 2D KDE. Passed to\n        :meth:`bokeh.plotting.Figure.patch`.\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if ``rug=False`` or for 2D KDE\n        Use ``space`` keyword (float) to control the position of the rugplot. The larger this number\n        the lower the rugplot. Passed to :class:`bokeh:bokeh.models.glyphs.Scatter`.\n    contour_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.contour`\n        to draw contour lines or :meth:`bokeh.plotting.Figure.patch`.\n        Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.contourf`\n        to draw filled contours. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.pcolormesh` or\n        :meth:`bokeh.plotting.Figure.image`.\n        Ignored for 1D KDE.\n    is_circular : {False, True, \"radians\", \"degrees\"}. Default False.\n        Select input type {\"radians\", \"degrees\"} for circular histogram or KDE plot. If True,\n        default input type is \"radians\". When this argument is present, it interprets ``values``\n        is a circular variable measured in radians and a circular KDE is used. Inputs in\n        \"degrees\" will undergo an internal conversion to radians.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    legend : bool\n        Add legend to the figure. By default True.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n    return_glyph : bool, optional\n        Internal argument to return glyphs for bokeh\n\n    Returns\n    -------\n    axes : matplotlib.Axes or bokeh.plotting.Figure\n        Object containing the kde plot\n    glyphs : list, optional\n        Bokeh glyphs present in plot.  Only provided if ``return_glyph`` is True.\n\n    See Also\n    --------\n    kde : One dimensional density estimation.\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n\n    Examples\n    --------\n    Plot default KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> mu_posterior = np.concatenate(non_centered.posterior[\"mu\"].values)\n        >>> tau_posterior = np.concatenate(non_centered.posterior[\"tau\"].values)\n        >>> az.plot_kde(mu_posterior)\n\n\n    Plot KDE with rugplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rug=True)\n\n    Plot KDE with adaptive bandwidth\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, adaptive=True)\n\n    Plot KDE with a different bandwidth estimator\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, bw=\"scott\")\n\n    Plot KDE with a bandwidth specified manually\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, bw=0.4)\n\n    Plot KDE for a circular variable\n\n    .. plot::\n        :context: close-figs\n\n        >>> rvs = np.random.vonmises(mu=np.pi, kappa=2, size=500)\n        >>> az.plot_kde(rvs, is_circular=True)\n\n\n    Plot a cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, cumulative=True)\n\n\n\n    Rotate plot 90 degrees\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rotated=True)\n\n\n    Plot 2d contour KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior)\n\n\n    Plot 2d contour KDE, without filling and contour lines using viridis cmap\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior,\n        ...             contour_kwargs={\"colors\":None, \"cmap\":plt.cm.viridis},\n        ...             contourf_kwargs={\"alpha\":0});\n\n    Plot 2d contour KDE, set the number of levels to 3.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(\n        ...     mu_posterior, values2=tau_posterior,\n        ...     contour_kwargs={\"levels\":3}, contourf_kwargs={\"levels\":3}\n        ... );\n\n    Plot 2d contour KDE with 30%, 60% and 90% HDI contours.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, hdi_probs=[0.3, 0.6, 0.9])\n\n    Plot 2d smooth KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, contour=False)\n\n    \n source"},{"id":154,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_khat","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_khat-Tuple","content":" ArviZPythonPlots.plot_khat  —  Method \n    Plot Pareto tail indices for diagnosing convergence.\n\n    Parameters\n    ----------\n    khats : ELPDData containing Pareto shapes information or array of\n        Pareto tail indices.\n    color : str or array_like, optional\n        Colors of the scatter plot, if color is a str all dots will\n        have the same color, if it is the size of the observations,\n        each dot will have the specified color, otherwise, it will be\n        interpreted as a list of the dims to be used for the color\n        code. If Matplotlib c argument is passed, it will override\n        the color argument\n    xlabels : bool, optional\n        Use coords as xticklabels\n    show_hlines : bool, optional\n        Show the horizontal lines, by default at the values [0, 0.5, 0.7, 1].\n    show_bins : bool, optional\n        Show the percentage of khats falling in each bin, as delimited by hlines.\n    bin_format : str, optional\n        The string is used as formatting guide calling ``bin_format.format(count, pct)``.\n    threshold : float, optional\n        Show the labels of k values larger than threshold. Defaults to `None`,\n        no observations will be highlighted.\n    hover_label : bool, optional\n        Show the datapoint label when hovering over it with the mouse. Requires an interactive\n        backend.\n    hover_format : str, optional\n        String used to format the hover label via ``hover_format.format(idx, coord_label)``\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    coords : mapping, optional\n        Coordinates of points to plot. **All** values are used for computation, but only a\n        a subset can be plotted for convenience.\n    legend : bool, optional\n        Include a legend to the plot. Only taken into account when color argument is a dim name.\n    markersize : int, optional\n        markersize for scatter plot. Defaults to `None` in which case it will\n        be chosen based on autoscaling for figsize.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    hlines_kwargs : dictionary, optional\n        Additional keywords passed to\n        :meth:`matplotlib.axes.Axes.hlines`.\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n    kwargs :\n        Additional keywords passed to\n        :meth:`matplotlib.axes.Axes.scatter`.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    psislw : Pareto smoothed importance sampling (PSIS).\n\n    Examples\n    --------\n    Plot estimated pareto shape parameters showing how many fall in each category.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> radon = az.load_arviz_data(\"radon\")\n        >>> loo_radon = az.loo(radon, pointwise=True)\n        >>> az.plot_khat(loo_radon, show_bins=True)\n\n    Show xlabels\n\n    .. plot::\n        :context: close-figs\n\n        >>> centered_eight = az.load_arviz_data(\"centered_eight\")\n        >>> khats = az.loo(centered_eight, pointwise=True).pareto_k\n        >>> az.plot_khat(khats, xlabels=True, threshold=1)\n\n    Use custom color scheme\n\n    .. plot::\n        :context: close-figs\n\n        >>> counties = radon.posterior.County[radon.constant_data.county_idx].values\n        >>> colors = [\n        ...     \"blue\" if county[-1] in (\"A\", \"N\") else \"green\" for county in counties\n        ... ]\n        >>> az.plot_khat(loo_radon, color=colors)\n\n    Notes\n    -----\n    The Generalized Pareto distribution (GPD) may be used to diagnose\n    convergence rates for importance sampling.  GPD has parameters\n    offset, scale, and shape. The shape parameter is usually denoted\n    with ``k``. ``k`` also tells how many finite moments the\n    distribution has. The pre-asymptotic convergence rate of\n    importance sampling can be estimated based on the fractional\n    number of finite moments of the importance ratio distribution. GPD\n    is fitted to the largest importance ratios and the estimated shape\n    parameter ``k``, i.e., ``\\hat{k}`` can then be used as a diagnostic\n    (most importantly if ``\\hat{k} > 0.7``, then the convergence rate\n    is impractically low). See [1]_.\n\n    References\n    ----------\n    .. [1] Vehtari, A., Simpson, D., Gelman, A., Yao, Y., Gabry, J.,\n        2019. Pareto Smoothed Importance Sampling. arXiv:1507.02646 [stat].\n\n    \n source"},{"id":155,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_lm","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_lm-Tuple","content":" ArviZPythonPlots.plot_lm  —  Method Posterior predictive and mean plots for regression-like data.\n\n    Parameters\n    ----------\n    y : str or DataArray or ndarray\n        If str, variable name from ``observed_data``.\n    idata : InferenceData, Optional\n        Optional only if ``y`` is not str.\n    x : str, tuple of strings, DataArray or array-like, optional\n        If str or tuple, variable name from ``constant_data``.\n        If ndarray, could be 1D, or 2D for multiple plots.\n        If None, coords name of ``y`` (``y`` should be DataArray).\n    y_model : str or Sequence, Optional\n        If str, variable name from ``posterior``.\n        Its dimensions should be same as ``y`` plus added chains and draws.\n    y_hat : str, Optional\n        If str, variable name from ``posterior_predictive``.\n        Its dimensions should be same as ``y`` plus added chains and draws.\n    num_samples : int, Optional, Default 50\n        Significant if ``kind_pp`` is \"samples\" or ``kind_model`` is \"lines\".\n        Number of samples to be drawn from posterior predictive or\n    kind_pp : {\"samples\", \"hdi\"}, Default \"samples\"\n        Options to visualize uncertainty in data.\n    kind_model : {\"lines\", \"hdi\"}, Default \"lines\"\n        Options to visualize uncertainty in mean of the data.\n    plot_dim : str, Optional\n        Necessary if ``y`` is multidimensional.\n    backend : str, Optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    y_kwargs : dict, optional\n        Passed to :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib\n        and :meth:`bokeh:bokeh.plotting.Figure.circle` in bokeh\n    y_hat_plot_kwargs : dict, optional\n        Passed to :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib\n        and :meth:`bokeh:bokeh.plotting.Figure.circle` in bokeh\n    y_hat_fill_kwargs : dict, optional\n        Passed to :func:`arviz.plot_hdi`\n    y_model_plot_kwargs : dict, optional\n        Passed to :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib\n        and :meth:`bokeh:bokeh.plotting.Figure.line` in bokeh\n    y_model_fill_kwargs : dict, optional\n        Significant if ``kind_model`` is \"hdi\". Passed to :func:`arviz.plot_hdi`\n    y_model_mean_kwargs : dict, optional\n        Passed to :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib\n        and :meth:`bokeh:bokeh.plotting.Figure.line` in bokeh\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used. Passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be\n        autoscaled based on ``figsize``.\n    axes : 2D numpy array-like of matplotlib_axes or bokeh_figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    show : bool, optional\n        Call backend show function.\n    legend : bool, optional\n        Add legend to figure. By default True.\n    grid : bool, optional\n        Add grid to figure. By default True.\n\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_ts : Plot timeseries data\n    plot_ppc : Plot for posterior/prior predictive checks\n\n    Examples\n    --------\n    Plot regression default plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> import numpy as np\n        >>> import xarray as xr\n        >>> idata = az.load_arviz_data('regression1d')\n        >>> x = xr.DataArray(np.linspace(0, 1, 100))\n        >>> idata.posterior[\"y_model\"] = idata.posterior[\"intercept\"] + idata.posterior[\"slope\"]*x\n        >>> az.plot_lm(idata=idata, y=\"y\", x=x)\n\n    Plot regression data and mean uncertainty\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_lm(idata=idata, y=\"y\", x=x, y_model=\"y_model\")\n\n    Plot regression data and mean uncertainty in hdi form\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_lm(\n        ...     idata=idata, y=\"y\", x=x, y_model=\"y_model\", kind_pp=\"hdi\", kind_model=\"hdi\"\n        ... )\n\n    Plot regression data for multi-dimensional y using plot_dim\n\n    .. plot::\n        :context: close-figs\n\n        >>> data = az.from_dict(\n        ...     observed_data = { \"y\": np.random.normal(size=(5, 7)) },\n        ...     posterior_predictive = {\"y\": np.random.randn(4, 1000, 5, 7) / 2},\n        ...     dims={\"y\": [\"dim1\", \"dim2\"]},\n        ...     coords={\"dim1\": range(5), \"dim2\": range(7)}\n        ... )\n        >>> az.plot_lm(idata=data, y=\"y\", plot_dim=\"dim1\")\n    \n source"},{"id":156,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_loo_pit","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_loo_pit-Tuple","content":" ArviZPythonPlots.plot_loo_pit  —  Method Plot Leave-One-Out (LOO) probability integral transformation (PIT) predictive checks.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        :class:`arviz.InferenceData` object.\n    y : array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat : array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as y plus an\n        extra dimension at the end of size n_samples (chains and draws stacked). If str or\n        None, ``idata`` must contain the posterior predictive group. If None, ``y_hat`` is taken\n        equal to y, thus, y must be str too.\n    log_weights : array or DataArray\n        Smoothed log_weights. It must have the same shape as ``y_hat``\n    ecdf : bool, optional\n        Plot the difference between the LOO-PIT Empirical Cumulative Distribution Function\n        (ECDF) and the uniform CDF instead of LOO-PIT kde.\n        In this case, instead of overlaying uniform distributions, the beta ``hdi_prob``\n        around the theoretical uniform CDF is shown. This approximation only holds\n        for large S and ECDF values not very close to 0 nor 1. For more information, see\n        `Vehtari et al. (2019)`, `Appendix G <https://avehtari.github.io/rhat_ess/rhat_ess.html>`_.\n    ecdf_fill : bool, optional\n        Use :meth:`matplotlib.axes.Axes.fill_between` to mark the area\n        inside the credible interval. Otherwise, plot the\n        border lines.\n    n_unif : int, optional\n        Number of datasets to simulate and overlay from the uniform distribution.\n    use_hdi : bool, optional\n        Compute expected hdi values instead of overlaying the sampled uniform distributions.\n    hdi_prob : float, optional\n        Probability for the highest density interval. Works with ``use_hdi=True`` or ``ecdf=True``.\n    figsize : (float, float), optional\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize : int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    color : str or array_like, optional\n        Color of the LOO-PIT estimated pdf plot. If ``plot_unif_kwargs`` has no \"color\" key,\n        a slightly lighter color than this argument will be used for the uniform kde lines.\n        This will ensure that LOO-PIT kde and uniform kde have different default colors.\n    legend : bool, optional\n        Show the legend of the figure.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    plot_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot`\n        for LOO-PIT line (kde or ECDF)\n    plot_unif_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot` for\n        overlaid uniform distributions or for beta credible interval\n        lines if ``ecdf=True``\n    hdi_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.axhspan`\n    fill_kwargs : dict, optional\n        Additional kwargs passed to :meth:`matplotlib.axes.Axes.fill_between`\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    plot_bpv : Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n    loo_pit : Compute leave one out (PSIS-LOO) probability integral transform (PIT) values.\n\n    References\n    ----------\n    * Gabry et al. (2017) see https://arxiv.org/abs/1709.01449\n    * https://mc-stan.org/bayesplot/reference/PPC-loo.html\n    * Gelman et al. BDA (2014) Section 6.3\n\n    Examples\n    --------\n    Plot LOO-PIT predictive checks overlaying the KDE of the LOO-PIT values to several\n    realizations of uniform variable sampling with the same number of observations.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"radon\")\n        >>> az.plot_loo_pit(idata=idata, y=\"y\")\n\n    Fill the area containing the 94% highest density interval of the difference between uniform\n    variables empirical CDF and the real uniform CDF. A LOO-PIT ECDF clearly outside of these\n    theoretical boundaries indicates that the observations and the posterior predictive\n    samples do not follow the same distribution.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_loo_pit(idata=idata, y=\"y\", ecdf=True)\n\n    \n source"},{"id":157,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_mcse","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_mcse-Tuple","content":" ArviZPythonPlots.plot_mcse  —  Method Plot quantile or local Monte Carlo Standard Error.\n\n    Parameters\n    ----------\n    idata : obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names : list of variable names, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars : {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        `pandas.filter`.\n    coords : dict, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    errorbar : bool, optional\n        Plot quantile value +/- mcse instead of plotting mcse.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    extra_methods : bool, optional\n        Plot mean and sd MCSE as horizontal lines. Only taken into account when\n        ``errorbar=False``.\n    rug : bool\n        Plot rug plot of values diverging or that reached the max tree depth.\n    rug_kind : bool\n        Variable in sample stats to use as rug mask. Must be a boolean variable.\n    n_points : int\n        Number of points for which to plot their quantile/local ess or number of subsets\n        in the evolution plot.\n    labeller : Labeller, optional\n        Class providing the method `make_label_vert` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    rug_kwargs : dict\n        kwargs passed to rug plot in\n        :meth:`mpl:matplotlib.axes.Axes.plot` or :class:`bokeh:bokeh.models.glyphs.Scatter`.\n    extra_kwargs : dict, optional\n        kwargs passed as extra method lines in\n        :meth:`mpl:matplotlib.axes.Axes.axhline` or :class:`bokeh:bokeh.models.Span`\n    text_kwargs : dict, optional\n        kwargs passed to :meth:`mpl:matplotlib.axes.Axes.annotate` for extra methods lines labels.\n        It accepts the additional key ``x`` to set ``xy=(text_kwargs[\"x\"], mcse)``.\n        text_kwargs are ignored for the bokeh plotting backend.\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :meth:`mpl:matplotlib.axes.Axes.hist` or\n        :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib depending on the value of `kind`.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    :func:`arviz.mcse`: Calculate Markov Chain Standard Error statistic.\n\n    References\n    ----------\n    * Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n\n    Examples\n    --------\n    Plot quantile Monte Carlo Standard Error.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"centered_eight\")\n        >>> coords = {\"school\": [\"Deerfield\", \"Lawrenceville\"]}\n        >>> az.plot_mcse(\n        ...     idata, var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    \n source"},{"id":158,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_pair","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_pair-Tuple","content":" ArviZPythonPlots.plot_pair  —  Method \n    Plot a scatter, kde and/or hexbin matrix with (optional) marginals on the diagonal.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    group: str, optional\n        Specifies which InferenceData group should be plotted.  Defaults to 'posterior'.\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n    marginals: bool, optional\n        If True pairplot will include marginal distributions for every variable\n    figsize: figure size tuple\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    kind : str or List[str]\n        Type of plot to display (scatter, kde and/or hexbin)\n    gridsize: int or (int, int), optional\n        Only works for ``kind=hexbin``. The number of hexagons in the x-direction.\n        The corresponding number of hexagons in the y-direction is chosen\n        such that the hexagons are approximately regular. Alternatively, gridsize\n        can be a tuple with two elements specifying the number of hexagons\n        in the x-direction and the y-direction.\n    divergences: Boolean\n        If True divergences will be plotted in a different color, only if group is either 'prior'\n        or 'posterior'.\n    colorbar: bool\n        If True a colorbar will be included as part of the plot (Defaults to False).\n        Only works when ``kind=hexbin``\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    divergences_kwargs: dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter` for divergences\n    scatter_kwargs:\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter` when using scatter kind\n    kde_kwargs: dict, optional\n        Additional keywords passed to :func:`arviz.plot_kde` when using kde kind\n    hexbin_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.hexbin` when\n        using hexbin kind\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    marginal_kwargs: dict, optional\n        Additional keywords passed to :func:`arviz.plot_dist`, modifying the\n        marginal distributions plotted in the diagonal.\n    point_estimate: str, optional\n        Select point estimate from 'mean', 'mode' or 'median'. The point estimate will be\n        plotted using a scatter marker and vertical/horizontal lines.\n    point_estimate_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.axvline`,\n        :meth:`matplotlib.axes.Axes.axhline` (matplotlib) or\n        :class:`bokeh:bokeh.models.Span` (bokeh)\n    point_estimate_marker_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter`\n        or :meth:`bokeh:bokeh.plotting.Figure.square` in point\n        estimate plot. Not available in bokeh\n    reference_values: dict, optional\n        Reference values for the plotted variables. The Reference values will be plotted\n        using a scatter marker\n    reference_values_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.circle` in reference values plot\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    Examples\n    --------\n    KDE Pair Plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> coords = {'school': ['Choate', 'Deerfield']}\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu', 'tau'],\n        >>>             kind='kde',\n        >>>             coords=coords,\n        >>>             divergences=True,\n        >>>             textsize=18)\n\n    Hexbin pair plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu'],\n        >>>             coords=coords,\n        >>>             textsize=18,\n        >>>             kind='hexbin')\n\n    Pair plot showing divergences and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        ...             var_names=['^t', 'mu'],\n        ...             filter_vars=\"regex\",\n        ...             coords=coords,\n        ...             divergences=True,\n        ...             textsize=18)\n    \n source"},{"id":159,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_parallel","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_parallel-Tuple","content":" ArviZPythonPlots.plot_parallel  —  Method \n    Plot parallel coordinates plot showing posterior points with and without divergences.\n\n    Described by https://arxiv.org/abs/1709.01449\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names\n        Variables to be plotted, if `None` all variables are plotted. Can be used to change the\n        order of the plotted variables. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: mapping, optional\n        Coordinates of ``var_names`` to be plotted.\n        Passed to :meth:`xarray.Dataset.sel`.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    legend: bool\n        Flag for plotting legend (defaults to True)\n    colornd: valid matplotlib color\n        color for non-divergent points. Defaults to 'k'\n    colord: valid matplotlib color\n        color for divergent points. Defaults to 'C1'\n    shadend: float\n        Alpha blending value for non-divergent points, between 0 (invisible) and 1 (opaque).\n        Defaults to .025\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    norm_method: str\n        Method for normalizing the data. Methods include normal, minmax and rank.\n        Defaults to none.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes.\n        Defaults to value set in ``rcParams``.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_pair : Plot a scatter, kde and/or hexbin matrix with (optional) marginals on the diagonal.\n    plot_trace : Plot distribution (histogram or kernel density estimates) and sampled values\n                 or rank plot\n\n    Examples\n    --------\n    Plot default parallel plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_parallel(data, var_names=[\"mu\", \"tau\"])\n\n\n    Plot parallel plot with normalization\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"normal\")\n\n    Plot parallel plot with minmax\n\n    .. plot::\n        :context: close-figs\n\n        >>> ax = az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"minmax\")\n        >>> ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n\n    Plot parallel plot with rank\n\n    .. plot::\n        :context: close-figs\n\n        >>> ax = az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"rank\")\n        >>> ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n    \n source"},{"id":160,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_posterior","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_posterior-Tuple","content":" ArviZPythonPlots.plot_posterior  —  Method Plot Posterior densities in the style of John K. Kruschke's book.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to the documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names\n        Variables to be plotted, two variables are required. Prefix the variables with ``~``\n        when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    hdi_prob: float, optional\n        Plots highest density interval for chosen percentage of density.\n        Use 'hide' to hide the highest density interval. Defaults to 0.94.\n    multimodal: bool\n        If true (default) it may compute more than one credible interval if the distribution is\n        multimodal and the modes are well separated.\n    skipna : bool\n        If true ignores nan values when computing the hdi and point estimates. Defaults to false.\n    round_to: int, optional\n        Controls formatting of floats. Defaults to 2 or the integer part, whichever is bigger.\n    point_estimate: Optional[str]\n        Plot point estimate per variable. Values should be 'mean', 'median', 'mode' or None.\n        Defaults to 'auto' i.e. it falls back to default set in rcParams.\n    group: str, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n    rope: tuple or dictionary of tuples\n        Lower and upper values of the Region Of Practical Equivalence. If a list is provided, its\n        length should match the number of variables.\n    ref_val: float or dictionary of floats\n        display the percentage below and above the values in ref_val. Must be None (default),\n        a constant, a list or a dictionary like see an example below. If a list is provided, its\n        length should match the number of variables.\n    rope_color: str, optional\n        Specifies the color of ROPE and displayed percentage within ROPE\n    ref_val_color: str, optional\n        Specifies the color of the displayed percentage\n    kind: str\n        Type of plot to display (kde or hist) For discrete variables this argument is ignored and\n        a histogram is always used. Defaults to rcParam ``plot.density_kind``\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when `circular` is False\n        and \"taylor\" (for now) when `circular` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is. Only works if `kind == kde`.\n    circular: bool, optional\n        If True, it interprets the values passed are from a circular variable measured in radians\n        and a circular KDE is used. Only valid for 1D KDE. Defaults to False.\n        Only works if `kind == kde`.\n    bins: integer or sequence or 'auto', optional\n        Controls the number of bins,accepts the same keywords :func:`matplotlib.pyplot.hist` does.\n        Only works if `kind == hist`. If None (default) it will use `auto` for continuous variables\n        and `range(xmin, xmax + 1)` for discrete variables.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :func:`matplotlib.pyplot.hist` or :func:`matplotlib.pyplot.plot` function\n        depending on the value of `kind`.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    plot_forest : Forest plot to compare HDI intervals from a number of distributions.\n\n    Examples\n    --------\n    Show a default kernel density plot following style of John Kruschke\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_posterior(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'])\n\n    Plot Region of Practical Equivalence (rope) and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', '^the'], filter_vars=\"regex\", rope=(-1, 1))\n\n    Plot Region of Practical Equivalence for selected distributions\n\n    .. plot::\n        :context: close-figs\n\n        >>> rope = {'mu': [{'rope': (-2, 2)}], 'theta': [{'school': 'Choate', 'rope': (2, 4)}]}\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], rope=rope)\n\n    Using `coords` argument to plot only a subset of data\n\n    .. plot::\n        :context: close-figs\n\n        >>> coords = {\"school\": [\"Choate\",\"Phillips Exeter\"]}\n        >>> az.plot_posterior(data, var_names=[\"mu\", \"theta\"], coords=coords)\n\n    Add reference lines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], ref_val=0)\n\n    Show point estimate of distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], point_estimate='mode')\n\n    Show reference values using variable names and coordinates\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, ref_val= {\"theta\": [{\"school\": \"Deerfield\", \"ref_val\": 4},\n        ...                                             {\"school\": \"Choate\", \"ref_val\": 3}]})\n\n    Show reference values using a list\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, ref_val=[1] + [5] * 8 + [1])\n\n\n    Plot posterior as a histogram\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'], kind='hist')\n\n    Change size of highest density interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'], hdi_prob=.75)\n    \n source"},{"id":161,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_ppc","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_ppc-Tuple","content":" ArviZPythonPlots.plot_ppc  —  Method \n    Plot for posterior/prior predictive checks.\n\n    Parameters\n    ----------\n    data: az.InferenceData object\n        :class:`arviz.InferenceData` object containing the observed and posterior/prior\n        predictive data.\n    kind: str\n        Type of plot to display (\"kde\", \"cumulative\", or \"scatter\"). Defaults to `kde`.\n    alpha: float\n        Opacity of posterior/prior predictive density curves.\n        Defaults to 0.2 for ``kind = kde`` and cumulative, for scatter defaults to 0.7.\n    mean: bool\n        Whether or not to plot the mean posterior/prior predictive distribution.\n        Defaults to ``True``.\n    observed: bool, default True\n        Whether or not to plot the observed data.\n    observed: bool, default False\n        Whether or not to plot a rug plot for the observed data. Only valid if `observed` is\n        `True` and for kind `kde` or `cumulative`.\n    color: str\n        Valid matplotlib ``color``. Defaults to ``C0``.\n    color: list\n        List with valid matplotlib colors corresponding to the posterior/prior predictive\n        distribution, observed data and mean of the posterior/prior predictive distribution.\n        Defaults to [\"C0\", \"k\", \"C1\"].\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None, it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None, it will be\n        autoscaled based on ``figsize``.\n    data_pairs: dict\n        Dictionary containing relations between observed data and posterior/prior predictive data.\n        Dictionary structure:\n\n        - key = data var_name\n        - value = posterior/prior predictive var_name\n\n        For example, ``data_pairs = {'y' : 'y_hat'}``\n        If None, it will assume that the observed data and the posterior/prior\n        predictive data have the same variable name.\n    var_names: list of variable names\n        Variables to be plotted, if `None` all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None.\n    flatten: list\n        List of dimensions to flatten in ``observed_data``. Only flattens across the coordinates\n        specified in the ``coords`` argument. Defaults to flattening all of the dimensions.\n    flatten_pp: list\n        List of dimensions to flatten in posterior_predictive/prior_predictive. Only flattens\n        across the coordinates specified in the ``coords`` argument. Defaults to flattening all\n        of the dimensions. Dimensions should match flatten excluding dimensions for ``data_pairs``\n        parameters. If ``flatten`` is defined and ``flatten_pp`` is None, then\n        ``flatten_pp = flatten``.\n    num_pp_samples: int\n        The number of posterior/prior predictive samples to plot. For ``kind`` = 'scatter' and\n        ``animation = False`` if defaults to a maximum of 5 samples and will set jitter to 0.7.\n        unless defined. Otherwise it defaults to all provided samples.\n    random_seed: int\n        Random number generator seed passed to ``numpy.random.seed`` to allow\n        reproducibility of the plot. By default, no seed will be provided\n        and the plot will change each call if a random sample is specified\n        by ``num_pp_samples``.\n    jitter: float\n        If ``kind`` is \"scatter\", jitter will add random uniform noise to the height\n        of the ppc samples and observed data. By default 0.\n    animated: bool\n        Create an animation of one posterior/prior predictive sample per frame.\n        Defaults to ``False``. Only works with matploblib backend.\n        To run animations inside a notebook you have to use the `nbAgg` matplotlib's backend.\n        Try with `%matplotlib notebook` or  `%matplotlib  nbAgg`. You can switch back to the\n        default matplotlib's backend with `%matplotlib  inline` or `%matplotlib  auto`.\n        If switching back and forth between matplotlib's backend, you may need to run twice the cell\n        with the animation.\n        If you experience problems rendering the animation try setting\n        `animation_kwargs({'blit':False}`) or changing the matplotlib's backend (e.g. to TkAgg)\n        If you run the animation from a script write `ax, ani = az.plot_ppc(.)`\n    animation_kwargs : dict\n        Keywords passed to  :class:`matplotlib.animation.FuncAnimation`. Ignored with\n        matplotlib backend.\n    legend : bool\n        Add legend to figure. By default ``True``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    group: {\"prior\", \"posterior\"}, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n        Other value can be 'prior'.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_bpv: Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n    plot_lm: Posterior predictive and mean plots for regression-like data.\n    plot_ppc: plot for posterior/prior predictive checks.\n    plot_ts: Plot timeseries data.\n\n    Examples\n    --------\n    Plot the observed data KDE overlaid on posterior predictive KDEs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('radon')\n        >>> az.plot_ppc(data, data_pairs={\"y\":\"y\"})\n\n    Plot the overlay with empirical CDFs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='cumulative')\n\n    Use the ``coords`` and ``flatten`` parameters to plot selected variable dimensions\n    across multiple plots. We will now modify the dimension ``obs_id`` to contain\n    indicate the name of the county where the measure was taken. The change has to\n    be done on both ``posterior_predictive`` and ``observed_data`` groups, which is\n    why we will use :meth:`~arviz.InferenceData.map` to apply the same function to\n    both groups. Afterwards, we will select the counties to be plotted with the\n    ``coords`` arg.\n\n    .. plot::\n        :context: close-figs\n\n        >>> obs_county = data.posterior[\"County\"][data.constant_data[\"county_idx\"]]\n        >>> data = data.assign_coords(obs_id=obs_county, groups=\"observed_vars\")\n        >>> az.plot_ppc(data, coords={'obs_id': ['ANOKA', 'BELTRAMI']}, flatten=[])\n\n    Plot the overlay using a stacked scatter plot that is particularly useful\n    when the sample sizes are small.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='scatter', flatten=[],\n        >>>             coords={'obs_id': ['AITKIN', 'BELTRAMI']})\n\n    Plot random posterior predictive sub-samples.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, num_pp_samples=30, random_seed=7)\n    \n source"},{"id":162,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_rank","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_rank-Tuple","content":" ArviZPythonPlots.plot_rank  —  Method Plot rank order statistics of chains.\n\n    From the paper: Rank plots are histograms of the ranked posterior draws (ranked over all\n    chains) plotted separately for each chain.\n    If all of the chains are targeting the same posterior, we expect the ranks in each chain to be\n    uniform, whereas if one chain has a different location or scale parameter, this will be\n    reflected in the deviation from uniformity. If rank plots of all chains look similar, this\n    indicates good mixing of the chains.\n\n    This plot was introduced by Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter,\n    Paul-Christian Burkner (2019): Rank-normalization, folding, and localization: An improved R-hat\n    for assessing convergence of MCMC. arXiv preprint https://arxiv.org/abs/1903.08008\n\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of  :func:`arviz.convert_to_dataset` for details\n    var_names: string or list of variable names\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    bins: None or passed to np.histogram\n        Binning strategy used for histogram. By default uses twice the result of Sturges' formula.\n        See :func:`numpy.histogram` documentation for, other available arguments.\n    kind: string\n        If bars (defaults), ranks are represented as stacked histograms (one per chain). If vlines\n        ranks are represented as vertical lines above or below ``ref_line``.\n    colors: string or list of strings\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplotlib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    ref_line: boolean\n        Whether to include a dashed line showing where a uniform distribution would lie\n    labels: bool\n        whether to plot or not the x and y labels, defaults to True\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    ref_line_kwargs : dict, optional\n        Reference line keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.axhline` or\n        :class:`bokeh:bokeh.models.Span`.\n    bar_kwargs : dict, optional\n        Bars keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.bar` or\n        :meth:`bokeh:bokeh.plotting.Figure.vbar`.\n    vlines_kwargs : dict, optional\n        Vlines keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.vlines` or\n        :meth:`bokeh:bokeh.plotting.Figure.multi_line`.\n    marker_vlines_kwargs : dict, optional\n        Marker for the vlines keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.circle`.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_trace : Plot distribution (histogram or kernel density estimates) and\n                 sampled values or rank plot.\n\n    Examples\n    --------\n    Show a default rank plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_rank(data)\n\n    Recreate Figure 13 from the arxiv preprint\n\n    .. plot::\n        :context: close-figs\n\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_rank(data, var_names='tau')\n\n    Use vlines to compare results for centered vs noncentered models\n\n    .. plot::\n        :context: close-figs\n\n        >>> import matplotlib.pyplot as plt\n        >>> centered_data = az.load_arviz_data('centered_eight')\n        >>> noncentered_data = az.load_arviz_data('non_centered_eight')\n        >>> _, ax = plt.subplots(1, 2, figsize=(12, 3))\n        >>> az.plot_rank(centered_data, var_names=\"mu\", kind='vlines', ax=ax[0])\n        >>> az.plot_rank(noncentered_data, var_names=\"mu\", kind='vlines', ax=ax[1])\n\n    Change the aesthetics using kwargs\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_rank(noncentered_data, var_names=\"mu\", kind=\"vlines\",\n        >>>              vlines_kwargs={'lw':0}, marker_vlines_kwargs={'lw':3});\n    \n source"},{"id":163,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_separation","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_separation-Tuple","content":" ArviZPythonPlots.plot_separation  —  Method Separation plot for binary outcome models.\n\n    Model predictions are sorted and plotted using a color code according to\n    the observed data.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        :class:`arviz.InferenceData` object.\n    y : array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat : array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as ``y``. If str or\n        None, ``idata`` must contain the posterior predictive group.\n    y_hat_line : bool, optional\n        Plot the sorted ``y_hat`` predictions.\n    expected_events : bool, optional\n        Plot the total number of expected events.\n    figsize : figure size tuple, optional\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    color : str, optional\n        Color to assign to the positive class. The negative class will be plotted using the\n        same color and an `alpha=0.3` transparency.\n    legend : bool, optional\n        Show the legend of the figure.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    plot_kwargs : dict, optional\n        Additional keywords passed to :meth:`mpl:matplotlib.axes.Axes.bar` or\n        :meth:`bokeh:bokeh.plotting.Figure.vbar` for separation plot.\n    y_hat_line_kwargs : dict, optional\n        Additional keywords passed to ax.plot for ``y_hat`` line.\n    exp_events_kwargs : dict, optional\n        Additional keywords passed to ax.scatter for ``expected_events`` marker.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_ppc : Plot for posterior/prior predictive checks.\n\n    References\n    ----------\n    .. [1] Greenhill, B. *et al.*, The Separation Plot: A New Visual Method\n       for Evaluating the Fit of Binary Models, *American Journal of\n       Political Science*, (2011) see https://doi.org/10.1111/j.1540-5907.2011.00525.x\n\n    Examples\n    --------\n    Separation plot for a logistic regression model.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data('classification10d')\n        >>> az.plot_separation(idata=idata, y='outcome', y_hat='outcome', figsize=(8, 1))\n\n    \n source"},{"id":164,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_trace","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_trace-Tuple","content":" ArviZPythonPlots.plot_trace  —  Method Plot distribution (histogram or kernel density estimates) and sampled values or rank plot.\n\n    If `divergences` data is available in `sample_stats`, will plot the location of divergences as\n    dashed vertical lines.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: str or list of str, optional\n        One or more variables to be plotted. Prefix the variables by ``~`` when you want\n        to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: dict of {str: slice or array_like}, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    divergences: {\"bottom\", \"top\", None}, optional\n        Plot location of divergences on the traceplots.\n    kind: {\"trace\", \"rank_bars\", \"rank_vlines\"}, optional\n        Choose between plotting sampled values per iteration and rank plots.\n    transform: callable, optional\n        Function to transform data (defaults to None i.e.the identity function)\n    figsize: tuple of (float, float), optional\n        If None, size is (12, variables * 2)\n    rug: bool, optional\n        If True adds a rugplot of samples. Defaults to False. Ignored for 2D KDE.\n        Only affects continuous variables.\n    lines: list of tuple of (str, dict, array_like), optional\n        List of (var_name, {'coord': selection}, [line, positions]) to be overplotted as\n        vertical lines on the density and horizontal lines on the trace.\n    circ_var_names : str or list of str, optional\n        List of circular variables to account for when plotting KDE.\n    circ_var_units : str\n        Whether the variables in ``circ_var_names`` are in \"degrees\" or \"radians\".\n    compact: bool, optional\n        Plot multidimensional variables in a single plot.\n    compact_prop: str or dict {str: array_like}, optional\n         Defines the property name and the property values to distinguish different\n        dimensions with compact=True.\n        When compact=True it defaults to color, it is\n        ignored otherwise.\n    combined: bool, optional\n        Flag for combining multiple chains into a single line. If False (default), chains will be\n        plotted separately.\n    chain_prop: str or dict {str: array_like}, optional\n        Defines the property name and the property values to distinguish different chains.\n        If compact=True it defaults to linestyle,\n        otherwise it uses the color to distinguish\n        different chains.\n    legend: bool, optional\n        Add a legend to the figure with the chain color code.\n    plot_kwargs, fill_kwargs, rug_kwargs, hist_kwargs: dict, optional\n        Extra keyword arguments passed to :func:`arviz.plot_dist`. Only affects continuous\n        variables.\n    trace_kwargs: dict, optional\n        Extra keyword arguments passed to :meth:`matplotlib.axes.Axes.plot`\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    rank_kwargs : dict, optional\n        Extra keyword arguments passed to :func:`arviz.plot_rank`\n    axes: axes, optional\n        Matplotlib axes or bokeh figures.\n    backend: {\"matplotlib\", \"bokeh\"}, optional\n        Select plotting backend.\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in rcParams.\n    backend_kwargs: dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_rank : Plot rank order statistics of chains.\n\n    Examples\n    --------\n    Plot a subset variables and select them with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('non_centered_eight')\n        >>> coords = {'school': ['Choate', 'Lawrenceville']}\n        >>> az.plot_trace(data, var_names=('theta'), filter_vars=\"like\", coords=coords)\n\n    Show all dimensions of multidimensional variables in the same plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(data, compact=True)\n\n    Display a rank plot instead of trace\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(data, var_names=[\"mu\", \"tau\"], kind=\"rank_bars\")\n\n    Combine all chains into one distribution and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(\n        >>>     data, var_names=('^theta'), filter_vars=\"regex\", coords=coords, combined=True\n        >>> )\n\n\n    Plot reference lines against distribution and trace\n\n    .. plot::\n        :context: close-figs\n\n        >>> lines = (('theta_t',{'school': \"Choate\"}, [-1]),)\n        >>> az.plot_trace(data, var_names=('theta_t', 'theta'), coords=coords, lines=lines)\n\n    \n source"},{"id":165,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_violin","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_violin-Tuple","content":" ArviZPythonPlots.plot_violin  —  Method Plot posterior of traces as violin plot.\n\n    Notes\n    -----\n    If multiple chains are provided for a variable they will be combined\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e. the identity function).\n    quartiles: bool, optional\n        Flag for plotting the interquartile range, in addition to the ``hdi_prob`` * 100%\n        intervals. Defaults to ``True``.\n    rug: bool\n        If ``True`` adds a jittered rugplot. Defaults to ``False``.\n    side : {\"both\", \"left\", \"right\"}, default \"both\"\n        If ``both``, both sides of the violin plot are rendered. If ``left`` or ``right``, only\n        the respective side is rendered. By separately plotting left and right halfs with\n        different data, split violin plots can be achieved.\n    hdi_prob: float, optional\n        Plots highest posterior density interval for chosen percentage of density.\n        Defaults to 0.94.\n    shade: float\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque). Defaults to 0.\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``circular`` is ``False``\n        and \"taylor\" (for now) when ``circular`` is ``True``.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    circular: bool, optional.\n        If ``True``, it interprets `values` is a circular variable measured in radians\n        and a circular KDE is used. Defaults to ``False``.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: int\n        Text size of the point_estimates, axis ticks, and highest density interval. If None it will\n        be autoscaled based on ``figsize``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    sharex: bool\n        Defaults to ``True``, violinplots share a common x-axis scale.\n    sharey: bool\n        Defaults to ``True``, violinplots share a common y-axis scale.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    shade_kwargs: dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.fill_between`, or\n        :meth:`matplotlib.axes.Axes.barh` to control the shade.\n    rug_kwargs: dict\n        Keywords passed to the rug plot. If true only the right half side of the violin will be\n        plotted.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_forest: Forest plot to compare HDI intervals from a number of distributions.\n\n    Examples\n    --------\n    Show a default violin plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_violin(data)\n\n    \n source"},{"id":168,"pagetitle":"rcParams","title":"rcParams","ref":"/ArviZPythonPlots/stable/api/rcparams/#rcparams-api","content":" rcParams ArviZPythonPlots.rcParams ArviZPythonPlots.rc_context"},{"id":169,"pagetitle":"rcParams","title":"Reference","ref":"/ArviZPythonPlots/stable/api/rcparams/#Reference","content":" Reference"},{"id":170,"pagetitle":"rcParams","title":"ArviZPythonPlots.rcParams","ref":"/ArviZPythonPlots/stable/api/rcparams/#ArviZPythonPlots.rcParams","content":" ArviZPythonPlots.rcParams  —  Constant Class to contain ArviZ default parameters.\n\n    It is implemented as a dict with validation when setting items.\n    \n source"},{"id":171,"pagetitle":"rcParams","title":"ArviZPythonPlots.rc_context","ref":"/ArviZPythonPlots/stable/api/rcparams/#ArviZPythonPlots.rc_context-Tuple","content":" ArviZPythonPlots.rc_context  —  Method \n    Return a context manager for managing rc settings.\n\n    Parameters\n    ----------\n    rc : dict, optional\n        Mapping containing the rcParams to modify temporally.\n    fname : str, optional\n        Filename of the file containing the rcParams to use inside the rc_context.\n\n    Examples\n    --------\n    This allows one to do::\n\n        with az.rc_context(fname='pystan.rc'):\n            idata = az.load_arviz_data(\"radon\")\n            az.plot_posterior(idata, var_names=[\"gamma\"])\n\n    The plot would have settings from 'screen.rc'\n\n    A dictionary can also be passed to the context manager::\n\n        with az.rc_context(rc={'plot.max_subplots': None}, fname='pystan.rc'):\n            idata = az.load_arviz_data(\"radon\")\n            az.plot_posterior(idata, var_names=[\"gamma\"])\n\n    The 'rc' dictionary takes precedence over the settings loaded from\n    'fname'. Passing a dictionary only is also valid.\n    \n source"},{"id":174,"pagetitle":"Plotting styles","title":"Plotting styles","ref":"/ArviZPythonPlots/stable/api/style/#style-api","content":" Plotting styles ArviZPythonPlots.styles ArviZPythonPlots.use_style"},{"id":175,"pagetitle":"Plotting styles","title":"Reference","ref":"/ArviZPythonPlots/stable/api/style/#Reference","content":" Reference"},{"id":176,"pagetitle":"Plotting styles","title":"ArviZPythonPlots.styles","ref":"/ArviZPythonPlots/stable/api/style/#ArviZPythonPlots.styles-Tuple{}","content":" ArviZPythonPlots.styles  —  Method styles() -> Vector{String} Get all available matplotlib styles for use with  use_style source"},{"id":177,"pagetitle":"Plotting styles","title":"ArviZPythonPlots.use_style","ref":"/ArviZPythonPlots/stable/api/style/#ArviZPythonPlots.use_style-Tuple{Any}","content":" ArviZPythonPlots.use_style  —  Method use_style(style::String)\nuse_style(style::Vector{String}) Use matplotlib style settings from a style specification  style . The style name of \"default\" is reserved for reverting back to the default style settings. ArviZ-specific styles include  [\"arviz-whitegrid\", \"arviz-darkgrid\", \"arviz-colors\", \"arviz-white\", \"arviz-doc\"] . To see all available style specifications, use  styles() . If a  Vector  of styles is provided, they are applied from first to last. source"},{"id":180,"pagetitle":"Examples gallery","title":"Example Gallery","ref":"/ArviZPythonPlots/stable/examples/#Example-Gallery","content":" Example Gallery"},{"id":181,"pagetitle":"Examples gallery","title":"Autocorrelation Plot","ref":"/ArviZPythonPlots/stable/examples/#Autocorrelation-Plot","content":" Autocorrelation Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_autocorr(data; var_names=[\"tau\", \"mu\"])\ngcf() See  plot_autocorr"},{"id":182,"pagetitle":"Examples gallery","title":"Bayes Factor Plot","ref":"/ArviZPythonPlots/stable/examples/#Bayes-Factor-Plot","content":" Bayes Factor Plot using ArviZ, ArviZPythonPlots\n\nuse_style(\"arviz-darkgrid\")\n\nidata = from_namedtuple((a = 1 .+ randn(5_000) ./ 2,), prior=(a = randn(5_000),))\nplot_bf(idata; var_name=\"a\", ref_val=0)\ngcf() See  plot_bf"},{"id":183,"pagetitle":"Examples gallery","title":"Bayesian P-Value Posterior Plot","ref":"/ArviZPythonPlots/stable/examples/#Bayesian-P-Value-Posterior-Plot","content":" Bayesian P-Value Posterior Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nplot_bpv(data)\ngcf() See  plot_bpv"},{"id":184,"pagetitle":"Examples gallery","title":"Bayesian P-Value with Median T Statistic Posterior Plot","ref":"/ArviZPythonPlots/stable/examples/#Bayesian-P-Value-with-Median-T-Statistic-Posterior-Plot","content":" Bayesian P-Value with Median T Statistic Posterior Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nplot_bpv(data; kind=\"t_stat\", t_stat=\"0.5\")\ngcf() See  plot_bpv"},{"id":185,"pagetitle":"Examples gallery","title":"Compare Plot","ref":"/ArviZPythonPlots/stable/examples/#Compare-Plot","content":" Compare Plot using ArviZ, ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nmodel_compare = compare(\n    (\n        var\"Centered 8 schools\" = load_example_data(\"centered_eight\"),\n        var\"Non-centered 8 schools\" = load_example_data(\"non_centered_eight\"),\n    ),\n)\nplot_compare(model_compare; figsize=(12, 4))\ngcf() See  compare ,  plot_compare"},{"id":186,"pagetitle":"Examples gallery","title":"Density Plot","ref":"/ArviZPythonPlots/stable/examples/#Density-Plot","content":" Density Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered_data = load_example_data(\"centered_eight\")\nnon_centered_data = load_example_data(\"non_centered_eight\")\nplot_density(\n    [centered_data, non_centered_data];\n    data_labels=[\"Centered\", \"Non Centered\"],\n    var_names=[\"theta\"],\n    shade=0.1,\n)\ngcf() See  plot_density"},{"id":187,"pagetitle":"Examples gallery","title":"Dist Plot","ref":"/ArviZPythonPlots/stable/examples/#Dist-Plot","content":" Dist Plot using ArviZPythonPlots, Distributions, Random\n\nRandom.seed!(308)\n\nuse_style(\"arviz-darkgrid\")\n\na = rand(Poisson(4), 1000)\nb = rand(Normal(0, 1), 1000)\n_, ax = subplots(1, 2; figsize=(10, 4))\nplot_dist(a; color=\"C1\", label=\"Poisson\", ax=ax[0])\nplot_dist(b; color=\"C2\", label=\"Gaussian\", ax=ax[1])\ngcf() See  plot_dist"},{"id":188,"pagetitle":"Examples gallery","title":"Dot Plot","ref":"/ArviZPythonPlots/stable/examples/#Dot-Plot","content":" Dot Plot using ArviZPythonPlots\n\nuse_style(\"arviz-darkgrid\")\n\ndata = randn(1000)\nplot_dot(data; dotcolor=\"C1\", point_interval=true)\ntitle(\"Gaussian Distribution\")\ngcf() See  plot_dot"},{"id":189,"pagetitle":"Examples gallery","title":"ECDF Plot","ref":"/ArviZPythonPlots/stable/examples/#ECDF-Plot","content":" ECDF Plot using ArviZPythonPlots, Distributions\n\nuse_style(\"arviz-darkgrid\")\n\nsample = randn(1_000)\ndist = Normal()\nplot_ecdf(sample; cdf=x -> cdf(dist, x), confidence_bands=true)\ngcf() See  plot_ecdf"},{"id":190,"pagetitle":"Examples gallery","title":"ELPD Plot","ref":"/ArviZPythonPlots/stable/examples/#ELPD-Plot","content":" ELPD Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nd1 = load_example_data(\"centered_eight\")\nd2 = load_example_data(\"non_centered_eight\")\nplot_elpd(Dict(\"Centered eight\" => d1, \"Non centered eight\" => d2); xlabels=true)\ngcf() See  plot_elpd"},{"id":191,"pagetitle":"Examples gallery","title":"Energy Plot","ref":"/ArviZPythonPlots/stable/examples/#Energy-Plot","content":" Energy Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_energy(data; figsize=(12, 8))\ngcf() See  plot_energy"},{"id":192,"pagetitle":"Examples gallery","title":"ESS Evolution Plot","ref":"/ArviZPythonPlots/stable/examples/#ESS-Evolution-Plot","content":" ESS Evolution Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nplot_ess(idata; var_names=[\"b\"], kind=\"evolution\")\ngcf() See  plot_ess"},{"id":193,"pagetitle":"Examples gallery","title":"ESS Local Plot","ref":"/ArviZPythonPlots/stable/examples/#ESS-Local-Plot","content":" ESS Local Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"non_centered_eight\")\nplot_ess(idata; var_names=[\"mu\"], kind=\"local\", marker=\"_\", ms=20, mew=2, rug=true)\ngcf() See  plot_ess"},{"id":194,"pagetitle":"Examples gallery","title":"ESS Quantile Plot","ref":"/ArviZPythonPlots/stable/examples/#ESS-Quantile-Plot","content":" ESS Quantile Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nplot_ess(idata; var_names=[\"sigma\"], kind=\"quantile\", color=\"C4\")\ngcf() See  plot_ess"},{"id":195,"pagetitle":"Examples gallery","title":"Forest Plot","ref":"/ArviZPythonPlots/stable/examples/#Forest-Plot","content":" Forest Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered_data = load_example_data(\"centered_eight\")\nnon_centered_data = load_example_data(\"non_centered_eight\")\nplot_forest(\n    [centered_data, non_centered_data];\n    model_names=[\"Centered\", \"Non Centered\"],\n    var_names=[\"mu\"],\n)\ntitle(\"Estimated theta for eight schools model\")\ngcf() See  plot_forest"},{"id":196,"pagetitle":"Examples gallery","title":"Ridge Plot","ref":"/ArviZPythonPlots/stable/examples/#Ridge-Plot","content":" Ridge Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nrugby_data = load_example_data(\"rugby\")\nplot_forest(\n    rugby_data;\n    kind=\"ridgeplot\",\n    var_names=[\"defs\"],\n    linewidth=4,\n    combined=true,\n    ridgeplot_overlap=1.5,\n    colors=\"blue\",\n    figsize=(9, 4),\n)\ntitle(\"Relative defensive strength\\nof Six Nation rugby teams\")\ngcf() See  plot_forest"},{"id":197,"pagetitle":"Examples gallery","title":"Plot HDI","ref":"/ArviZPythonPlots/stable/examples/#Plot-HDI","content":" Plot HDI using Random\nusing ArviZPythonPlots\n\nRandom.seed!(308)\n\nuse_style(\"arviz-darkgrid\")\n\nx_data = randn(100)\ny_data = 2 .+ x_data .* 0.5\ny_data_rep = 0.5 .* randn(200, 100) .+ transpose(y_data)\n\nplot(x_data, y_data; color=\"C6\")\nplot_hdi(x_data, y_data_rep; color=\"k\", plot_kwargs=Dict(\"ls\" => \"--\"))\ngcf() See  plot_hdi"},{"id":198,"pagetitle":"Examples gallery","title":"Joint Plot","ref":"/ArviZPythonPlots/stable/examples/#Joint-Plot","content":" Joint Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_pair(\n    data;\n    var_names=[\"theta\"],\n    coords=Dict(\"school\" => [\"Choate\", \"Phillips Andover\"]),\n    kind=\"hexbin\",\n    marginals=true,\n    figsize=(10, 10),\n)\ngcf() See  plot_pair"},{"id":199,"pagetitle":"Examples gallery","title":"KDE Plot","ref":"/ArviZPythonPlots/stable/examples/#KDE-Plot","content":" KDE Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\n\n## Combine different posterior draws from different chains\nobs = data.posterior_predictive.obs\nsize_obs = size(obs)\ny_hat = reshape(obs, prod(size_obs[1:2]), size_obs[3:end]...)\n\nplot_kde(\n    y_hat;\n    label=\"Estimated Effect\\n of SAT Prep\",\n    rug=true,\n    plot_kwargs=Dict(\"linewidth\" => 2, \"color\" => \"black\"),\n    rug_kwargs=Dict(\"color\" => \"black\"),\n)\ngcf() See  plot_kde"},{"id":200,"pagetitle":"Examples gallery","title":"2d KDE","ref":"/ArviZPythonPlots/stable/examples/#d-KDE","content":" 2d KDE using Random\nusing ArviZPythonPlots\n\nRandom.seed!(308)\n\nuse_style(\"arviz-darkgrid\")\n\nplot_kde(rand(100), rand(100))\ngcf() See  plot_kde"},{"id":201,"pagetitle":"Examples gallery","title":"KDE Quantiles Plot","ref":"/ArviZPythonPlots/stable/examples/#KDE-Quantiles-Plot","content":" KDE Quantiles Plot using Random\nusing Distributions\nusing ArviZPythonPlots\n\nRandom.seed!(308)\n\nuse_style(\"arviz-darkgrid\")\n\ndist = rand(Beta(rand(Uniform(0.5, 10)), 5), 1000)\nplot_kde(dist; quantiles=[0.25, 0.5, 0.75])\ngcf() See  plot_kde"},{"id":202,"pagetitle":"Examples gallery","title":"Pareto Shape Plot","ref":"/ArviZPythonPlots/stable/examples/#Pareto-Shape-Plot","content":" Pareto Shape Plot using ArviZ, ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nloo_data = loo(idata)\nplot_khat(loo_data; show_bins=true)\ngcf() See  loo ,  plot_khat"},{"id":203,"pagetitle":"Examples gallery","title":"LOO-PIT ECDF Plot","ref":"/ArviZPythonPlots/stable/examples/#LOO-PIT-ECDF-Plot","content":" LOO-PIT ECDF Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\n\nplot_loo_pit(idata; y=\"y\", ecdf=true, color=\"maroon\")\ngcf() See  loo_pit ,  plot_loo_pit"},{"id":204,"pagetitle":"Examples gallery","title":"LOO-PIT Overlay Plot","ref":"/ArviZPythonPlots/stable/examples/#LOO-PIT-Overlay-Plot","content":" LOO-PIT Overlay Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"non_centered_eight\")\nplot_loo_pit(; idata, y=\"obs\", color=\"indigo\")\ngcf() See  loo_pit ,  plot_loo_pit"},{"id":205,"pagetitle":"Examples gallery","title":"Quantile Monte Carlo Standard Error Plot","ref":"/ArviZPythonPlots/stable/examples/#Quantile-Monte-Carlo-Standard-Error-Plot","content":" Quantile Monte Carlo Standard Error Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_mcse(data; var_names=[\"tau\", \"mu\"], rug=true, extra_methods=true)\ngcf() See  plot_mcse"},{"id":206,"pagetitle":"Examples gallery","title":"Quantile MCSE Errobar Plot","ref":"/ArviZPythonPlots/stable/examples/#Quantile-MCSE-Errobar-Plot","content":" Quantile MCSE Errobar Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"radon\")\nplot_mcse(data; var_names=[\"sigma_a\"], color=\"C4\", errorbar=true)\ngcf() See  plot_mcse"},{"id":207,"pagetitle":"Examples gallery","title":"Pair Plot","ref":"/ArviZPythonPlots/stable/examples/#Pair-Plot","content":" Pair Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered; var_names=[\"theta\", \"mu\", \"tau\"], coords, divergences=true, textsize=22\n)\ngcf() See  plot_pair"},{"id":208,"pagetitle":"Examples gallery","title":"Hexbin Pair Plot","ref":"/ArviZPythonPlots/stable/examples/#Hexbin-Pair-Plot","content":" Hexbin Pair Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[\"theta\", \"mu\", \"tau\"],\n    kind=\"hexbin\",\n    coords,\n    colorbar=true,\n    divergences=true,\n)\ngcf() See  plot_pair"},{"id":209,"pagetitle":"Examples gallery","title":"KDE Pair Plot","ref":"/ArviZPythonPlots/stable/examples/#KDE-Pair-Plot","content":" KDE Pair Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[\"theta\", \"mu\", \"tau\"],\n    kind=\"kde\",\n    coords,\n    divergences=true,\n    textsize=22,\n)\ngcf() See  plot_pair"},{"id":210,"pagetitle":"Examples gallery","title":"Point Estimate Pair Plot","ref":"/ArviZPythonPlots/stable/examples/#Point-Estimate-Pair-Plot","content":" Point Estimate Pair Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[\"mu\", \"theta\"],\n    kind=[\"scatter\", \"kde\"],\n    kde_kwargs=Dict(\"fill_last\" => false),\n    marginals=true,\n    coords,\n    point_estimate=\"median\",\n    figsize=(10, 8),\n)\ngcf() See  plot_pair"},{"id":211,"pagetitle":"Examples gallery","title":"Parallel Plot","ref":"/ArviZPythonPlots/stable/examples/#Parallel-Plot","content":" Parallel Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nax = plot_parallel(data; var_names=[\"theta\", \"tau\", \"mu\"])\nax.set_xticklabels(ax.get_xticklabels(); rotation=70)\ndraw()\ngcf() See  plot_parallel"},{"id":212,"pagetitle":"Examples gallery","title":"Posterior Plot","ref":"/ArviZPythonPlots/stable/examples/#Posterior-Plot","content":" Posterior Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\"])\nplot_posterior(data; var_names=[\"mu\", \"theta\"], coords, rope=(-1, 1))\ngcf() See  plot_posterior"},{"id":213,"pagetitle":"Examples gallery","title":"Posterior Predictive Check Plot","ref":"/ArviZPythonPlots/stable/examples/#Posterior-Predictive-Check-Plot","content":" Posterior Predictive Check Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_ppc(data; data_pairs=Dict(\"obs\" => \"obs\"), alpha=0.03, figsize=(12, 6), textsize=14)\ngcf() See  plot_ppc"},{"id":214,"pagetitle":"Examples gallery","title":"Posterior Predictive Check Cumulative Plot","ref":"/ArviZPythonPlots/stable/examples/#Posterior-Predictive-Check-Cumulative-Plot","content":" Posterior Predictive Check Cumulative Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_ppc(data; alpha=0.3, kind=\"cumulative\", figsize=(12, 6), textsize=14)\ngcf() See  plot_ppc"},{"id":215,"pagetitle":"Examples gallery","title":"Rank Plot","ref":"/ArviZPythonPlots/stable/examples/#Rank-Plot","content":" Rank Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_rank(data; var_names=[\"tau\", \"mu\"])\ngcf() See  plot_rank"},{"id":216,"pagetitle":"Examples gallery","title":"Regression Plot","ref":"/ArviZPythonPlots/stable/examples/#Regression-Plot","content":" Regression Plot using ArviZ, ArviZPythonPlots, ArviZExampleData, DimensionalData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nx = range(0, 1; length=100)\nposterior = data.posterior\nconstant_data = convert_to_dataset((; x); default_dims=())\ny_model = broadcast_dims(muladd, posterior.intercept, posterior.slope, constant_data.x)\nposterior = merge(posterior, (; y_model))\ndata = merge(data, InferenceData(; posterior, constant_data))\nplot_lm(\"y\"; idata=data, x=\"x\", y_model=\"y_model\")\ngcf() See  plot_lm"},{"id":217,"pagetitle":"Examples gallery","title":"Separation Plot","ref":"/ArviZPythonPlots/stable/examples/#Separation-Plot","content":" Separation Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"classification10d\")\nplot_separation(data; y=\"outcome\", y_hat=\"outcome\", figsize=(8, 1))\ngcf() See  plot_separation"},{"id":218,"pagetitle":"Examples gallery","title":"Trace Plot","ref":"/ArviZPythonPlots/stable/examples/#Trace-Plot","content":" Trace Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_trace(data; var_names=[\"tau\", \"mu\"])\ngcf() See  plot_trace"},{"id":219,"pagetitle":"Examples gallery","title":"Violin Plot","ref":"/ArviZPythonPlots/stable/examples/#Violin-Plot","content":" Violin Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_violin(data; var_names=[\"mu\", \"tau\"])\ngcf() See  plot_violin"},{"id":220,"pagetitle":"Examples gallery","title":"Styles","ref":"/ArviZPythonPlots/stable/examples/#Styles","content":" Styles using ArviZPythonPlots, Distributions, PythonCall\n\nx = range(0, 1; length=100)\ndist = pdf.(Beta(2, 5), x)\n\nstyle_list = [\n    \"default\",\n    [\"default\", \"arviz-colors\"],\n    \"arviz-darkgrid\",\n    \"arviz-whitegrid\",\n    \"arviz-white\",\n    \"arviz-grayscale\",\n    [\"arviz-white\", \"arviz-redish\"],\n    [\"arviz-white\", \"arviz-bluish\"],\n    [\"arviz-white\", \"arviz-orangish\"],\n    [\"arviz-white\", \"arviz-brownish\"],\n    [\"arviz-white\", \"arviz-purplish\"],\n    [\"arviz-white\", \"arviz-cyanish\"],\n    [\"arviz-white\", \"arviz-greenish\"],\n    [\"arviz-white\", \"arviz-royish\"],\n    [\"arviz-white\", \"arviz-viridish\"],\n    [\"arviz-white\", \"arviz-plasmish\"],\n    \"arviz-doc\",\n    \"arviz-docgrid\",\n]\n\nfig = figure(; figsize=(20, 10))\nfor (idx, style) in enumerate(style_list)\n    pywith(pyplot.style.context(style; after_reset=true)) do _\n        ax = fig.add_subplot(5, 4, idx; label=idx)\n        colors = pyplot.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n        for i in 0:(length(colors) - 1)\n            ax.plot(x, dist .- i, \"C$i\"; label=\"C$i\")\n        end\n        ax.set_title(style)\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"f(x)\"; rotation=0, labelpad=15)\n        ax.set_xticklabels([])\n    end\nend\ntight_layout()\ngcf()"},{"id":223,"pagetitle":"Home","title":"PosteriorStats","ref":"/PosteriorStats/stable/#PosteriorStats","content":" PosteriorStats PosteriorStats implements widely-used and well-characterized statistical analyses for the Bayesian workflow. These functions generally estimate properties of posterior and/or posterior predictive distributions. The default implementations defined here operate on Monte Carlo samples. See the  API  for details."},{"id":224,"pagetitle":"Home","title":"Extending this package","ref":"/PosteriorStats/stable/#Extending-this-package","content":" Extending this package The methods defined here are intended to be extended by two types of packages. packages that implement data types for storing Monte Carlo samples packages that implement other representations for posterior distributions than Monte Carlo draws"},{"id":227,"pagetitle":"API","title":"API","ref":"/PosteriorStats/stable/api/#API","content":" API"},{"id":228,"pagetitle":"API","title":"Summary statistics","ref":"/PosteriorStats/stable/api/#Summary-statistics","content":" Summary statistics"},{"id":229,"pagetitle":"API","title":"PosteriorStats.SummaryStats","ref":"/PosteriorStats/stable/api/#PosteriorStats.SummaryStats","content":" PosteriorStats.SummaryStats  —  Type A container for a column table of values computed by  summarize . This object implements the Tables and TableTraits interfaces and has a custom  show  method. name : The name of the collection of summary statistics, used as the table title in display. data : The summary statistics for each parameter, with an optional first column  parameter  containing the parameter names. source"},{"id":230,"pagetitle":"API","title":"PosteriorStats.default_diagnostics","ref":"/PosteriorStats/stable/api/#PosteriorStats.default_diagnostics","content":" PosteriorStats.default_diagnostics  —  Function default_diagnostics(focus=Statistics.mean; kwargs...) Default diagnostics to be computed with  summarize . The value of  focus  determines the diagnostics to be returned: Statistics.mean :  mcse_mean ,  mcse_std ,  ess_tail ,  ess_bulk ,  rhat Statistics.median :  mcse_median ,  ess_tail ,  ess_bulk ,  rhat source"},{"id":231,"pagetitle":"API","title":"PosteriorStats.default_stats","ref":"/PosteriorStats/stable/api/#PosteriorStats.default_stats","content":" PosteriorStats.default_stats  —  Function default_stats(focus=Statistics.mean; prob_interval=0.94, kwargs...) Default statistics to be computed with  summarize . The value of  focus  determines the statistics to be returned: Statistics.mean :  mean ,  std ,  hdi_3% ,  hdi_97% Statistics.median :  median ,  mad ,  eti_3% ,  eti_97% If  prob_interval  is set to a different value than the default, then different HDI and ETI statistics are computed accordingly.  hdi  refers to the highest-density interval, while  eti  refers to the equal-tailed interval (i.e. the credible interval computed from symmetric quantiles). See also:  hdi source"},{"id":232,"pagetitle":"API","title":"PosteriorStats.default_summary_stats","ref":"/PosteriorStats/stable/api/#PosteriorStats.default_summary_stats","content":" PosteriorStats.default_summary_stats  —  Function default_summary_stats(focus=Statistics.mean; kwargs...) Combinatiton of  default_stats  and  default_diagnostics  to be used with  summarize . source"},{"id":233,"pagetitle":"API","title":"PosteriorStats.summarize","ref":"/PosteriorStats/stable/api/#PosteriorStats.summarize","content":" PosteriorStats.summarize  —  Function summarize(data, stats_funs...; name=\"SummaryStats\", [var_names]) -> SummaryStats Compute the summary statistics in  stats_funs  on each param in  data . stats_funs  is a collection of functions that reduces a matrix with shape  (draws, chains)  to a scalar or a collection of scalars. Alternatively, an item in  stats_funs  may be a  Pair  of the form  name => fun  specifying the name to be used for the statistic or of the form  (name1, ...) => fun  when the function returns a collection. When the function returns a collection, the names in this latter format must be provided. If no stats functions are provided, then those specified in  default_summary_stats  are computed. var_names  specifies the names of the parameters in  data . If not provided, the names are inferred from  data . To support computing summary statistics from a custom object, overload this method specifying the type of  data . See also  SummaryStats ,  default_summary_stats ,  default_stats ,  default_diagnostics . Examples Compute  mean ,  std  and the Monte Carlo standard error (MCSE) of the mean estimate: julia> using Statistics, StatsBase\n\njulia> x = randn(1000, 4, 3) .+ reshape(0:10:20, 1, 1, :);\n\njulia> summarize(x, mean, std, :mcse_mean => sem; name=\"Mean/Std\")\nMean/Std\n       mean    std  mcse_mean\n 1   0.0003  0.990      0.016\n 2  10.02    0.988      0.016\n 3  19.98    0.988      0.016 Avoid recomputing the mean by using  mean_and_std , and provide parameter names: julia> summarize(x, (:mean, :std) => mean_and_std, mad; var_names=[:a, :b, :c])\nSummaryStats\n         mean    std    mad\n a   0.000305  0.990  0.978\n b  10.0       0.988  0.995\n c  20.0       0.988  0.979 Note that when an estimator and its MCSE are both computed, the MCSE is used to determine the number of significant digits that will be displayed. julia> summarize(x; var_names=[:a, :b, :c])\nSummaryStats\n       mean   std  hdi_3%  hdi_97%  mcse_mean  mcse_std  ess_tail  ess_bulk  r ⋯\n a   0.0003  0.99   -1.92     1.78      0.016     0.012      3567      3663  1 ⋯\n b  10.02    0.99    8.17    11.9       0.016     0.011      3841      3906  1 ⋯\n c  19.98    0.99   18.1     21.9       0.016     0.012      3892      3749  1 ⋯\n                                                                1 column omitted Compute just the statistics with an 89% HDI on all parameters, and provide the parameter names: julia> summarize(x, default_stats(; prob_interval=0.89)...; var_names=[:a, :b, :c])\nSummaryStats\n         mean    std  hdi_5.5%  hdi_94.5%\n a   0.000305  0.990     -1.63       1.52\n b  10.0       0.988      8.53      11.6\n c  20.0       0.988     18.5       21.6 Compute the summary stats focusing on  Statistics.median : julia> summarize(x, default_summary_stats(median)...; var_names=[:a, :b, :c])\nSummaryStats\n    median    mad  eti_3%  eti_97%  mcse_median  ess_tail  ess_median  rhat\n a   0.004  0.978   -1.83     1.89        0.020      3567        3336  1.00\n b  10.02   0.995    8.17    11.9         0.023      3841        3787  1.00\n c  19.99   0.979   18.1     21.9         0.020      3892        3829  1.00 source"},{"id":234,"pagetitle":"API","title":"General statistics","ref":"/PosteriorStats/stable/api/#General-statistics","content":" General statistics"},{"id":235,"pagetitle":"API","title":"PosteriorStats.hdi","ref":"/PosteriorStats/stable/api/#PosteriorStats.hdi","content":" PosteriorStats.hdi  —  Function hdi(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) Estimate the unimodal highest density interval (HDI) of  samples  for the probability  prob . The HDI is the minimum width Bayesian credible interval (BCI). That is, it is the smallest possible interval containing  (100*prob) % of the probability mass. [Hyndman1996] samples  is an array of shape  (draws[, chains[, params...]]) . If multiple parameters are present, then  lower  and  upper  are arrays with the shape  (params...,) , computed separately for each marginal. This implementation uses the algorithm of  [ChenShao1999] . Note Any default value of  prob  is arbitrary. The default value of  prob=0.94  instead of a more common default like  prob=0.95  is chosen to reminder the user of this arbitrariness. Examples Here we calculate the 83% HDI for a normal random variable: julia> x = randn(2_000);\n\njulia> hdi(x; prob=0.83) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => -1.38266\n  :upper => 1.25982 We can also calculate the HDI for a 3-dimensional array of samples: julia> x = randn(1_000, 1, 1) .+ reshape(0:5:10, 1, 1, :);\n\njulia> hdi(x) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => [-1.9674, 3.0326, 8.0326]\n  :upper => [1.90028, 6.90028, 11.9003] source"},{"id":236,"pagetitle":"API","title":"PosteriorStats.hdi!","ref":"/PosteriorStats/stable/api/#PosteriorStats.hdi!","content":" PosteriorStats.hdi!  —  Function hdi!(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) A version of  hdi  that sorts  samples  in-place while computing the HDI. source"},{"id":237,"pagetitle":"API","title":"PosteriorStats.r2_score","ref":"/PosteriorStats/stable/api/#PosteriorStats.r2_score","content":" PosteriorStats.r2_score  —  Function r2_score(y_true::AbstractVector, y_pred::AbstractVecOrMat) -> (; r2, r2_std) $R²$  for linear Bayesian regression models. [GelmanGoodrich2019] Arguments y_true : Observed data of length  noutputs y_pred : Predicted data with size  (ndraws[, nchains], noutputs) Examples julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"regression1d\");\n\njulia> y_true = idata.observed_data.y;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.y, (:draw, :chain, :y_dim_0));\n\njulia> r2_score(y_true, y_pred) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.683197\n  :r2_std => 0.0368838 source"},{"id":238,"pagetitle":"API","title":"LOO and WAIC","ref":"/PosteriorStats/stable/api/#LOO-and-WAIC","content":" LOO and WAIC"},{"id":239,"pagetitle":"API","title":"PosteriorStats.AbstractELPDResult","ref":"/PosteriorStats/stable/api/#PosteriorStats.AbstractELPDResult","content":" PosteriorStats.AbstractELPDResult  —  Type abstract type AbstractELPDResult An abstract type representing the result of an ELPD computation. Every subtype stores estimates of both the expected log predictive density ( elpd ) and the effective number of parameters  p , as well as standard errors and pointwise estimates of each, from which other relevant estimates can be computed. Subtypes implement the following functions: elpd_estimates information_criterion source"},{"id":240,"pagetitle":"API","title":"PosteriorStats.PSISLOOResult","ref":"/PosteriorStats/stable/api/#PosteriorStats.PSISLOOResult","content":" PosteriorStats.PSISLOOResult  —  Type Results of Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO). See also:  loo ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates psis_result : Pareto-smoothed importance sampling (PSIS) results source"},{"id":241,"pagetitle":"API","title":"PosteriorStats.WAICResult","ref":"/PosteriorStats/stable/api/#PosteriorStats.WAICResult","content":" PosteriorStats.WAICResult  —  Type Results of computing the widely applicable information criterion (WAIC). See also:  waic ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates source"},{"id":242,"pagetitle":"API","title":"PosteriorStats.elpd_estimates","ref":"/PosteriorStats/stable/api/#PosteriorStats.elpd_estimates","content":" PosteriorStats.elpd_estimates  —  Function elpd_estimates(result::AbstractELPDResult; pointwise=false) -> (; elpd, elpd_mcse, lpd) Return the (E)LPD estimates from the  result . source"},{"id":243,"pagetitle":"API","title":"PosteriorStats.information_criterion","ref":"/PosteriorStats/stable/api/#PosteriorStats.information_criterion","content":" PosteriorStats.information_criterion  —  Function information_criterion(elpd, scale::Symbol) Compute the information criterion for the given  scale  from the  elpd  estimate. scale  must be one of  (:deviance, :log, :negative_log) . See also:  loo ,  waic source information_criterion(result::AbstractELPDResult, scale::Symbol; pointwise=false) Compute information criterion for the given  scale  from the existing ELPD  result . scale  must be one of  (:deviance, :log, :negative_log) . If  pointwise=true , then pointwise estimates are returned. source"},{"id":244,"pagetitle":"API","title":"PosteriorStats.loo","ref":"/PosteriorStats/stable/api/#PosteriorStats.loo","content":" PosteriorStats.loo  —  Function loo(log_likelihood; reff=nothing, kwargs...) -> PSISLOOResult{<:NamedTuple,<:NamedTuple} Compute the Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO).  [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . Keywords reff::Union{Real,AbstractArray{<:Real}} : The relative effective sample size(s) of the  likelihood  values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using  MCMCDiagnosticTools.ess . kwargs : Remaining keywords are forwarded to [ PSIS.psis ]. See also:  PSISLOOResult ,  waic Examples Manually compute  $R_\\mathrm{eff}$  and calculate PSIS-LOO of a model: julia> using ArviZExampleData, MCMCDiagnosticTools\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> reff = ess(log_like; kind=:basic, split_chains=1, relative=true);\n\njulia> loo(log_like; reff)\nPSISLOOResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.34\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  7 (87.5%)  151\n  (0.5, 0.7]  okay  1 (12.5%)  446 source"},{"id":245,"pagetitle":"API","title":"PosteriorStats.waic","ref":"/PosteriorStats/stable/api/#PosteriorStats.waic","content":" PosteriorStats.waic  —  Function waic(log_likelihood::AbstractArray) -> WAICResult{<:NamedTuple,<:NamedTuple} Compute the widely applicable information criterion (WAIC). [Watanabe2010] [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . See also:  WAICResult ,  loo Examples Calculate WAIC of a model: julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> waic(log_like)\nWAICResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.33 source"},{"id":246,"pagetitle":"API","title":"Model comparison","ref":"/PosteriorStats/stable/api/#Model-comparison","content":" Model comparison"},{"id":247,"pagetitle":"API","title":"PosteriorStats.ModelComparisonResult","ref":"/PosteriorStats/stable/api/#PosteriorStats.ModelComparisonResult","content":" PosteriorStats.ModelComparisonResult  —  Type ModelComparisonResult Result of model comparison using ELPD. This struct implements the Tables and TableTraits interfaces. Each field returns a collection of the corresponding entry for each model: name : Names of the models, if provided. rank : Ranks of the models (ordered by decreasing ELPD) elpd_diff : ELPD of a model subtracted from the largest ELPD of any model elpd_diff_mcse : Monte Carlo standard error of the ELPD difference weight : Model weights computed with  weights_method elpd_result :  AbstactELPDResult s for each model, which can be used to access useful stats like ELPD estimates, pointwise estimates, and Pareto shape values for PSIS-LOO weights_method : Method used to compute model weights with  model_weights source"},{"id":248,"pagetitle":"API","title":"PosteriorStats.compare","ref":"/PosteriorStats/stable/api/#PosteriorStats.compare","content":" PosteriorStats.compare  —  Function compare(models; kwargs...) -> ModelComparisonResult Compare models based on their expected log pointwise predictive density (ELPD). The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353 Arguments models : a  Tuple ,  NamedTuple , or  AbstractVector  whose values are either  AbstractELPDResult  entries or any argument to  elpd_method . Keywords weights_method::AbstractModelWeightsMethod=Stacking() : the method to be used to weight the models. See  model_weights  for details elpd_method=loo : a method that computes an  AbstractELPDResult  from an argument in  models . sort::Bool=true : Whether to sort models by decreasing ELPD. Returns ModelComparisonResult : A container for the model comparison results. The fields contain a similar collection to  models . Examples Compare the centered and non centered models of the eight school problem using the defaults:  loo  and  Stacking  weights. A custom  myloo  method formates the inputs as expected by  loo . julia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> function myloo(idata)\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n\njulia> mc = compare(models; elpd_method=myloo)\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\nModelComparisonResult with Stacking weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0       1.0  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067     0.0  0.9   ⋯\n                                                                1 column omitted\njulia> mc.weight |> pairs\npairs(::NamedTuple) with 2 entries:\n  :non_centered => 1.0\n  :centered     => 5.34175e-19 Compare the same models from pre-computed PSIS-LOO results and computing  BootstrappedPseudoBMA  weights: julia> elpd_results = mc.elpd_result;\n\njulia> compare(elpd_results; weights_method=BootstrappedPseudoBMA())\nModelComparisonResult with BootstrappedPseudoBMA weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0      0.52  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067    0.48  0.9   ⋯\n                                                                1 column omitted source"},{"id":249,"pagetitle":"API","title":"PosteriorStats.model_weights","ref":"/PosteriorStats/stable/api/#PosteriorStats.model_weights","content":" PosteriorStats.model_weights  —  Function model_weights(elpd_results; method=Stacking())\nmodel_weights(method::AbstractModelWeightsMethod, elpd_results) Compute weights for each model in  elpd_results  using  method . elpd_results  is a  Tuple ,  NamedTuple , or  AbstractVector  with  AbstractELPDResult  entries. The weights are returned in the same type of collection. Stacking  is the recommended approach, as it performs well even when the true data generating process is not included among the candidate models. See  [YaoVehtari2018]  for details. See also:  AbstractModelWeightsMethod ,  compare Examples Compute  Stacking  weights for two models: julia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> elpd_results = map(models) do idata\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\n\njulia> model_weights(elpd_results; method=Stacking()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 5.34175e-19\n  :non_centered => 1.0 Now we compute  BootstrappedPseudoBMA  weights for the same models: julia> model_weights(elpd_results; method=BootstrappedPseudoBMA()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 0.483723\n  :non_centered => 0.516277 source The following model weighting methods are available"},{"id":250,"pagetitle":"API","title":"PosteriorStats.AbstractModelWeightsMethod","ref":"/PosteriorStats/stable/api/#PosteriorStats.AbstractModelWeightsMethod","content":" PosteriorStats.AbstractModelWeightsMethod  —  Type abstract type AbstractModelWeightsMethod An abstract type representing methods for computing model weights. Subtypes implement  model_weights (method, elpd_results) . source"},{"id":251,"pagetitle":"API","title":"PosteriorStats.BootstrappedPseudoBMA","ref":"/PosteriorStats/stable/api/#PosteriorStats.BootstrappedPseudoBMA","content":" PosteriorStats.BootstrappedPseudoBMA  —  Type struct BootstrappedPseudoBMA{R<:Random.AbstractRNG, T<:Real} <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging using Akaike-type weighting with the Bayesian bootstrap (pseudo-BMA+) [YaoVehtari2018] . The Bayesian bootstrap stabilizes the model weights. BootstrappedPseudoBMA(; rng=Random.default_rng(), samples=1_000, alpha=1)\nBootstrappedPseudoBMA(rng, samples, alpha) Construct the method. rng::Random.AbstractRNG : The random number generator to use for the Bayesian bootstrap samples::Int64 : The number of samples to draw for bootstrapping alpha::Real : The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. The default (1) corresponds to a uniform distribution on the simplex. See also:  Stacking source"},{"id":252,"pagetitle":"API","title":"PosteriorStats.PseudoBMA","ref":"/PosteriorStats/stable/api/#PosteriorStats.PseudoBMA","content":" PosteriorStats.PseudoBMA  —  Type struct PseudoBMA <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging (pseudo-BMA) and Akaike-type weighting. PseudoBMA(; regularize=false)\nPseudoBMA(regularize) Construct the method with optional regularization of the weights using the standard error of the ELPD estimate. Note This approach is not recommended, as it produces unstable weight estimates. It is recommended to instead use  BootstrappedPseudoBMA  to stabilize the weights or  Stacking . For details, see  [YaoVehtari2018] . See also:  Stacking source"},{"id":253,"pagetitle":"API","title":"PosteriorStats.Stacking","ref":"/PosteriorStats/stable/api/#PosteriorStats.Stacking","content":" PosteriorStats.Stacking  —  Type struct Stacking{O<:Optim.AbstractOptimizer} <: AbstractModelWeightsMethod Model weighting using stacking of predictive distributions [YaoVehtari2018] . Stacking(; optimizer=Optim.LBFGS(), options=Optim.Options()\nStacking(optimizer[, options]) Construct the method, optionally customizing the optimization. optimizer::Optim.AbstractOptimizer : The optimizer to use for the optimization of the weights. The optimizer must support projected gradient optimization via a  manifold  field. options::Optim.Options : The Optim options to use for the optimization of the weights. See also:  BootstrappedPseudoBMA source"},{"id":254,"pagetitle":"API","title":"Predictive checks","ref":"/PosteriorStats/stable/api/#Predictive-checks","content":" Predictive checks"},{"id":255,"pagetitle":"API","title":"PosteriorStats.loo_pit","ref":"/PosteriorStats/stable/api/#PosteriorStats.loo_pit","content":" PosteriorStats.loo_pit  —  Function loo_pit(y, y_pred, log_weights; kwargs...) -> Union{Real,AbstractArray} Compute leave-one-out probability integral transform (LOO-PIT) checks. Arguments y : array of observations with shape  (params...,) y_pred : array of posterior predictive samples with shape  (draws, chains, params...) . log_weights : array of normalized log LOO importance weights with shape  (draws, chains, params...) . Keywords is_discrete : If not provided, then it is set to  true  iff elements of  y  and  y_pred  are all integer-valued. If  true , then data are smoothed using  smooth_data  to make them non-discrete before estimating LOO-PIT values. kwargs : Remaining keywords are forwarded to  smooth_data  if data is discrete. Returns pitvals : LOO-PIT values with same size as  y . If  y  is a scalar, then  pitvals  is a scalar. LOO-PIT is a marginal posterior predictive check. If  $y_{-i}$  is the array  $y$  of observations with the  $i$ th observation left out, and  $y_i^*$  is a posterior prediction of the  $i$ th observation, then the LOO-PIT value for the  $i$ th observation is defined as \\[P(y_i^* \\le y_i \\mid y_{-i}) = \\int_{-\\infty}^{y_i} p(y_i^* \\mid y_{-i}) \\mathrm{d} y_i^*\\] The LOO posterior predictions and the corresponding observations should have similar distributions, so if conditional predictive distributions are well-calibrated, then all LOO-PIT values should be approximately uniformly distributed on  $[0, 1]$ . [Gabry2019] Examples Calculate LOO-PIT values using as test quantity the observed values themselves. julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> y = idata.observed_data.obs;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.obs, (:draw, :chain, :school));\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> log_weights = loo(log_like).psis_result.log_weights;\n\njulia> loo_pit(y, y_pred, log_weights)\n8-element DimArray{Float64,1} with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 Calculate LOO-PIT values using as test quantity the square of the difference between each observation and  mu . julia> using Statistics\n\njulia> mu = idata.posterior.mu;\n\njulia> T = y .- median(mu);\n\njulia> T_pred = y_pred .- mu;\n\njulia> loo_pit(T .^ 2, T_pred .^ 2, log_weights)\n8-element DimArray{Float64,1} with dimensions:\n  Dim{:school} Categorical{String} String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n \"Choate\"            0.873577\n \"Deerfield\"         0.243686\n \"Phillips Andover\"  0.357563\n \"Phillips Exeter\"   0.149908\n \"Hotchkiss\"         0.435094\n \"Lawrenceville\"     0.220627\n \"St. Paul's\"        0.775086\n \"Mt. Hermon\"        0.296706 source"},{"id":256,"pagetitle":"API","title":"Utilities","ref":"/PosteriorStats/stable/api/#Utilities","content":" Utilities"},{"id":257,"pagetitle":"API","title":"PosteriorStats.smooth_data","ref":"/PosteriorStats/stable/api/#PosteriorStats.smooth_data","content":" PosteriorStats.smooth_data  —  Function smooth_data(y; dims=:, interp_method=CubicSpline, offset_frac=0.01) Smooth  y  along  dims  using  interp_method . interp_method  is a 2-argument callabale that takes the arguments  y  and  x  and returns a DataInterpolations.jl interpolation method, defaulting to a cubic spline interpolator. offset_frac  is the fraction of the length of  y  to use as an offset when interpolating. source Hyndman1996 Rob J. Hyndman (1996) Computing and Graphing Highest Density Regions,             Amer. Stat., 50(2): 120-6.             DOI:  10.1080/00031305.1996.10474359 jstor . ChenShao1999 Ming-Hui Chen & Qi-Man Shao (1999)              Monte Carlo Estimation of Bayesian Credible and HPD Intervals,              J Comput. Graph. Stat., 8:1, 69-92.              DOI:  10.1080/10618600.1999.10474802 jstor . GelmanGoodrich2019 Andrew Gelman, Ben Goodrich, Jonah Gabry & Aki Vehtari (2019) R-squared for Bayesian Regression Models, The American Statistician, 73:3, 307-9, DOI:  10.1080/00031305.2018.1549100 . Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html Watanabe2010 Watanabe, S. Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. 11(116):3571−3594, 2010. https://jmlr.csail.mit.edu/papers/v11/watanabe10a.html Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 Gabry2019 Gabry, J., Simpson, D., Vehtari, A., Betancourt, M. & Gelman, A. Visualization in Bayesian Workflow. J. R. Stat. Soc. Ser. A Stat. Soc. 182, 389–402 (2019). doi:  10.1111/rssa.12378  arXiv:  1709.01449"},{"id":262,"pagetitle":"Home","title":"PSIS","ref":"/PSIS/stable/#PSIS","content":" PSIS PSIS.jl implements the Pareto smoothed importance sampling (PSIS) algorithm from  [VehtariSimpson2021] . Given a set of importance weights used in some estimator, PSIS both improves the reliability of the estimates by smoothing the importance weights and acts as a diagnostic of the reliability of the estimates. See  psis  for details."},{"id":263,"pagetitle":"Home","title":"Example","ref":"/PSIS/stable/#Example","content":" Example In this example, we use PSIS to smooth log importance ratios for importance sampling 30 isotropic Student  $t$ -distributed parameters using standard normal distributions as proposals. using PSIS, Distributions\nproposal = Normal()\ntarget = TDist(7)\nndraws, nchains, nparams = (1_000, 1, 30)\nx = rand(proposal, ndraws, nchains, nparams)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) ┌ Warning: 8 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/work/PSIS.jl/PSIS.jl/src/core.jl:323\n┌ Warning: 1 parameters had Pareto shape values k > 1. Corresponding importance sampling estimates are likely to be unstable and are unlikely to converge with additional samples.\n└ @ PSIS ~/work/PSIS.jl/PSIS.jl/src/core.jl:326 PSISResult with 1000 draws, 1 chains, and 30 parameters\nPareto shape (k) diagnostic values:\n                         Count       Min. ESS \n (-Inf, 0.5]  good       7 (23.3%)  959\n  (0.5, 0.7]  okay       14 (46.7%)   927\n    (0.7, 1]  bad         8 (26.7%)   ——\n    (1, Inf)  very bad    1 (3.3%)    —— As indicated by the warnings, this is a poor choice of a proposal distribution, and estimates are unlikely to converge (see  PSISResult  for an explanation of the shape thresholds). When running PSIS with many parameters, it is useful to plot the Pareto shape values to diagnose convergence. See  Plotting PSIS results  for examples. VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO]"},{"id":266,"pagetitle":"API","title":"API","ref":"/PSIS/stable/api/#API","content":" API"},{"id":267,"pagetitle":"API","title":"Core functionality","ref":"/PSIS/stable/api/#Core-functionality","content":" Core functionality"},{"id":268,"pagetitle":"API","title":"PSIS.PSISResult","ref":"/PSIS/stable/api/#PSIS.PSISResult","content":" PSIS.PSISResult  —  Type PSISResult Result of Pareto-smoothed importance sampling (PSIS) using  psis . Properties log_weights : un-normalized Pareto-smoothed log weights weights : normalized Pareto-smoothed weights (allocates a copy) pareto_shape : Pareto  $k=ξ$  shape parameter nparams : number of parameters in  log_weights ndraws : number of draws in  log_weights nchains : number of chains in  log_weights reff : the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size. ess : estimated effective sample size of estimate of mean using smoothed importance samples (see  ess_is ) tail_length : length of the upper tail of  log_weights  that was smoothed tail_dist : the generalized Pareto distribution that was fit to the tail of  log_weights . Note that the tail weights are scaled to have a maximum of 1, so  tail_dist * exp(maximum(log_ratios))  is the corresponding fit directly to the tail of  log_ratios . normalized::Bool :indicates whether  log_weights  are log-normalized along the sample dimensions. Diagnostic The  pareto_shape  parameter  $k=ξ$  of the generalized Pareto distribution  tail_dist  can be used to diagnose reliability and convergence of estimates using the importance weights  [VehtariSimpson2021] . if  $k < \\frac{1}{3}$ , importance sampling is stable, and importance sampling (IS) and PSIS both are reliable. if  $k ≤ \\frac{1}{2}$ , then the importance ratio distributon has finite variance, and the central limit theorem holds. As  $k$  approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE. if  $\\frac{1}{2} < k ≤ 0.7$ , then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime. if  $0.7 < k ≤ 1$ , then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended. if  $k > 1$ , then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes. See  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":269,"pagetitle":"API","title":"PSIS.psis","ref":"/PSIS/stable/api/#PSIS.psis","content":" PSIS.psis  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":270,"pagetitle":"API","title":"PSIS.ess_is","ref":"/PSIS/stable/api/#PSIS.ess_is","content":" PSIS.ess_is  —  Function ess_is(weights; reff=1) Estimate effective sample size (ESS) for importance sampling over the sample dimensions. Given normalized weights  $w_{1:n}$ , the ESS is estimated using the L2-norm of the weights: \\[\\mathrm{ESS}(w_{1:n}) = \\frac{r_{\\mathrm{eff}}}{\\sum_{i=1}^n w_i^2}\\] where  $r_{\\mathrm{eff}}$  is the relative efficiency of the  log_weights . ess_is(result::PSISResult; bad_shape_nan=true) Estimate ESS for Pareto-smoothed importance sampling. Note ESS estimates for Pareto shape values  $k > 0.7$ , which are unreliable and misleadingly high, are set to  NaN . To avoid this, set  bad_shape_nan=false . source"},{"id":271,"pagetitle":"API","title":"Plotting","ref":"/PSIS/stable/api/#Plotting","content":" Plotting"},{"id":272,"pagetitle":"API","title":"PSIS.PSISPlots","ref":"/PSIS/stable/api/#PSIS.PSISPlots","content":" PSIS.PSISPlots  —  Module A module defining  paretoshapeplot  for plotting Pareto shape values with Plots.jl source"},{"id":273,"pagetitle":"API","title":"PSIS.PSISPlots.paretoshapeplot","ref":"/PSIS/stable/api/#PSIS.PSISPlots.paretoshapeplot","content":" PSIS.PSISPlots.paretoshapeplot  —  Function paretoshapeplot(values; showlines=false, ...)\nparetoshapeplot!(values; showlines=false, kwargs...) Plot shape parameters of fitted Pareto tail distributions for diagnosing convergence. values  may be either a vector of Pareto shape parameters or a  PSIS.PSISResult . If  showlines==true , horizontal lines indicating relevant Pareto shape thresholds are drawn. See  PSIS.PSISResult  for an explanation of the thresholds. All remaining  kwargs  are forwarded to the plotting function. See  psis ,  PSISResult . Examples using PSIS, Distributions, Plots\nproposal = Normal()\ntarget = TDist(7)\nx = rand(proposal, 1_000, 100)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios)\nparetoshapeplot(result) We can also plot the Pareto shape parameters directly: paretoshapeplot(result.pareto_shape) We can also use  plot  directly: plot(result.pareto_shape; showlines=true) source VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO]"},{"id":276,"pagetitle":"Internal","title":"Internal","ref":"/PSIS/stable/internal/#Internal","content":" Internal"},{"id":277,"pagetitle":"Internal","title":"PSIS.GeneralizedPareto","ref":"/PSIS/stable/internal/#PSIS.GeneralizedPareto","content":" PSIS.GeneralizedPareto  —  Type GeneralizedPareto{T<:Real} The generalized Pareto distribution. This is equivalent to  Distributions.GeneralizedPareto  and can be converted to one with  convert(Distributions.GeneralizedPareto, d) . Constructor GeneralizedPareto(μ, σ, k) Construct the generalized Pareto distribution (GPD) with location parameter  $μ$ , scale parameter  $σ$  and shape parameter  $k$ . Note The shape parameter  $k$  is equivalent to the commonly used shape parameter  $ξ$ . This is the same parameterization used by  [VehtariSimpson2021]  and is related to that used by  [ZhangStephens2009]  as  $k \\mapsto -k$ . source"},{"id":278,"pagetitle":"Internal","title":"PSIS.fit_gpd","ref":"/PSIS/stable/internal/#PSIS.fit_gpd-Tuple{AbstractArray}","content":" PSIS.fit_gpd  —  Method fit_gpd(x; μ=0, kwargs...) Fit a  GeneralizedPareto  with location  μ  to the data  x . The fit is performed using the Empirical Bayes method of  [ZhangStephens2009] . Keywords prior_adjusted::Bool=true , If  true , a weakly informative Normal prior centered on  $\\frac{1}{2}$  is used for the shape  $k$ . sorted::Bool=issorted(x) : If  true ,  x  is assumed to be sorted. If  false , a sorted copy of  x  is made. min_points::Int=30 : The minimum number of quadrature points to use when estimating the posterior mean of  $\\theta = \\frac{\\xi}{\\sigma}$ . source ZhangStephens2009 Jin Zhang & Michael A. Stephens (2009) A New and Efficient Estimation Method for the Generalized Pareto Distribution, Technometrics, 51:3, 316-325, DOI:  10.1198/tech.2009.08017"},{"id":281,"pagetitle":"Plotting","title":"Plotting PSIS results","ref":"/PSIS/stable/plotting/#Plotting-PSIS-results","content":" Plotting PSIS results PSIS.jl includes plotting recipes for  PSISResult  using any Plots.jl backend, as well as the utility plotting function  PSISPlots.paretoshapeplot . We demonstrate this with a simple example. using PSIS, Distributions\nproposal = Normal()\ntarget = TDist(7)\nndraws, nchains, nparams = (1_000, 1, 20)\nx = rand(proposal, ndraws, nchains, nparams)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) PSISResult with 1000 draws, 1 chains, and 20 parameters\nPareto shape (k) diagnostic values:\n                     Count       Min. ESS \n (-Inf, 0.5]  good   4 (20.0%)  959\n  (0.5, 0.7]  okay   10 (50.0%)   927\n    (0.7, 1]  bad     6 (30.0%)   ——"},{"id":282,"pagetitle":"Plotting","title":"Plots.jl","ref":"/PSIS/stable/plotting/#Plots.jl","content":" Plots.jl PSISResult  objects can be plotted directly: using Plots\nplot(result; showlines=true, marker=:+, legend=false, linewidth=2) This is equivalent to calling  PSISPlots.paretoshapeplot(result; kwargs...) ."},{"id":285,"pagetitle":"Home","title":"MCMCDiagnosticTools","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools","content":" MCMCDiagnosticTools MCMCDiagnosticTools provides functionality for diagnosing samples generated using Markov Chain Monte Carlo."},{"id":286,"pagetitle":"Home","title":"Background","ref":"/MCMCDiagnosticTools/stable/#Background","content":" Background Some methods were originally part of  Mamba.jl  and then  MCMCChains.jl . This package is a joint collaboration between the  Turing  and  ArviZ  projects."},{"id":287,"pagetitle":"Home","title":"Effective sample size and  $\\widehat{R}$","ref":"/MCMCDiagnosticTools/stable/#Effective-sample-size-and-\\\\widehat{R}","content":" Effective sample size and  $\\widehat{R}$"},{"id":288,"pagetitle":"Home","title":"MCMCDiagnosticTools.ess","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.ess","content":" MCMCDiagnosticTools.ess  —  Function ess(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind=:bulk,\n    relative::Bool=false,\n    autocov_method=AutocovMethod(),\n    split_chains::Int=2,\n    maxlag::Int=250,\n    kwargs...\n) Estimate the effective sample size (ESS) of the  samples  of shape  (draws, [chains[, parameters...]])  with the  autocov_method . Optionally, the  kind  of ESS estimate to be computed can be specified (see below). Some  kind s accept additional  kwargs . If  relative  is  true , the relative ESS is returned, i.e.  ess / (draws * chains) . split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. There must be at least 3 draws in each chain after splitting. maxlag  indicates the maximum lag for which autocovariance is computed and must be greater than 0. For a given estimand, it is recommended that the ESS is at least  100 * chains  and that  $\\widehat{R} < 1.01$ . [VehtariGelman2021] See also:  AutocovMethod ,  FFTAutocovMethod ,  BDAAutocovMethod ,  rhat ,  ess_rhat ,  mcse Kinds of ESS estimates If  kind  isa a  Symbol , it may take one of the following values: :bulk : basic ESS computed on rank-normalized draws. This kind diagnoses poor convergence   in the bulk of the distribution due to trends or different locations of the chains. :tail : minimum of the quantile-ESS for the symmetric quantiles where    tail_prob=0.1  is the probability in the tails. This kind diagnoses poor convergence in   the tails of the distribution. If this kind is chosen,  kwargs  may contain a    tail_prob  keyword. :basic : basic ESS, equivalent to specifying  kind=Statistics.mean . Note While Bulk-ESS is conceptually related to basic ESS, it is well-defined even if the chains do not have finite variance. [VehtariGelman2021]  For each parameter, rank-normalization proceeds by first ranking the inputs using \"tied ranking\" and then transforming the ranks to normal quantiles so that the result is standard normally distributed. This transform is monotonic. Otherwise,  kind  specifies one of the following estimators, whose ESS is to be estimated: Statistics.mean Statistics.median Statistics.std StatsBase.mad Base.Fix2(Statistics.quantile, p::Real) source"},{"id":289,"pagetitle":"Home","title":"MCMCDiagnosticTools.rhat","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rhat","content":" MCMCDiagnosticTools.rhat  —  Function rhat(samples::AbstractArray{Union{Real,Missing}}; kind::Symbol=:rank, split_chains=2) Compute the  $\\widehat{R}$  diagnostics for each parameter in  samples  of shape  (draws, [chains[, parameters...]]) . [VehtariGelman2021] kind  indicates the kind of  $\\widehat{R}$  to compute (see below). split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. See also  ess ,  ess_rhat ,  rstar Kinds of  $\\widehat{R}$ The following  kind s are supported: :rank : maximum of  $\\widehat{R}$  with  kind=:bulk  and  kind=:tail . :bulk : basic  $\\widehat{R}$  computed on rank-normalized draws. This kind diagnoses   poor convergence in the bulk of the distribution due to trends or different locations of   the chains. :tail :  $\\widehat{R}$  computed on draws folded around the median and then   rank-normalized. This kind diagnoses poor convergence in the tails of the distribution   due to different scales of the chains. :basic : Classic  $\\widehat{R}$ . source"},{"id":290,"pagetitle":"Home","title":"MCMCDiagnosticTools.ess_rhat","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.ess_rhat","content":" MCMCDiagnosticTools.ess_rhat  —  Function ess_rhat(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind::Symbol=:rank,\n    kwargs...,\n) -> NamedTuple{(:ess, :rhat)} Estimate the effective sample size and  $\\widehat{R}$  of the  samples  of shape  (draws, [chains[, parameters...]]) . When both ESS and  $\\widehat{R}$  are needed, this method is often more efficient than calling  ess  and  rhat  separately. See  rhat  for a description of supported  kind s and  ess  for a description of  kwargs . source The following  autocov_method s are supported:"},{"id":291,"pagetitle":"Home","title":"MCMCDiagnosticTools.AutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.AutocovMethod","content":" MCMCDiagnosticTools.AutocovMethod  —  Type AutocovMethod <: AbstractAutocovMethod The  AutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021]  and uses the biased estimator of the autocovariance, as discussed by  [Geyer1992] . source"},{"id":292,"pagetitle":"Home","title":"MCMCDiagnosticTools.FFTAutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.FFTAutocovMethod","content":" MCMCDiagnosticTools.FFTAutocovMethod  —  Type FFTAutocovMethod <: AbstractAutocovMethod The  FFTAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. The algorithm is the same as the one of  AutocovMethod  but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation. Info To be able to use this method, you have to load a package that implements the  AbstractFFTs.jl  interface such as  FFTW.jl  or  FastTransforms.jl . source"},{"id":293,"pagetitle":"Home","title":"MCMCDiagnosticTools.BDAAutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.BDAAutocovMethod","content":" MCMCDiagnosticTools.BDAAutocovMethod  —  Type BDAAutocovMethod <: AbstractAutocovMethod The  BDAAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021] . and uses the variogram estimator of the autocorrelation function discussed by  [BDA3] . source"},{"id":294,"pagetitle":"Home","title":"Monte Carlo standard error","ref":"/MCMCDiagnosticTools/stable/#Monte-Carlo-standard-error","content":" Monte Carlo standard error"},{"id":295,"pagetitle":"Home","title":"MCMCDiagnosticTools.mcse","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.mcse","content":" MCMCDiagnosticTools.mcse  —  Function mcse(samples::AbstractArray{<:Union{Missing,Real}}; kind=Statistics.mean, kwargs...) Estimate the Monte Carlo standard errors (MCSE) of the estimator  kind  applied to  samples  of shape  (draws, [chains[, parameters...]]) . See also:  ess Kinds of MCSE estimates The estimator whose MCSE should be estimated is specified with  kind .  kind  must accept a vector of the same  eltype  as  samples  and return a real estimate. For the following estimators, the effective sample size  ess  and an estimate of the asymptotic variance are used to compute the MCSE, and  kwargs  are forwarded to  ess : Statistics.mean Statistics.median Statistics.std Base.Fix2(Statistics.quantile, p::Real) For other estimators, the subsampling bootstrap method (SBM) [FlegalJones2011] [Flegal2012]  is used as a fallback, and the only accepted  kwargs  are  batch_size , which indicates the size of the overlapping batches used to estimate the MCSE, defaulting to  floor(Int, sqrt(draws * chains)) . Note that SBM tends to underestimate the MCSE, especially for highly autocorrelated chains. One should verify that autocorrelation is low by checking the bulk- and tail-ESS values. source"},{"id":296,"pagetitle":"Home","title":"R⋆ diagnostic","ref":"/MCMCDiagnosticTools/stable/#R-diagnostic","content":" R⋆ diagnostic"},{"id":297,"pagetitle":"Home","title":"MCMCDiagnosticTools.rstar","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rstar","content":" MCMCDiagnosticTools.rstar  —  Function rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples,\n    chain_indices::AbstractVector{Int};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the table  samples  with the  classifier . samples  must be either an  AbstractMatrix , an  AbstractVector , or a table (i.e. implements the Tables.jl interface) whose rows are draws and whose columns are parameters. chain_indices  indicates the chain ids of each row of  samples . This method supports ragged chains, i.e. chains of nonequal lengths. source rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples::AbstractArray{<:Real};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the  samples  with the  classifier . samples  is an array of draws with the shape  (draws, [chains[, parameters...]]) .` This implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari. The  classifier  has to be a supervised classifier of the MLJ framework (see the  MLJ documentation  for a list of supported models). It is trained with a  subset  of the samples from each chain. Each chain is split into  split_chains  separate chains to additionally check for within-chain convergence. The training of the classifier can be inspected by adjusting the  verbosity  level. If the classifier is deterministic, i.e., if it predicts a class, the value of the  $R^*$  statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the  $R^*$  statistic is returned (algorithm 2). Note The correctness of the statistic depends on the convergence of the  classifier  used internally in the statistic. Examples julia> using MLJBase, MLJIteration, EvoTrees, Statistics, StatisticalMeasures\n\njulia> samples = fill(4.0, 100, 3, 2); One can compute the distribution of the  $R^*$  statistic (algorithm 2) with a probabilistic classifier. For instance, we can use a gradient-boosted trees model with  nrounds = 100  sequentially stacked trees and learning rate  eta = 0.05 : julia> model = EvoTreeClassifier(; nrounds=100, eta=0.05);\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 Note, however, that it is recommended to determine  nrounds  based on early-stopping. With the MLJ framework, this can be achieved in the following way (see the  MLJ documentation  for additional explanations): julia> model = IteratedModel(;\n           model=EvoTreeClassifier(; eta=0.05),\n           iteration_parameter=:nrounds,\n           resampling=Holdout(),\n           measures=log_loss,\n           controls=[Step(5), Patience(2), NumberLimit(100)],\n           retrain=true,\n       );\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 For deterministic classifiers, a single  $R^*$  statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models. julia> evotree_deterministic = Pipeline(model; operation=predict_mode);\n\njulia> value = rstar(evotree_deterministic, samples);\n\njulia> round(value; digits=2)\n1.0 References Lambert, B., & Vehtari, A. (2020).  $R^*$ : A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers. source"},{"id":298,"pagetitle":"Home","title":"Bayesian fraction of missing information","ref":"/MCMCDiagnosticTools/stable/#Bayesian-fraction-of-missing-information","content":" Bayesian fraction of missing information"},{"id":299,"pagetitle":"Home","title":"MCMCDiagnosticTools.bfmi","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.bfmi","content":" MCMCDiagnosticTools.bfmi  —  Function bfmi(energy::AbstractVector{<:Real}) -> Real\nbfmi(energy::AbstractMatrix{<:Real}; dims::Int=1) -> AbstractVector{<:Real} Calculate the estimated Bayesian fraction of missing information (BFMI). When sampling with Hamiltonian Monte Carlo (HMC), BFMI quantifies how well momentum resampling matches the marginal energy distribution. The current advice is that values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may change. A BFMI value below the threshold often indicates poor adaptation of sampling parameters or that the target distribution has heavy tails that were not well explored by the Markov chain. For more information, see Section 6.1 of  [Betancourt2018]  or  [Betancourt2016]  for a complete account. energy  is either a vector of Hamiltonian energies of draws or a matrix of energies of draws for multiple chains.  dims  indicates the dimension in  energy  that contains the draws. The default  dims=1  assumes  energy  has the shape  draws  or  (draws, chains) . If a different shape is provided,  dims  must be set accordingly. If  energy  is a vector, a single BFMI value is returned. Otherwise, a vector of BFMI values for each chain is returned. source"},{"id":300,"pagetitle":"Home","title":"Other diagnostics","ref":"/MCMCDiagnosticTools/stable/#Other-diagnostics","content":" Other diagnostics Note These diagnostics are older and less widely used."},{"id":301,"pagetitle":"Home","title":"MCMCDiagnosticTools.discretediag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.discretediag","content":" MCMCDiagnosticTools.discretediag  —  Function discretediag(samples::AbstractArray{<:Real,3}; frac=0.3, method=:weiss, nsim=1_000) Compute discrete diagnostic on  samples  with shape  (draws, chains, parameters) . method  can be one of  :weiss ,  :hangartner ,  :DARBOOT ,  :MCBOOT ,  :billinsgley , and  :billingsleyBOOT . References Benjamin E. Deonovic, & Brian J. Smith. (2017). Convergence diagnostics for MCMC draws of a categorical variable. source"},{"id":302,"pagetitle":"Home","title":"MCMCDiagnosticTools.gelmandiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gelmandiag","content":" MCMCDiagnosticTools.gelmandiag  —  Function gelmandiag(samples::AbstractArray{<:Real,3}; alpha::Real=0.95) Compute the Gelman, Rubin and Brooks diagnostics  [Gelman1992] [Brooks1998]  on  samples  with shape  (draws, chains, parameters) .  Values of the diagnostic’s potential scale reduction factor (PSRF) that are close to one suggest convergence.  As a rule-of-thumb, convergence is rejected if the 97.5 percentile of a PSRF is greater than 1.2. source"},{"id":303,"pagetitle":"Home","title":"MCMCDiagnosticTools.gelmandiag_multivariate","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gelmandiag_multivariate","content":" MCMCDiagnosticTools.gelmandiag_multivariate  —  Function gelmandiag_multivariate(samples::AbstractArray{<:Real,3}; alpha::Real=0.05) Compute the multivariate Gelman, Rubin and Brooks diagnostics on  samples  with shape  (draws, chains, parameters) . source"},{"id":304,"pagetitle":"Home","title":"MCMCDiagnosticTools.gewekediag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gewekediag","content":" MCMCDiagnosticTools.gewekediag  —  Function gewekediag(x::AbstractVector{<:Real}; first::Real=0.1, last::Real=0.5, kwargs...) Compute the Geweke diagnostic  [Geweke1991]  from the  first  and  last  proportion of samples  x . The diagnostic is designed to asses convergence of posterior means estimated with autocorrelated samples.  It computes a normal-based test statistic comparing the sample means in two windows containing proportions of the first and last iterations.  Users should ensure that there is sufficient separation between the two windows to assume that their samples are independent.  A non-significant test p-value indicates convergence.  Significant p-values indicate non-convergence and the possible need to discard initial samples as a burn-in sequence or to simulate additional samples. kwargs  are forwarded to  mcse . source"},{"id":305,"pagetitle":"Home","title":"MCMCDiagnosticTools.heideldiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.heideldiag","content":" MCMCDiagnosticTools.heideldiag  —  Function heideldiag(\n    x::AbstractVector{<:Real}; alpha::Real=0.05, eps::Real=0.1, start::Int=1, kwargs...\n) Compute the Heidelberger and Welch diagnostic  [Heidelberger1983] . This diagnostic tests for non-convergence (non-stationarity) and whether ratios of estimation interval halfwidths to means are within a target ratio. Stationarity is rejected (0) for significant test p-values. Halfwidth tests are rejected (0) if observed ratios are greater than the target, as is the case for  s2  and  beta[1] . kwargs  are forwarded to  mcse . source"},{"id":306,"pagetitle":"Home","title":"MCMCDiagnosticTools.rafterydiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rafterydiag","content":" MCMCDiagnosticTools.rafterydiag  —  Function rafterydiag(\n    x::AbstractVector{<:Real}; q=0.025, r=0.005, s=0.95, eps=0.001, range=1:length(x)\n) Compute the Raftery and Lewis diagnostic  [Raftery1992] . This diagnostic is used to determine the number of iterations required to estimate a specified quantile  q  within a desired degree of accuracy.  The diagnostic is designed to determine the number of autocorrelated samples required to estimate a specified quantile  $\\theta_q$ , such that  $\\Pr(\\theta \\le \\theta_q) = q$ , within a desired degree of accuracy. In particular, if  $\\hat{\\theta}_q$  is the estimand and  $\\Pr(\\theta \\le \\hat{\\theta}_q) = \\hat{P}_q$  the estimated cumulative probability, then accuracy is specified in terms of  r  and  s , where  $\\Pr(q - r < \\hat{P}_q < q + r) = s$ . Thinning may be employed in the calculation of the diagnostic to satisfy its underlying assumptions. However, users may not want to apply the same (or any) thinning when estimating posterior summary statistics because doing so results in a loss of information. Accordingly, sample sizes estimated by the diagnostic tend to be conservative (too large). Furthermore, the argument  r  specifies the margin of error for estimated cumulative probabilities and  s  the probability for the margin of error.  eps  specifies the tolerance within which the probabilities of transitioning from initial to retained iterations are within the equilibrium probabilities for the chain. This argument determines the number of samples to discard as a burn-in sequence and is typically left at its default value. source VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 Geyer1992 Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483. VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 BDA3 Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press. FlegalJones2011 Flegal JM, Jones GL. (2011) Implementing MCMC: estimating with confidence.                 Handbook of Markov Chain Monte Carlo. pp. 175-97.                  pdf Flegal2012 Flegal JM. (2012) Applicability of subsampling bootstrap methods in Markov chain Monte Carlo.            Monte Carlo and Quasi-Monte Carlo Methods 2010. pp. 363-72.            doi:  10.1007/978-3-642-27440-4_18 Betancourt2018 Betancourt M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo.  arXiv:1701.02434v2  [stat.ME] Betancourt2016 Betancourt M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.  arXiv:1604.00695v1  [stat.ME] Gelman1992 Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical science, 7(4), 457-472. Brooks1998 Brooks, S. P., & Gelman, A. (1998). General methods for monitoring convergence of iterative simulations. Journal of computational and graphical statistics, 7(4), 434-455. Geweke1991 Geweke, J. F. (1991). Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments (No. 148). Federal Reserve Bank of Minneapolis. Heidelberger1983 Heidelberger, P., & Welch, P. D. (1983). Simulation run length control in the presence of an initial transient. Operations Research, 31(6), 1109-1144. Raftery1992 A L Raftery and S Lewis. Bayesian Statistics, chapter How Many Iterations in the Gibbs Sampler? Volume 4. Oxford University Press, New York, 1992."},{"id":309,"pagetitle":"Home","title":"InferenceObjects","ref":"/InferenceObjects/stable/#InferenceObjects","content":" InferenceObjects InferenceObjects.jl is a Julia implementation of the  InferenceData schema  for storing results of Bayesian inference. Its purpose is to serve the following three goals: Usefulness in the analysis of Bayesian inference results. Reproducibility of Bayesian inference analysis. Interoperability between different inference backends and programming languages. The implementation consists primarily of the  InferenceData  and  Dataset  structures. InferenceObjects also provides the function  convert_to_inference_data , which may be overloaded by inference packages to define how various inference outputs can be converted to an  InferenceData . For examples of how  InferenceData  can be used, see the  ArviZ.jl documentation ."},{"id":312,"pagetitle":"Dataset","title":"Dataset","ref":"/InferenceObjects/stable/dataset/#Dataset","content":" Dataset InferenceObjects.Dataset InferenceObjects.convert_to_dataset InferenceObjects.namedtuple_to_dataset"},{"id":313,"pagetitle":"Dataset","title":"Type definition","ref":"/InferenceObjects/stable/dataset/#Type-definition","content":" Type definition"},{"id":314,"pagetitle":"Dataset","title":"InferenceObjects.Dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.Dataset","content":" InferenceObjects.Dataset  —  Type Dataset{L} <: DimensionalData.AbstractDimStack{L} Container of dimensional arrays sharing some dimensions. This type is an  DimensionalData.AbstractDimStack  that implements the same interface as  DimensionalData.DimStack  and has identical usage. When a  Dataset  is passed to Python, it is converted to an  xarray.Dataset  without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an  xarray.Dataset  is passed to Julia, its data must be copied. Constructors Dataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n) In most cases, use  convert_to_dataset  to create a  Dataset  instead of directly using a constructor. source"},{"id":315,"pagetitle":"Dataset","title":"General conversion","ref":"/InferenceObjects/stable/dataset/#General-conversion","content":" General conversion"},{"id":316,"pagetitle":"Dataset","title":"InferenceObjects.convert_to_dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.convert_to_dataset","content":" InferenceObjects.convert_to_dataset  —  Function convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset Convert a supported object to a  Dataset . In most cases, this function calls  convert_to_inference_data  and returns the corresponding  group . source"},{"id":317,"pagetitle":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.namedtuple_to_dataset","content":" InferenceObjects.namedtuple_to_dataset  —  Function namedtuple_to_dataset(data; kwargs...) -> Dataset Convert  NamedTuple  mapping variable names to arrays to a  Dataset . Any non-array values will be converted to a 0-dimensional array. Keywords attrs::AbstractDict{<:AbstractString} : a collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable. library::Union{String,Module} : library used for performing inference. Will be attached to the  attrs  metadata. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. source"},{"id":318,"pagetitle":"Dataset","title":"DimensionalData","ref":"/InferenceObjects/stable/dataset/#DimensionalData","content":" DimensionalData As a  DimensionalData.AbstractDimStack ,  Dataset  also implements the  AbstractDimStack  API and can be used like a  DimStack . See  DimensionalData's documentation  for example usage."},{"id":319,"pagetitle":"Dataset","title":"Tables inteface","ref":"/InferenceObjects/stable/dataset/#Tables-inteface","content":" Tables inteface Dataset  implements the  Tables  interface. This allows  Dataset s to be used as sources for any function that can accept a table. For example, it's straightforward to: write to CSV with CSV.jl flatten to a DataFrame with DataFrames.jl plot with StatsPlots.jl plot with AlgebraOfGraphics.jl"},{"id":322,"pagetitle":"InferenceData","title":"InferenceData","ref":"/InferenceObjects/stable/inference_data/#InferenceData","content":" InferenceData InferenceObjects.InferenceData Base.cat Base.getindex Base.getproperty Base.merge Base.propertynames Base.setindex InferenceObjects.convert_to_inference_data InferenceObjects.from_dict InferenceObjects.from_namedtuple InferenceObjects.from_netcdf InferenceObjects.to_netcdf"},{"id":323,"pagetitle":"InferenceData","title":"Type definition","ref":"/InferenceObjects/stable/inference_data/#Type-definition","content":" Type definition"},{"id":324,"pagetitle":"InferenceData","title":"InferenceObjects.InferenceData","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.InferenceData","content":" InferenceObjects.InferenceData  —  Type InferenceData{group_names,group_types} Container for inference data storage using DimensionalData. This object implements the  InferenceData schema . Internally, groups are stored in a  NamedTuple , which can be accessed using  parent(::InferenceData) . Constructors InferenceData(groups::NamedTuple)\nInferenceData(; groups...) Construct an inference data from either a  NamedTuple  or keyword arguments of groups. Groups must be  Dataset  objects. Instead of directly creating an  InferenceData , use the exported  from_xyz  functions or  convert_to_inference_data . source"},{"id":325,"pagetitle":"InferenceData","title":"Property interface","ref":"/InferenceObjects/stable/inference_data/#Property-interface","content":" Property interface"},{"id":326,"pagetitle":"InferenceData","title":"Base.getproperty","ref":"/InferenceObjects/stable/inference_data/#Base.getproperty","content":" Base.getproperty  —  Function getproperty(data::InferenceData, name::Symbol) -> Dataset Get group with the specified  name . source"},{"id":327,"pagetitle":"InferenceData","title":"Base.propertynames","ref":"/InferenceObjects/stable/inference_data/#Base.propertynames","content":" Base.propertynames  —  Function propertynames(data::InferenceData) -> Tuple{Symbol} Get names of groups source"},{"id":328,"pagetitle":"InferenceData","title":"Indexing interface","ref":"/InferenceObjects/stable/inference_data/#Indexing-interface","content":" Indexing interface"},{"id":329,"pagetitle":"InferenceData","title":"Base.getindex","ref":"/InferenceObjects/stable/inference_data/#Base.getindex","content":" Base.getindex  —  Function Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData Return a new  InferenceData  containing the specified groups sliced to the specified coords. coords  specifies a dimension name mapping to an index, a  DimensionalData.Selector , or an  IntervalSets.AbstractInterval . If one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised. Examples Select data from all groups for just the specified id values. julia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\" Select data from just the posterior, returning a  Dataset  if the indices index more than one element from any of the variables: julia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:19:25.982\" Note that if a single index is provided, the behavior is still to slice so that the dimension is preserved. source"},{"id":330,"pagetitle":"InferenceData","title":"Base.setindex","ref":"/InferenceObjects/stable/inference_data/#Base.setindex","content":" Base.setindex  —  Function Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData Create a new  InferenceData  containing the  group  with the specified  name . If a group with  name  is already in  data , it is replaced. source"},{"id":331,"pagetitle":"InferenceData","title":"Iteration interface","ref":"/InferenceObjects/stable/inference_data/#Iteration-interface","content":" Iteration interface InferenceData  also implements the same iteration interface as its underlying  NamedTuple . That is, iterating over an  InferenceData  iterates over its groups."},{"id":332,"pagetitle":"InferenceData","title":"General conversion","ref":"/InferenceObjects/stable/inference_data/#General-conversion","content":" General conversion"},{"id":333,"pagetitle":"InferenceData","title":"InferenceObjects.convert_to_inference_data","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.convert_to_inference_data","content":" InferenceObjects.convert_to_inference_data  —  Function convert_to_inference_data(obj; group, kwargs...) -> InferenceData Convert a supported object to an  InferenceData  object. If  obj  converts to a single dataset,  group  specifies which dataset in the resulting  InferenceData  that is. See  convert_to_dataset Arguments obj  can be many objects. Basic supported types are: InferenceData : return unchanged Dataset / DimensionalData.AbstractDimStack : add to  InferenceData  as the only group NamedTuple / AbstractDict : create a  Dataset  as the only group AbstractArray{<:Real} : create a  Dataset  as the only group, given an arbitrary name, if the name is not set More specific types may be documented separately. Keywords group::Symbol = :posterior : If  obj  converts to a single dataset, assign the resulting dataset to this group. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. kwargs : remaining keywords forwarded to converter functions source"},{"id":334,"pagetitle":"InferenceData","title":"InferenceObjects.from_dict","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_dict","content":" InferenceObjects.from_dict  —  Function from_dict(posterior::AbstractDict; kwargs...) -> InferenceData Convert a  Dict  to an  InferenceData . Arguments posterior : The data to be converted. Its strings must be  Symbol  or  AbstractString , and its values must be arrays. Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior::Dict=nothing : Draws from the prior prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata = Dict(\n    :x => rand(ndraws, nchains),\n    :y => randn(2, ndraws, nchains),\n    :z => randn(3, 2, ndraws, nchains),\n)\nidata = from_dict(data) source"},{"id":335,"pagetitle":"InferenceData","title":"InferenceObjects.from_namedtuple","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_namedtuple","content":" InferenceObjects.from_namedtuple  —  Function from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData Convert a  NamedTuple  or container of  NamedTuple s to an  InferenceData . If containers are passed, they are flattened into a single  NamedTuple  with array elements whose first dimensions correspond to the dimensions of the containers. Arguments posterior : The data to be converted. It may be of the following types: ::NamedTuple : The keys are the variable names and the values are arrays with dimensions  (ndraws, nchains[, sizes...]) . ::Vector{Vector{<:NamedTuple}} : A vector of length  nchains  whose elements have length  ndraws . Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior=nothing : Draws from the prior. Accepts the same types as  posterior . prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Note If a  NamedTuple  is provided for  observed_data ,  constant_data , or predictions constant data`, any non-array values (e.g. integers) are converted to 0-dimensional arrays. Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata1 = (\n    x=rand(ndraws, nchains), y=randn(ndraws, nchains, 2), z=randn(ndraws, nchains, 3, 2)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata2 = from_namedtuple(data2) source"},{"id":336,"pagetitle":"InferenceData","title":"General functions","ref":"/InferenceObjects/stable/inference_data/#General-functions","content":" General functions"},{"id":337,"pagetitle":"InferenceData","title":"Base.cat","ref":"/InferenceObjects/stable/inference_data/#Base.cat","content":" Base.cat  —  Function cat(data::InferenceData...; [groups=keys(data[1]),] dims) -> InferenceData Concatenate  InferenceData  objects along the specified dimension  dims . Only the groups in  groups  are concatenated. Remaining groups are  merge d into the new  InferenceData  object. Examples Here is how we can concatenate all groups of two  InferenceData  objects along the existing  chain  dimension: julia> coords = (; a_dim=[\"x\", \"y\", \"z\"]);\n\njulia> dims = dims=(; a=[:a_dim]);\n\njulia> data = Dict(:a => randn(100, 4, 3), :b => randn(100, 4));\n\njulia> idata = from_dict(data; coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1 = cat(idata, idata; dims=:chain)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim} (100×8×3)\n  :b Float64 dims: Dim{:draw}, Dim{:chain} (100×8)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\" Alternatively, we can concatenate along a new  run  dimension, which will be created. julia> idata_cat2 = cat(idata, idata; dims=:run)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat2.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\" We can also concatenate only a subset of groups and merge the rest, which is useful when some groups are present only in some of the  InferenceData  objects or will be identical in all of them: julia> observed_data = Dict(:y => randn(10));\n\njulia> idata2 = from_dict(data; observed_data=observed_data, coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3 = cat(idata, idata2; groups=(:posterior,), dims=:run)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3.posterior\nDataset with dimensions:\n  Dim{:draw},\n  Dim{:chain},\n  Dim{:a_dim} Categorical{String} String[\"x\", \"y\", \"z\"] ForwardOrdered,\n  Dim{:run}\nand 2 layers:\n  :a Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:a_dim}, Dim{:run} (100×4×3×2)\n  :b Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:run} (100×4×2)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-04-03T18:41:35.779\"\n\njulia> idata_cat3.observed_data\nDataset with dimensions: Dim{:y_dim_1}\nand 1 layer:\n  :y Float64 dims: Dim{:y_dim_1} (10)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2023-02-17T15:11:00.59\" source"},{"id":338,"pagetitle":"InferenceData","title":"Base.merge","ref":"/InferenceObjects/stable/inference_data/#Base.merge","content":" Base.merge  —  Function merge(data::InferenceData...) -> InferenceData Merge  InferenceData  objects. The result contains all groups in  data  and  others . If a group appears more than once, the one that occurs last is kept. See also:  cat Examples Here we merge an  InferenceData  containing only a posterior group with one containing only a prior group to create a new one containing both groups. julia> idata1 = from_dict(Dict(:a => randn(100, 4, 3), :b => randn(100, 4)))\nInferenceData with groups:\n  > posterior\n\njulia> idata2 = from_dict(; prior=Dict(:a => randn(100, 1, 3), :c => randn(100, 1)))\nInferenceData with groups:\n  > prior\n\njulia> idata_merged = merge(idata1, idata2)\nInferenceData with groups:\n  > posterior\n  > prior source"},{"id":339,"pagetitle":"InferenceData","title":"I/O extensions","ref":"/InferenceObjects/stable/inference_data/#I/O-extensions","content":" I/O extensions The following types of storage are provided via extensions."},{"id":340,"pagetitle":"InferenceData","title":"NetCDF I/O using NCDatasets.jl","ref":"/InferenceObjects/stable/inference_data/#NetCDF-I/O-using-NCDatasets.jl","content":" NetCDF I/O using NCDatasets.jl"},{"id":341,"pagetitle":"InferenceData","title":"InferenceObjects.from_netcdf","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_netcdf","content":" InferenceObjects.from_netcdf  —  Function from_netcdf(path::AbstractString; kwargs...) -> InferenceData Load an  InferenceData  from an unopened NetCDF file. Remaining  kwargs  are passed to  NCDatasets.NCDataset . This method loads data eagerly. To instead load data lazily, pass an opened  NCDataset  to  from_netcdf . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_netcdf(\"centered_eight.nc\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data from_netcdf(ds::NCDatasets.NCDataset; load_mode) -> InferenceData Load an  InferenceData  from an opened NetCDF file. load_mode  defaults to  :lazy , which avoids reading variables into memory. Operations on these arrays will be slow.  load_mode  can also be  :eager , which copies all variables into memory. It is then safe to close  ds . If  load_mode  is  :lazy  and  ds  is closed after constructing  InferenceData , using the variable arrays will have undefined behavior. Examples Here is how we might load an  InferenceData  from an  InferenceData  lazily from a web-hosted NetCDF file. julia> using HTTP, InferenceObjects, NCDatasets\n\njulia> resp = HTTP.get(\"https://github.com/arviz-devs/arviz_example_data/blob/main/data/centered_eight.nc?raw=true\");\n\njulia> ds = NCDataset(\"centered_eight\", \"r\"; memory = resp.body);\n\njulia> idata = from_netcdf(ds)\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\njulia> idata_copy = copy(idata); # disconnect from the loaded dataset\n\njulia> close(ds); source"},{"id":342,"pagetitle":"InferenceData","title":"InferenceObjects.to_netcdf","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.to_netcdf","content":" InferenceObjects.to_netcdf  —  Function to_netcdf(data, dest::AbstractString; group::Symbol=:posterior, kwargs...)\nto_netcdf(data, dest::NCDatasets.NCDataset; group::Symbol=:posterior) Write  data  to a NetCDF file. data  is any type that can be converted to an  InferenceData  using  convert_to_inference_data . If not an  InferenceData , then  group  specifies which group the data represents. dest  specifies either the path to the NetCDF file or an opened NetCDF file. If  dest  is a path, remaining  kwargs  are passed to  NCDatasets.NCDataset . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_namedtuple((; x = randn(4, 100, 3), z = randn(4, 100)))\nInferenceData with groups:\n  > posterior\n\njulia> to_netcdf(idata, \"data.nc\")\n\"data.nc\" source"},{"id":345,"pagetitle":"Home","title":"ArviZExampleData","ref":"/ArviZExampleData/stable/#ArviZExampleData","content":" ArviZExampleData This package provides utilities for loading datasets defined in the  arviz_example_data  repository. The resulting objects are  InferenceObjects.jl 's  InferenceData . These utilities are used in  ArviZ.jl ."},{"id":348,"pagetitle":"API","title":"API","ref":"/ArviZExampleData/stable/api/#API","content":" API"},{"id":349,"pagetitle":"API","title":"ArviZExampleData.describe_example_data","ref":"/ArviZExampleData/stable/api/#ArviZExampleData.describe_example_data","content":" ArviZExampleData.describe_example_data  —  Function describe_example_data() -> String Return a string containing descriptions of all available datasets. Examples julia> describe_example_data(\"radon\") |> println\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472 source"},{"id":350,"pagetitle":"API","title":"ArviZExampleData.load_example_data","ref":"/ArviZExampleData/stable/api/#ArviZExampleData.load_example_data","content":" ArviZExampleData.load_example_data  —  Function load_example_data(name; kwargs...) -> InferenceObjects.InferenceData\nload_example_data() -> Dict{String,AbstractFileMetadata} Load a local or remote pre-made dataset. kwargs  are forwarded to  InferenceObjects.from_netcdf . Pass no parameters to get a  Dict  listing all available datasets. Data files are handled by DataDeps.jl. A file is downloaded only when it is requested and then cached for future use. Examples julia> keys(load_example_data())\nKeySet for a OrderedCollections.OrderedDict{String, ArviZExampleData.AbstractFileMetadata} with 9 entries. Keys:\n  \"centered_eight\"\n  \"non_centered_eight\"\n  \"radon\"\n  \"rugby\"\n  \"regression1d\"\n  \"regression10d\"\n  \"classification1d\"\n  \"classification10d\"\n  \"glycan_torsion_angles\"\n\njulia> load_example_data(\"centered_eight\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > log_likelihood\n  > sample_stats\n  > prior\n  > prior_predictive\n  > observed_data\n  > constant_data source"},{"id":353,"pagetitle":"Datasets","title":"Datasets","ref":"/ArviZExampleData/stable/datasets/#Datasets","content":" Datasets The following shows the currently available example datasets: using ArviZExampleData\n\nprintln(describe_example_data()) centered_eight\n==============\n\nA centered parameterization of the eight schools model. Provided as an example of a model that NUTS has trouble fitting. Compare to `non_centered_eight`.\n\nThe eight schools model is a hierarchical model used for an analysis of the effectiveness of classes that were designed to improve students' performance on the Scholastic Aptitude Test.\n\nSee Bayesian Data Analysis (Gelman et. al.) for more details.\n\nlocal: /home/runner/.julia/artifacts/1a48fb48ab2b35cdeeb84e0dcdcda134e24a1c20/arviz_example_data-0.1.1/data/centered_eight.nc\n\nnon_centered_eight\n==================\n\nA non-centered parameterization of the eight schools model. This is a hierarchical model where sampling problems may be fixed by a non-centered parametrization. Compare to `centered_eight`.\n\nThe eight schools model is a hierarchical model used for an analysis of the effectiveness of classes that were designed to improve students' performance on the Scholastic Aptitude Test.\n\nSee Bayesian Data Analysis (Gelman et. al.) for more details.\n\nlocal: /home/runner/.julia/artifacts/1a48fb48ab2b35cdeeb84e0dcdcda134e24a1c20/arviz_example_data-0.1.1/data/non_centered_eight.nc\n\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472\n\nrugby\n=====\n\nThe Six Nations Championship is a yearly rugby competition between Italy, Ireland, Scotland, England, France and Wales. Fifteen games are played each year, representing all combinations of the six teams.\n\nThis example uses and includes results from 2014 - 2017, comprising 60 total games. It models latent parameters for each team's attack and defense, as well as a parameter for home team advantage.\n\nSee https://docs.pymc.io/notebooks/rugby_analytics.html by Peader Coyle for more details and references.\n\nremote: http://ndownloader.figshare.com/files/16254359\n\nregression1d\n============\n\nA synthetic one dimensional linear regression dataset with latent slope, intercept, and noise (\"eps\"). One hundred data points, fit with PyMC3.\n\nTrue slope and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16254899\n\nregression10d\n=============\n\nA synthetic multi-dimensional (10 dimensions) linear regression dataset with latent weights (\"w\"), intercept, and noise (\"eps\"). Five hundred data points, fit with PyMC3.\n\nTrue weights and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16255736\n\nclassification1d\n================\n\nA synthetic one dimensional logistic regression dataset with latent slope and intercept, passed into a Bernoulli random variable. One hundred data points, fit with PyMC3.\n\nTrue slope and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16256678\n\nclassification10d\n=================\n\nA synthetic multi dimensional (10 dimensions) logistic regression dataset with latent weights (\"w\") and intercept, passed into a Bernoulli random variable. Five hundred data points, fit with PyMC3.\n\nTrue weights and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16256681\n\nglycan_torsion_angles\n=====================\n\nTorsion angles phi and psi are critical for determining the three dimensional structure of bio-molecules. Combinations of phi and psi torsion angles that produce clashes between atoms in the bio-molecule result in high energy, unlikely structures.\n\nThis model uses a Von Mises distribution to propose torsion angles for the structure of a glycan molecule (pdb id: 2LIQ), and a Potential to estimate the proposed structure's energy. Said Potential is bound by Boltzman's law.\n\nremote: http://ndownloader.figshare.com/files/22882652"},{"id":356,"pagetitle":"For developers","title":"For developers","ref":"/ArviZExampleData/stable/for_developers/#For-developers","content":" For developers This package has  arviz_example_data  as a data dependency, which is included as an  artifact . When  arviz_example_data  is updated, and a new release is made,  Artifacts.toml  should be updated to point to the new tarball corresponding to the release: julia> using ArtifactUtils\n\njulia> version = v\"0.1.0\";\n\njulia> tarball_url = \"https://github.com/arviz-devs/arviz_example_data/archive/refs/tags/v$version.tar.gz\";\n\njulia> add_artifact!(\"Artifacts.toml\", \"arviz_example_data\", tarball_url; force=true);"},{"id":361,"pagetitle":"DimensionalData.jl","title":"DimensionalData ¤","ref":"/DimensionalData/stable/#dimensionaldata","content":" DimensionalData ¤ DimensionalData.jl provides tools and abstractions for working with datasets that have named dimensions, and optionally a lookup index. DimensionalData is a pluggable, generalised version of  AxisArrays.jl  with a cleaner syntax, and additional functionality found in NamedDims.jl. It has similar goals to pythons  xarray , and is primarily written for use with spatial data in  Rasters.jl ."},{"id":362,"pagetitle":"DimensionalData.jl","title":"Goals ¤","ref":"/DimensionalData/stable/#goals","content":" Goals ¤ Clean, readable syntax. Minimise required parentheses, minimise of exported Zero-cost dimensional indexing  a[Y(4), X(5)]  of a single value. methods, and instead extend Base methods whenever possible. Plotting is easy: data should plot sensibly and correctly with useful labels, by default. Least surprise: everything works the same as in Base, but with named dims. If a method accepts numeric indices or  dims=X  in base, you should be able to use DimensionalData.jl dims. Minimal interface: implementing a dimension-aware type should be easy. Maximum extensibility: always use method dispatch. Regular types over special syntax. Recursion over @generated. Always dispatch on abstract types. Type stability: dimensional methods should be type stable  more often  than Base methods Functional style: structs are always rebuilt, and other than the array data, fields are not mutated in place."},{"id":363,"pagetitle":"DimensionalData.jl","title":"For package developers ¤","ref":"/DimensionalData/stable/#for-package-developers","content":" For package developers ¤"},{"id":364,"pagetitle":"DimensionalData.jl","title":"Data types and the interface ¤","ref":"/DimensionalData/stable/#data-types-and-the-interface","content":" Data types and the interface ¤ DimensionalData.jl provides the concrete  DimArray  type. But its behaviours are intended to be easily applied to other array types. more The main requirement for extending DimensionalData.jl is to define a  dims  method\nthat returns a  Tuple  of  Dimension  that matches the dimension order\nand axis values of your data. Define  rebuild  and base methods for  similar \nand  parent  if you want the metadata to persist through transformations (see\nthe  DimArray  and  AbstractDimArray  types). A  refdims  method\nreturns the lost dimensions of a previous transformation, passed in to the\n rebuild  method.  refdims  can be discarded, the main loss being plot labels\nand ability to reconstruct dimensions in  cat . Inheriting from  AbstractDimArray  in this way will give nearly all the functionality\nof using  DimArray ."},{"id":365,"pagetitle":"DimensionalData.jl","title":"LookupArrays and Dimensions ¤","ref":"/DimensionalData/stable/#lookuparrays-and-dimensions","content":" LookupArrays and Dimensions ¤ Sub modules  LookupArrays  and  Dimensions  define the behviour of dimensions and their lookup index. LookupArrays  and  Dimensions"},{"id":368,"pagetitle":"Crash course - DimensionalData.jl","title":"Dimensions and DimArrays ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#dimensions-and-dimarrays","content":" Dimensions and DimArrays ¤ The core type of DimensionalData.jl is the  Dimension  and the types that inherit from it, such as  Ti ,  X ,  Y ,  Z , the generic  Dim{:x} , or others that you define manually using the  @dim  macro. Dimension s are primarily used in  DimArray , other  AbstractDimArray . We can use dimensions without a value index - these simply label the axis. A  DimArray  with labelled dimensions is constructed by: using   DimensionalData \n A   =   rand ( X ( 5 ),   Y ( 5 )) \n 5×5 DimArray{Float64,2} with dimensions: X, Y\n 0.639371   0.728187  0.659382  0.276835  0.00472781\n 0.129775   0.754515  0.449496  0.235907  0.508677\n 0.743006   0.74282   0.722859  0.319098  0.948961\n 0.0282573  0.433492  0.498678  0.486587  0.936406\n 0.670863   0.532404  0.566762  0.911621  0.298562\n get value A [ Y ( 1 ),   X ( 2 )] \n 0.12977484906823067\n As shown above,  Dimension s can be used to construct arrays in  rand ,  ones ,  zeros  and  fill  with either a range for a lookup index or a number for the dimension length. Or we can use the  Dim{X}  dims by using  Symbol s, and indexing with keywords: A   =   DimArray ( rand ( 5 ,   5 ),   ( :a ,   :b )) \n 5×5 DimArray{Float64,2} with dimensions: Dim{:a}, Dim{:b}\n 0.882199  0.358436  0.843655  0.388739  0.335111\n 0.941056  0.488275  0.891005  0.156489  0.298791\n 0.516715  0.96201   0.371907  0.566943  0.627032\n 0.505783  0.87138   0.63168   0.138088  0.417325\n 0.479977  0.402996  0.470668  0.433306  0.73448\n get value A [ a = 3 ,   b = 5 ] \n 0.6270316143310354\n Often, we want to provide a lookup index for the dimension: using   Dates \n t   =   DateTime ( 2001 ) : Month ( 1 ) : DateTime ( 2001 , 12 ) \n x   =   10 : 10 : 100 \n A   =   rand ( X ( x ),   Ti ( t )) \n 10×12 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 10:10:100 ForwardOrdered Regular Points,\n  Ti Sampled{DateTime} DateTime(\"2001-01-01T00:00:00\"):Month(1):DateTime(\"2001-12-01T00:00:00\") ForwardOrdered Regular Points\n       2001-01-01T00:00:00  …   2001-12-01T00:00:00\n  10  0.876025                             0.400058\n  20  0.189044                             0.0200001\n  30  0.947597                             0.224468\n  40  0.925587                             0.0946553\n  50  0.374405                          …  0.402854\n  60  0.621711                             0.729083\n  70  0.218432                             0.977081\n  80  0.076455                             0.520698\n  90  0.441223                             0.194285\n 100  0.392635                          …  0.166256\n Here both  X  and  Ti  are dimensions from  DimensionalData . The currently exported dimensions are  X, Y, Z, Ti  ( Ti  is shortening of  Time ). The length of each dimension index has to match the size of the corresponding array axis. This can also be done with  Symbol , using  Dim{X} : A2   =   DimArray ( rand ( 12 ,   10 ),   ( time = t ,   distance = x )) \n 12×10 DimArray{Float64,2} with dimensions: \n  Dim{:time} Sampled{DateTime} DateTime(\"2001-01-01T00:00:00\"):Month(1):DateTime(\"2001-12-01T00:00:00\") ForwardOrdered Regular Points,\n  Dim{:distance} Sampled{Int64} 10:10:100 ForwardOrdered Regular Points\n                                   …  80          90         100\n  2001-01-01T00:00:00      0.587341    0.789156    0.918303\n  2001-02-01T00:00:00      0.526937    0.747982    0.902167\n  2001-03-01T00:00:00      0.361154    0.621076    0.624956\n  2001-04-01T00:00:00      0.763525    0.909315    0.696621\n  2001-05-01T00:00:00  …   0.762242    0.620386    0.400846\n  2001-06-01T00:00:00      0.448785    0.220544    0.952638\n  2001-07-01T00:00:00      0.933273    0.119281    0.892109\n  2001-08-01T00:00:00      0.622369    0.26441     0.172646\n  2001-09-01T00:00:00      0.882309    0.137325    0.460293\n  2001-10-01T00:00:00  …   0.915926    0.209489    0.58446\n  2001-11-01T00:00:00      0.0275966   0.521232    0.881461\n  2001-12-01T00:00:00      0.118496    0.39596     0.128754\n Symbols can be more convenient to use than defining custom dims with  @dim , but have some downsides. They don't inherit from a specific  Dimension  type, so plots will not know what axis to put them on. They also cannot use the basic constructor methods like  rand  or  zeros , as we cannot dispatch on  Symbol  for Base methods without \"type-piracy\"."},{"id":369,"pagetitle":"Crash course - DimensionalData.jl","title":"Indexing the array by name and index ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#indexing-the-array-by-name-and-index","content":" Indexing the array by name and index ¤ Dimensions can be used to index the array by name, without having to worry about the order of the dimensions. The simplest case is to select a dimension by index. Let's say every 2nd point of the  Ti  dimension and every 3rd point of the  X  dimension. This is done with the simple  Ti(range)  syntax like so: A [ X ( 1 : 3 : 11 ),   Ti ( 1 : 2 : 11 )] \n 4×6 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 10:30:100 ForwardOrdered Regular Points,\n  Ti Sampled{DateTime} DateTime(\"2001-01-01T00:00:00\"):Month(2):DateTime(\"2001-11-01T00:00:00\") ForwardOrdered Regular Points\n       2001-01-01T00:00:00  …   2001-11-01T00:00:00\n  10  0.876025                             0.783858\n  40  0.925587                             0.749641\n  70  0.218432                             0.502718\n 100  0.392635                             0.0408583\n When specifying only one dimension, all elements of the other dimensions are assumed to be included: A [ X ( 1 : 3 : 10 )] \n 4×12 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 10:30:100 ForwardOrdered Regular Points,\n  Ti Sampled{DateTime} DateTime(\"2001-01-01T00:00:00\"):Month(1):DateTime(\"2001-12-01T00:00:00\") ForwardOrdered Regular Points\n       2001-01-01T00:00:00  …   2001-12-01T00:00:00\n  10  0.876025                             0.400058\n  40  0.925587                             0.0946553\n  70  0.218432                             0.977081\n 100  0.392635                             0.166256\n Indexing Indexing  AbstractDimArray s works with  getindex ,  setindex!  and    view . The result is still an  AbstracDimArray , unless using all single    Int  or  Selector s that resolve to  Int . Dimension s can be used to construct arrays in  rand ,  ones ,  zeros  and  fill  with either a range for a lookup index or a number for the dimension length. using   DimensionalData \n A1   =   ones ( X ( 1 : 40 ),   Y ( 50 )) \n 40×50 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 1:40 ForwardOrdered Regular Points,\n  Y\n  1  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  2  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  3  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  4  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  5  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  6  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n  ⋮                      ⋮              ⋱  ⋮                        ⋮    \n 35  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 36  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 37  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 38  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 39  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 40  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n We can also use dim wrappers for indexing, so that the dimension order in the underlying array does not need to be known: A1 [ Y ( 1 ),   X ( 1 : 10 )] \n 10-element DimArray{Float64,1} with dimensions: \n  X Sampled{Int64} 1:10 ForwardOrdered Regular Points\nand reference dimensions: Y\n  1  1.0\n  2  1.0\n  3  1.0\n  4  1.0\n  5  1.0\n  6  1.0\n  7  1.0\n  8  1.0\n  9  1.0\n 10  1.0\n"},{"id":370,"pagetitle":"Crash course - DimensionalData.jl","title":"Indexing Performance ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#indexing-performance","content":" Indexing Performance ¤ Indexing with  Dimension  has no runtime cost: A2   =   ones ( X ( 3 ),   Y ( 3 )) \n 3×3 DimArray{Float64,2} with dimensions: X, Y\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n time ? using   BenchmarkTools \n\n println ( @btime   $ A2 [ X ( 1 ),   Y ( 2 )]) \n   43.391 ns (0 allocations: 0 bytes)\n1.0\n and println ( @btime   parent ( $ A2 )[ 1 ,   2 ]) \n   4.700 ns (0 allocations: 0 bytes)\n1.0\n"},{"id":371,"pagetitle":"Crash course - DimensionalData.jl","title":"Specifying  dims  keyword arguments with  Dimension ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#specifying-dims-keyword-arguments-with-dimension","content":" Specifying  dims  keyword arguments with  Dimension ¤ In many Julia functions like  size  or  sum , you can specify the dimension along which to perform the operation as an  Int . It is also possible to do this using  Dimension  types with  AbstractDimArray : A3   =   rand ( X ( 3 ),   Y ( 4 ),   Ti ( 5 )); \n sum ( A3 ;   dims = Ti ) \n 3×4×1 DimArray{Float64,3} with dimensions: X, Y, Ti\n[:, :, 1]\n 2.10074  2.78093  2.09635  3.42244\n 3.83632  3.26228  2.64418  2.27151\n 2.92203  2.87378  3.62777  2.45956\n This also works in methods from  Statistics : using   Statistics \n mean ( A3 ;   dims = Ti ) \n 3×4×1 DimArray{Float64,3} with dimensions: X, Y, Ti\n[:, :, 1]\n 0.420147  0.556187  0.419271  0.684488\n 0.767263  0.652455  0.528836  0.454303\n 0.584406  0.574757  0.725554  0.491912\n"},{"id":372,"pagetitle":"Crash course - DimensionalData.jl","title":"Methods where dims, dim types, or  Symbol s can be used to indicate the array dimension: ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#methods-where-dims-dim-types-or-symbols-can-be-used-to-indicate-the-array-dimension","content":" Methods where dims, dim types, or  Symbol s can be used to indicate the array dimension: ¤ size ,  axes ,  firstindex ,  lastindex cat ,  reverse ,  dropdims reduce ,  mapreduce sum ,  prod ,  maximum ,  minimum , mean ,  median ,  extrema ,  std ,  var ,  cor ,  cov permutedims ,  adjoint ,  transpose ,  Transpose mapslices ,  eachslice"},{"id":373,"pagetitle":"Crash course - DimensionalData.jl","title":"LookupArrays and Selectors ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#lookuparrays-and-selectors","content":" LookupArrays and Selectors ¤ Indexing by value in  DimensionalData  is done with  Selectors . IntervalSets.jl is now used for selecting ranges of values (formerly  Between ). Selector Description At(x) get the index exactly matching the passed in value(s) Near(x) get the closest index to the passed in value(s) Contains(x) get indices where the value x falls within an interval Where(f) filter the array axis by a function of the dimension index values. [ Not(x) ] get all indices  not  selected by  x , which can be another selector. [ a..b ] get all indices between two values, inclusively. [ OpenInterval(a, b) ] get all indices between  a  and  b , exclusively. [ Interval{A,B}(a, b) ] get all indices between  a  and  b , as  :closed  or  :open . Selectors find indices in the  LookupArray , for each dimension. Here we use an  Interval  to select a range between integers and  DateTime : A [ X ( 12 .. 35 ),   Ti ( Date ( 2001 ,   5 ) .. Date ( 2001 ,   7 ))] \n 2×3 DimArray{Float64,2} with dimensions: \n  X Sampled{Int64} 20:10:30 ForwardOrdered Regular Points,\n  Ti Sampled{DateTime} DateTime(\"2001-05-01T00:00:00\"):Month(1):DateTime(\"2001-07-01T00:00:00\") ForwardOrdered Regular Points\n      2001-05-01T00:00:00  …   2001-07-01T00:00:00\n 20  0.783905                             0.448705\n 30  0.055066                             0.696884\n To select intervals in DimArrays (e.g.  A2 ) you need to specify  dimname=a .. b : A2 [ distance = 12   ..   35 ,   time = Date ( 2001 ,   5 )   ..   Date ( 2001 ,   7 )] \n 3×3 DimArray{Float64,2} with dimensions: X, Y\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n Selectors can be used in  getindex ,  setindex!  and  view  to select indices matching the passed in value(s) We can use selectors inside dim wrappers, here selecting values from  DateTime  and  Int : using   Dates \n timespan   =   DateTime ( 2001 , 1 ) : Month ( 1 ) : DateTime ( 2001 , 12 ) \n A4   =   rand ( Ti ( timespan ),   X ( 10 : 10 : 100 )) \n A4 [ X ( Near ( 35 )),   Ti ( At ( DateTime ( 2001 , 5 )))] \n 0.030303009210617127\n Without dim wrappers selectors must be in the right order, and specify all axes: using   Unitful \n A5   =   rand ( Y (( 1 : 10 : 100 ) u \"m\" ),   Ti (( 1 : 5 : 100 ) u \"s\" )); \n A5 [ 10.5 u \"m\"   ..   50.5 u \"m\" ,   Near ( 23 u \"s\" )] \n 4-element DimArray{Float64,1} with dimensions: \n  Y Sampled{Quantity{Int64, 𝐋, Unitful.FreeUnits{(m,), 𝐋, nothing}}} (11:10:41) m ForwardOrdered Regular Points\nand reference dimensions: \n  Ti Sampled{Quantity{Int64, 𝐓, Unitful.FreeUnits{(s,), 𝐓, nothing}}} (21:5:21) s ForwardOrdered Regular Points\n 11 m  0.491441\n 21 m  0.352957\n 31 m  0.500162\n 41 m  0.00797389\n We can also use Linear indices as in standard  Array : A5 [ 1 : 5 ] \n 5-element Vector{Float64}:\n 0.35576854559670334\n 0.09747210569935671\n 0.3121423090340645\n 0.721088681333341\n 0.6940972009616628\n But unless the  DimArray  is one dimensional, this will return a regular  Array . It is not possible to keep the  LookupArray  or even  Dimension s after linear indexing is used."},{"id":374,"pagetitle":"Crash course - DimensionalData.jl","title":"LookupArrays and traits ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#lookuparrays-and-traits","content":" LookupArrays and traits ¤ Using a regular range or  Vector  as a lookup index has a number of downsides. We cannot use  searchsorted  for fast searches without knowing the order of the array, and this is slow to compute at runtime. It also means  reverse  or rotations cannot be used while keeping the  DimArray  wrapper. Step sizes are also a problem. Some ranges like  LinRange  lose their step size with a length of  1 . Often, instead of a range, multi-dimensional data formats provide a  Vector  of evenly spaced values for a lookup, with a step size specified separately. Converting to a range introduces floating point errors that means points may not be selected with  At  without setting tolerances. This means using a lookup wrapper with traits is more generally robust and versatile than simply using a range or vector. DimensionalData provides types for specifying details about the dimension index, in the  LookupArrays  sub-module: using   DimensionalData \n using   . LookupArrays \n The main  LookupArray  are : Sampled Categorical , NoLookup Each comes with specific traits that are either fixed or variable, depending on the contained index. These enable optimisations with  Selector s, and modified behaviours, such as: Selection of  Intervals  or  Points , which will give slightly different results for selectors like  ..  - as whole intervals are   selected, and have different  bounds  values. Tracking of lookup order. A reverse order is labelled  ReverseOrdered  and will still work with  searchsorted , and for plots to always be the right way   up when either the index or the array is backwards. Reversing a  DimArray    will reverse the  LookupArray  for that dimension, swapping  ReverseOrdered    to  ForwardOrdered . Sampled Intervals  can have index located at a  Locus  of: Start , Center End Which specifies the point of the interval represented in the index, to match different data standards, e.g. GeoTIFF ( Start ) and NetCDF ( Center ). A  Span  specifies the gap between  Points  or the size of Intervals . This may be: Regular , in the case of a range and equally spaced vector, Irregular  for unequally spaced vectors Explicit  for the case where all interval start and end points are specified explicitly - as is common in the NetCDF standard. These traits all for subtypes of  Aligned . Unaligned  also exists to handle dimensions with an index that is rotated or otherwise transformed in relation to the underlying array, such as  Transformed ."},{"id":375,"pagetitle":"Crash course - DimensionalData.jl","title":"LookupArray detection ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#lookuparray-detection","content":" LookupArray detection ¤ Aligned  types will be detected automatically if not specified - which usually isn't required. An empty  Dimension  or a  Type  or  Symbol  will be assigned  NoLookup  - this behaves as a simple named dimension without a lookup index. A  Dimension  containing and index of  String ,  Char ,  Symbol  or mixed types will be given the  Categorical  mode, A range will be assigned  Sampled , defaulting to  Regular ,  Points Other  AbstractVector  will be assigned  Sampled Irregular Points . In all cases the  Order  of  ForwardOrdered  or  ReverseOrdered  will be be detected, otherwise  Unordered  for an unsorted  Array . See the  LookupArray  API docs for more detail."},{"id":376,"pagetitle":"Crash course - DimensionalData.jl","title":"Referenced dimensions ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#referenced-dimensions","content":" Referenced dimensions ¤ The reference dimensions record the previous dimensions that an array was selected from. These can be use for plot labelling, and tracking array changes so that  cat  can reconstruct the lookup array from previous dimensions that have been sliced."},{"id":377,"pagetitle":"Crash course - DimensionalData.jl","title":"Warnings ¤","ref":"/DimensionalData/stable/crash/generated/course/course/#warnings","content":" Warnings ¤ Indexing with unordered or reverse-ordered arrays has undefined behaviour. It will trash the dimension index, break  searchsorted  and nothing will make sense any more. So do it at you own risk. However, indexing with sorted vectors of  Int  can be useful, so it's allowed. But it may do strange things to interval sizes for  Intervals  that are not  Explicit . This selects the first 5 entries of the underlying array. In the case that  A  has only one dimension, it will be retained. Multidimensional  AbstracDimArray  indexed this way will return a regular array. This page was generated using  Literate.jl ."},{"id":380,"pagetitle":"Reference - DimensionalData.jl","title":"API ¤","ref":"/DimensionalData/stable/reference/#api","content":" API ¤"},{"id":381,"pagetitle":"Reference - DimensionalData.jl","title":"Arrays ¤","ref":"/DimensionalData/stable/reference/#arrays","content":" Arrays ¤ # DimensionalData.AbstractDimArray  —  Type . AbstractDimArray   <:   AbstractArray \n Abstract supertype for all \"dim\" arrays. These arrays return a  Tuple  of  Dimension  from a  dims  method, and can be rebuilt using  rebuild . parent  must return the source array. They should have  metadata ,  name  and  refdims  methods, although these are optional. A  rebuild  method for  AbstractDimArray  must accept  data ,  dims ,  refdims ,  name ,  metadata  arguments. Indexing  AbstractDimArray  with non-range  AbstractArray  has undefined effects on the  Dimension  index. Use forward-ordered arrays only\" source # DimensionalData.DimArray  —  Type . DimArray   <:   AbstractDimArray \n\n DimArray ( data ,   dims ,   refdims ,   name ,   metadata ) \n DimArray ( data ,   dims :: Tuple ;   refdims = (),   name = NoName (),   metadata = NoMetadata ()) \n The main concrete subtype of  AbstractDimArray . DimArray  maintains and updates its  Dimension s through transformations and moves dimensions to reference dimension  refdims  after reducing operations (like e.g.  mean ). Arguments data : An  AbstractArray . dims : A  Tuple  of  Dimension name : A string name for the array. Shows in plots and tables. refdims : refence dimensions. Usually set programmatically to track past   slices and reductions of dimension for labelling and reconstruction. metadata :  Dict  or  Metadata  object, or  NoMetadata() Indexing can be done with all regular indices, or with  Dimension s and/or  Selector s.  Indexing  AbstractDimArray  with non-range  AbstractArray  has undefined effects on the  Dimension  index. Use forward-ordered arrays only\" Example: using   Dates ,   DimensionalData \n\n ti   =   ( Ti ( DateTime ( 2001 ) : Month ( 1 ) : DateTime ( 2001 , 12 )), \n x   =   X ( 10 : 10 : 100 )) \n A   =   DimArray ( rand ( 12 , 10 ),   ( ti ,   x ),   \"example\" ) \n\n julia >   A [ X ( Near ([ 12 ,   35 ])),   Ti ( At ( DateTime ( 2001 , 5 )))]; \n\n julia >   A [ Near ( DateTime ( 2001 ,   5 ,   4 )),   Between ( 20 ,   50 )]; \n source"},{"id":382,"pagetitle":"Reference - DimensionalData.jl","title":"Multi-array datasets ¤","ref":"/DimensionalData/stable/reference/#multi-array-datasets","content":" Multi-array datasets ¤ # DimensionalData.AbstractDimStack  —  Type . AbstractDimStack \n Abstract supertype for dimensional stacks. These have multiple layers of data, but share dimensions. Notably, their behaviour lies somewhere between a  DimArray  and a  NamedTuple : indexing with a  Symbol  as in  dimstack[:symbol]  returns a  DimArray  layer. iteration amd  map  are apply over array layers, as indexed with a  Symbol . getindex  and many base methods are applied as for  DimArray  - to avoid the need    to allways use  map . This design gives very succinct code when working with many-layered, mixed-dimension objects.  But it may be jarring initially - the most surprising outcome is that  dimstack[1]  will return a  NamedTuple  of values for the first index in all layers, while  first(dimstack)  will return the first value of the iterator - the  DimArray  for the first layer. See  DimStack  for the concrete implementation. Most methods are defined on the abstract type. To extend  AbstractDimStack , implement argument and keyword version of   rebuild  and also  rebuild_from_arrays . The constructor of an  AbstractDimStack  must accept a  NamedTuple . source # DimensionalData.DimStack  —  Type . DimStack   <:   AbstractDimStack \n\n DimStack ( data :: AbstractDimArray ... ) \n DimStack ( data :: Tuple { Vararg { AbstractDimArray }}) \n DimStack ( data :: NamedTuple { Keys , Vararg { AbstractDimArray }}) \n DimStack ( data :: NamedTuple ,   dims :: DimTuple ;   metadata = NoMetadata ()) \n DimStack holds multiple objects sharing some dimensions, in a  NamedTuple . Notably, their behaviour lies somewhere between a  DimArray  and a  NamedTuple : indexing with a  Symbol  as in  dimstack[:symbol]  returns a  DimArray  layer. iteration amd  map  are apply over array layers, as indexed with a  Symbol . getindex  or  view  with  Int ,  Dimension s or  Selector s that resolve to  Int  will   return a  NamedTuple  of values from each layer in the stack.   This has very good performace, and avoids the need to always use  map . getindex  or  view  with a  Vector  or  Colon  will return another  DimStack  where   all data layers have been sliced. setindex!  must pass a  Tuple  or  NamedTuple  maching the layers. many base and  Statistics  methods ( sum ,  mean  etc) will work as for a  DimArray    again removing the need to use  map . For example, here we take the mean over the time dimension for all layers : mean ( mydimstack ;   dims = Ti ) \n And this equivalent to: map ( A   ->   mean ( A ;   dims = Ti ),   mydimstack ) \n This design gives succinct code when working with many-layered, mixed-dimension objects.  But it may be jarring initially - the most surprising outcome is that  dimstack[1]  will return a  NamedTuple  of values for the first index in all layers, while  first(dimstack)  will return the first value of the iterator - the  DimArray  for the first layer. DimStack  can be constructed from multiple  AbstractDimArray  or a  NamedTuple  of  AbstractArray  and a matching  dims  tuple. Most  Base  and  Statistics  methods that apply to  AbstractArray  can be used on all layers of the stack simulataneously. The result is a  DimStack , or a  NamedTuple  if methods like  mean  are used without  dims  arguments, and return a single non-array value. Example julia>   using   DimensionalData \n\n julia>   A   =   [ 1.0   2.0   3.0 ;   4.0   5.0   6.0 ]; \n\n julia>   dimz   =   ( X ([ :a ,   :b ]),   Y ( 10.0 : 10.0 : 30.0 )) \n X Symbol[:a, :b], \n Y 10.0:10.0:30.0 \n\n julia>   da1   =   DimArray ( 1 A ,   dimz ;   name = :one ); \n\n julia>   da2   =   DimArray ( 2 A ,   dimz ;   name = :two ); \n\n julia>   da3   =   DimArray ( 3 A ,   dimz ;   name = :three ); \n\n julia>   s   =   DimStack ( da1 ,   da2 ,   da3 ); \n\n julia>   s [ At ( :b ),   At ( 10.0 )] \n (one = 4.0, two = 8.0, three = 12.0) \n\n julia>   s [ X ( At ( :a ))]   isa   DimStack \n true \n source"},{"id":383,"pagetitle":"Reference - DimensionalData.jl","title":"Dimension indices generators ¤","ref":"/DimensionalData/stable/reference/#dimension-indices-generators","content":" Dimension indices generators ¤ # DimensionalData.DimIndices  —  Type . DimIndices   <:   AbstractArray \n\n DimIndices ( x ) \n DimIndices ( dims :: Tuple ) \n DimIndices ( dims :: Dimension ) \n Like  CartesianIndices , but for  Dimension s. Behaves as an  Array  of  Tuple  of  Dimension(i)  for all combinations of the axis indices of  dims . This can be used to view/index into arbitrary dimensions over an array, and is especially useful when combined with  otherdims , to iterate over the indices of unknown dimension. source # DimensionalData.DimKeys  —  Type . DimKeys   <:   AbstractArray \n\n DimKeys ( x ) \n DimKeys ( dims :: Tuple ) \n DimKeys ( dims :: Dimension ) \n Like  CartesianIndices , but for the lookup values of Dimensions. Behaves as an  Array  of  Tuple  of  Dimension(At(lookupvalue))  for all combinations of the lookup values of  dims . source # DimensionalData.DimPoints  —  Type . DimPoints   <:   AbstractArray \n\n DimPoints ( x ;   order ) \n DimPoints ( dims :: Tuple ;   order ) \n DimPoints ( dims :: Dimension ;   order ) \n Like  CartesianIndices , but for the point values of the dimension index.  Behaves as an  Array  of  Tuple  lookup values (whatever they are) for all combinations of the lookup values of  dims . Either a  Dimension , a  Tuple  of  Dimension  or an object that defines a  dims  method can be passed in. Keywords order : determines the order of the points, the same as the order of  dims  by default. source"},{"id":384,"pagetitle":"Reference - DimensionalData.jl","title":"Tables.jl/TableTraits.jl interface ¤","ref":"/DimensionalData/stable/reference/#tablesjltabletraitsjl-interface","content":" Tables.jl/TableTraits.jl interface ¤ # DimensionalData.AbstractDimTable  —  Type . AbstractDimTable   <:   Tables . AbstractColumns \n Abstract supertype for dim tables source # DimensionalData.DimTable  —  Type . DimTable   <:   AbstractDimTable \n\n DimTable ( s :: AbstractDimStack ;   mergedims = nothing ) \n DimTable ( x :: AbstractDimArray ;   layersfrom = nothing ,   mergedims = nothing ) \n DimTable ( xs :: Vararg { AbstractDimArray };   layernames = nothing ,   mergedims = nothing ) \n Construct a Tables.jl/TableTraits.jl compatible object out of an  AbstractDimArray  or  AbstractDimStack . This table will have columns for the array data and columns for each  Dimension  index, as a [ DimColumn ]. These are lazy, and generated as required. Column names are converted from the dimension types using  DimensionalData.dim2key . This means type  Ti  becomes the column name  :Ti , and  Dim{:custom}  becomes  :custom . To get dimension columns, you can index with  Dimension  ( X() ) or  Dimension  type ( X ) as well as the regular  Int  or  Symbol . Keywords mergedims : Combine two or more dimensions into a new dimension. layersfrom : Treat a dimension of an  AbstractDimArray  as layers of an  AbstractDimStack . Example julia>   a   =   DimArray ( rand ( 32 , 32 , 3 ),   ( X , Y , Dim { :band })); \n\n julia>   DimTable ( a ,   layersfrom = Dim { :band },   mergedims = ( X , Y ) =>: geometry ) \n DimTable with 1024 rows, 4 columns, and schema: \n  :geometry  Tuple{Int64, Int64} \n  :band_1    Float64 \n  :band_2    Float64 \n  :band_3    Float64 \n source # DimensionalData.DimColumn  —  Type . DimColumn { T , D <: Dimension }   <:   AbstractVector { T } \n\n DimColumn ( dim :: Dimension ,   dims :: Tuple { Vararg { DimTuple }}) \n DimColumn ( dim :: DimColumn ,   length :: Int ,   dimstride :: Int ) \n A table column based on a  Dimension  and it's relationship with other  Dimension s in  dims . length  is the product of all dim lengths (usually the length of the corresponding array data), while stride is the product of the preceding dimension lengths, which may or may not be the real stride of the corresponding array depending on the data type. For  A isa Array , the  dimstride  will match the  stride . When the second argument is a  Tuple  of  Dimension , the  length  and  dimstride  fields are calculated from the dimensions, relative to the column dimension  dim . This object will be returned as a column of  DimTable . source"},{"id":385,"pagetitle":"Reference - DimensionalData.jl","title":"Common methods ¤","ref":"/DimensionalData/stable/reference/#common-methods","content":" Common methods ¤ Common functions for obtaining information from objects: # DimensionalData.Dimensions.dims  —  Function . dims ( x ,   [ dims :: Tuple ])   =>   Tuple { Vararg { Dimension }} \n dims ( x ,   dim )   =>   Dimension \n Return a tuple of  Dimension s for an object, in the order that matches the axes or columns of the underlying data. dims  can be  Dimension ,  Dimension  types, or  Symbols  for  Dim{Symbol} . The default is to return  nothing . source # DimensionalData.Dimensions.refdims  —  Function . refdims ( x ,   [ dims :: Tuple ])   =>   Tuple { Vararg { Dimension }} \n refdims ( x ,   dim )   =>   Dimension \n Reference dimensions for an array that is a slice or view of another array with more dimensions. slicedims(a, dims)  returns a tuple containing the current new dimensions and the new reference dimensions. Refdims can be stored in a field or disgarded, as it is mostly to give context to plots. Ignoring refdims will simply leave some captions empty. The default is to return an empty  Tuple () . source # DimensionalData.Dimensions.LookupArrays.metadata  —  Function . metadata ( x )   =>   ( object   metadata ) \n metadata ( x ,   dims :: Tuple )    =>   Tuple   ( Dimension   metadata ) \n metadata ( xs :: Tuple )   =>   Tuple \n Returns the metadata for an object or for the specified dimension(s) Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.name  —  Function . name ( x )   =>   Symbol \n name ( xs : Tuple )   =>   NTuple { N , Symbol } \n name ( x ,   dims :: Tuple )   =>   NTuple { N , Symbol } \n name ( x ,   dim )   =>   Symbol \n Get the name of an array or Dimension, or a tuple of of either as a Symbol. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source Utility methods for transforming DimensionalData objects: # DimensionalData.Dimensions.LookupArrays.set  —  Function . set ( x ,   val ) \n set ( x ,   args :: Pairs ... )   =>   x   with   updated   field / s \n set ( x ,   args ... ;   kw ... )   =>   x   with   updated   field / s \n set ( x ,   args :: Tuple { Vararg { Dimension }};   kw ... )   =>   x   with   updated   field / s \n\n set ( dim :: Dimension ,   index :: AbstractArray )   =>   Dimension \n set ( dim :: Dimension ,   lookup :: LookupArray )   =>   Dimension \n set ( dim :: Dimension ,   lookupcomponent :: LookupArrayTrait )   =>   Dimension \n set ( dim :: Dimension ,   metadata :: AbstractMetadata )   =>   Dimension \n Set the properties of an object, its internal data or the traits of its dimensions and lookup index. As DimensionalData is so strongly typed you do not need to specify what field of a  LookupArray  to  set  - there is no ambiguity. To set fields of a  LookupArray  you need to specify the dimension. This can be done using  X => val  pairs,  X = val  keyword arguments, or  X(val)  wrapped arguments. When a  Dimension  or  LookupArray  is passed to  set  to replace the existing ones, fields that are not set will keep their original values. Notes: Changing a lookup index range/vector will also update the step size and order where applicable. Setting the  Order  like  ForwardOrdered  will  not  reverse the array or dimension to match. Use  reverse  and  reorder  to do this. Examples julia>   using   DimensionalData ;   const   DD   =   DimensionalData \n DimensionalData \n\n julia>   da   =   DimArray ( zeros ( 3 ,   4 ),   ( custom = 10.0 : 010.0 : 30.0 ,   Z =- 20 : 010.0 : 10.0 )); \n\n julia>   set ( da ,   ones ( 3 ,   4 )) \n 3×4 DimArray{Float64,2} with dimensions: \n   Dim{:custom} Sampled{Float64} 10.0:10.0:30.0 ForwardOrdered Regular Points, \n   Z Sampled{Float64} -20.0:10.0:10.0 ForwardOrdered Regular Points \n        -20.0  -10.0  0.0  10.0 \n  10.0    1.0    1.0  1.0   1.0 \n  20.0    1.0    1.0  1.0   1.0 \n  30.0    1.0    1.0  1.0   1.0  \n Change the  Dimension  wrapper type: julia>   set ( da ,   :Z   =>   Ti ,   :custom   =>   Z ) \n 3×4 DimArray{Float64,2} with dimensions: \n   Z Sampled{Float64} 10.0:10.0:30.0 ForwardOrdered Regular Points, \n   Ti Sampled{Float64} -20.0:10.0:10.0 ForwardOrdered Regular Points \n        -20.0  -10.0  0.0  10.0 \n  10.0    0.0    0.0  0.0   0.0 \n  20.0    0.0    0.0  0.0   0.0 \n  30.0    0.0    0.0  0.0   0.0  \n Change the lookup  Vector : julia>   set ( da ,   Z   =>   [ :a ,   :b ,   :c ,   :d ],   :custom   =>   [ 4 ,   5 ,   6 ]) \n 3×4 DimArray{Float64,2} with dimensions: \n   Dim{:custom} Sampled{Int64} Int64[4, 5, 6] ForwardOrdered Regular Points, \n   Z Sampled{Symbol} Symbol[:a, :b, :c, :d] ForwardOrdered Regular Points \n      :a   :b   :c   :d \n  4  0.0  0.0  0.0  0.0 \n  5  0.0  0.0  0.0  0.0 \n  6  0.0  0.0  0.0  0.0 \n Change the  LookupArray  type: julia>   set ( da ,   Z = DD . NoLookup (),   custom = DD . Sampled ()) \n 3×4 DimArray{Float64,2} with dimensions: \n   Dim{:custom} Sampled{Float64} 10.0:10.0:30.0 ForwardOrdered Regular Points, \n   Z \n  10.0  0.0  0.0  0.0  0.0 \n  20.0  0.0  0.0  0.0  0.0 \n  30.0  0.0  0.0  0.0  0.0 \n Change the  Sampling  trait: julia>   set ( da ,   :custom   =>   DD . Irregular ( 10 ,   12 ),   Z   =>   DD . Regular ( 9.9 )) \n 3×4 DimArray{Float64,2} with dimensions: \n   Dim{:custom} Sampled{Float64} 10.0:10.0:30.0 ForwardOrdered Irregular Points, \n   Z Sampled{Float64} -20.0:10.0:10.0 ForwardOrdered Regular Points \n        -20.0  -10.0  0.0  10.0 \n  10.0    0.0    0.0  0.0   0.0 \n  20.0    0.0    0.0  0.0   0.0 \n  30.0    0.0    0.0  0.0   0.0 \n source # DimensionalData.Dimensions.LookupArrays.rebuild  —  Function . rebuild ( x ,   args ... ) \n rebuild ( x ;   kw ... ) \n Rebuild an object struct with updated field values. x  can be a  AbstractDimArray , a  Dimension ,  LookupArray  or other custom types. This is an abstraction that alows inbuilt and custom types to be rebuilt to update their fields, as most objects in DimensionalData.jl are immutable. The arguments version can be concise but depends on a fixed order defined for some DimensionalData objects. It should be defined based on the object type in DimensionalData, adding the fields specific to your object. The keyword version ignores order, and is mostly automated  using  ConstructionBase.setproperties . It should only be defined if your object has  missing fields or fields with different names to DimensionalData objects. The arguments required are defined for the abstract type that has a  rebuild  method. source # DimensionalData.modify  —  Function . modify ( f ,   A :: AbstractDimArray )   =>   AbstractDimArray \n modify ( f ,   s :: AbstractDimStack )   =>   AbstractDimStack \n modify ( f ,   dim :: Dimension )   =>   Dimension \n modify ( f ,   x ,   lookupdim :: Dimension )   =>   typeof ( x ) \n Modify the parent data, rebuilding the object wrapper without change.  f  must return a  AbstractArray  of the same size as the original. This method is mostly useful as a way of swapping the parent array type of an object. Example If we have a previously-defined  DimArray , we can copy it to an Nvidia GPU with: A   =   DimArray ( rand ( 100 ,   100 ),   ( X ,   Y )) \n modify ( CuArray ,   A ) \n This also works for all the data layers in a  DimStack . source # DimensionalData.broadcast_dims  —  Function . broadcast_dims ( f ,   sources :: AbstractDimArray ... )   =>   AbstractDimArray \n Broadcast function  f  over the  AbstractDimArray s in  sources , permuting and reshaping dimensions to match where required. The result will contain all the dimensions in  all passed in arrays in the order in which they are found. Arguments sources :  AbstractDimArrays  to broadcast over with  f . This is like broadcasting over every slice of  A  if it is sliced by the dimensions of  B . source # DimensionalData.broadcast_dims!  —  Function . broadcast_dims! ( f ,   dest :: AbstractDimArray ,   sources :: AbstractDimArray ... )   =>   dest \n Broadcast function  f  over the  AbstractDimArray s in  sources , writing to  dest .   sources  are permuting and reshaping dimensions to match where required. The result will contain all the dimensions in all passed in arrays, in the order in which they are found. Arguments dest :  AbstractDimArray  to update. sources :  AbstractDimArrays  to broadcast over with  f . source # DimensionalData.mergedims  —  Function . mergedims ( old_dims   =>   new_dim )   =>   Dimension \n Return a dimension  new_dim  whose indices are a  MergedLookup  of the indices of  old_dims . source mergedims(dims, old_dims => new_dim, others::Pair...) => dims_new\n If dimensions  old_dims ,  new_dim , etc. are found in  dims , then return new  dims_new  where all dims in  old_dims  have been combined into a single dim  new_dim . The returned dimension will keep only the name of  new_dim . Its coords will be a  MergedLookup  of the coords of the dims in  old_dims . New dimensions are always placed at the end of  dims_new .  others  contains other dimension pairs to be merged. Example julia>   ds   =   ( X ( 0 : 0.1 : 0.4 ),   Y ( 10 : 10 : 100 ),   Ti ([ 0 ,   3 ,   4 ])); \n julia>   mergedims ( ds ,   Ti   =>   :time ,   ( X ,   Y )   =>   :space ) \n Dim{:time} MergedLookup{Tuple{Int64}} Tuple{Int64}[(0,), (3,), (4,)] Ti, \n Dim{:space} MergedLookup{Tuple{Float64, Int64}} Tuple{Float64, Int64}[(0.0, 10), (0.1, 10), …, (0.3, 100), (0.4, 100)] X, Y \n source mergedims(A::AbstractDimArray, dim_pairs::Pair...) => AbstractDimArray\nmergedims(A::AbstractDimStack, dim_pairs::Pair...) => AbstractDimStack\n Return a new array or stack whose dimensions are the result of  mergedims(dims(A), dim_pairs) . source # DimensionalData.unmergedims  —  Function . unmergedims ( merged_dims :: Tuple { Vararg { Dimension }})   =>   Tuple { Vararg { Dimension }} \n Return the unmerged dimensions from a tuple of merged dimensions. However, the order of the original dimensions are not necessarily preserved. source unmergedims(A::AbstractDimArray, original_dims) => AbstractDimArray\nunmergedims(A::AbstractDimStack, original_dims) => AbstractDimStack\n Return a new array or stack whose dimensions are restored to their original prior to calling  mergedims(A, dim_pairs) . source # DimensionalData.reorder  —  Function . reorder ( A :: Union { AbstractDimArray , AbstractDimStack },   order :: Pair ... ) \n reorder ( A :: Union { AbstractDimArray , AbstractDimStack },   order ) \n reorder ( A :: Dimension ,   order :: Order ) \n Reorder every dims index/array to  order , or reorder index for the the given dimension(s) in  order . order  can be an  Order ,  Dimension => Order  pairs. A Tuple of Dimensions or any object that defines  dims  can be used in which case dimensions are If no axis reversal is required the same objects will be returned, without allocation. Example **Create a DimArray**\n\nda = DimArray([1 2 3; 4 5 6], (X(10:10:20), Y(300:-100:100)))\n\n**Reverse it**\n\nrev = reverse(da, dims=Y)\n\n**using `da` in reorder will return it to the original order**\n\nreorder(rev, da) == da\n\n**output**\n\ntrue\n\n\n<a target='_blank' href='https://github.com/rafaqz/DimensionalData.jl/blob/b6a68dc2a607e1dc87a8f961483e4e4889cc6eeb/src/utils.jl#L2-L27' class='documenter-source'>source</a><br>\n\n<a id='Base.cat' href='#Base.cat'>#</a>\n**`Base.cat`** &mdash; *Function*.\n\n\n\n```julia\nBase.cat(stacks::AbstractDimStack...; [keys=keys(stacks[1])], dims)\n Concatenate all or a subset of layers for all passed in stacks. Keywords keys :  Tuple  of  Symbol  for the stack keys to concatenate. dims : Dimension of child array to concatenate on. Example Concatenate the :sea surface temp and :humidity layers in the time dimension: cat ( stacks ... ;   keys = ( :sea_surface_temp ,   :humidity ),   dims = Ti ) \n source # Base.map  —  Function . Base . map ( f ,   stacks :: AbstractDimStack ... ) \n Apply function  f  to each layer of the  stacks . If  f  returns  DimArray s the result will be another  DimStack . Other values will be returned in a  NamedTuple . source # Base.copy!  —  Function . Base . copy! ( dst :: AbstractArray ,   src :: AbstractGimStack ,   key :: Key ) \n Copy the stack layer  key  to  dst , which can be any  AbstractArray . Example Copy the  :humidity  layer from  stack  to  array . copy! ( array ,   stack ,   :humidity ) \n source Base.copy!(dst::AbstractDimStack, src::AbstractDimStack, [keys=keys(dst)])\n Copy all or a subset of layers from one stack to another. Example Copy just the  :sea_surface_temp  and  :humidity  layers from  src  to  dst . copy! ( dst :: AbstractDimStack ,   src :: AbstractDimStack ,   keys = ( :sea_surface_temp ,   :humidity )) \n source # Base.eachslice  —  Function . Base . eachslice ( stack :: AbstractDimStack ;   dims ) \n Create a generator that iterates over dimensions  dims  of  stack , returning stacks that select all the data from the other dimensions in  stack  using views. The generator has  size  and  axes  equivalent to those of the provided  dims . Examples julia >   ds   =   DimStack (( \n             x = DimArray ( randn ( 2 ,   3 ,   4 ),   ( X ([ :x1 ,   :x2 ]),   Y ( 1 : 3 ),   Z )), \n             y = DimArray ( randn ( 2 ,   3 ,   5 ),   ( X ([ :x1 ,   :x2 ]),   Y ( 1 : 3 ),   Ti )) \n         )); \n\n julia >   slices   =   eachslice ( ds ;   dims = ( Z ,   X )); \n\n julia >   size ( slices ) \n ( 4 ,   2 ) \n\n julia >   map ( dims ,   axes ( slices )) \n Z , \n X   Categorical { Symbol }   Symbol [ x1 ,   x2 ]   ForwardOrdered \n\n julia >   first ( slices ) \n DimStack   with   dimensions : \n    Y   Sampled { Int64 }   1 : 3   ForwardOrdered   Regular   Points , \n    Ti \n and   2   layers : \n    :x   Float64   dims :   Y   ( 3 ) \n    :y   Float64   dims :   Y ,   Ti   ( 3 × 5 ) \n source Most base methods work as expected, using  Dimension  wherever a  dims  keyword is used. They are not allspecifically documented here. Shorthand constructors: # Base.fill  —  Function . Base . fill ( x ,   dims :: Dimension ... ;   kw ... )   =>   DimArray \n Base . fill ( x ,   dims :: Tuple { Vararg { Dimension }};   kw ... )   =>   DimArray \n Create a  DimArray  with a fill value of  x . There are two kinds of  Dimension  value acepted: A  Dimension  holding an  AbstractVector  will set the dimension index to  that  AbstractVector , and detect the dimension lookup. A  Dimension  holding an  Integer  will set the length of the axis, and set the dimension lookup to  NoLookup . Keywords are the same as for  DimArray . Example ```@doctest\njulia> using DimensionalData julia> rand(Bool, X(2), Y(4))\n2×4 DimArray{Bool,2} with dimensions: X, Y\n 1  0  0  1\n 1  0  1  1\n <a target='_blank' href='https://github.com/rafaqz/DimensionalData.jl/blob/b6a68dc2a607e1dc87a8f961483e4e4889cc6eeb/src/array/array.jl#L409-L432' class='documenter-source'>source</a><br>\n\n<a id='Base.rand' href='#Base.rand'>#</a>\n**`Base.rand`** &mdash; *Function*.\n\n\n\n```julia\nBase.rand(x, dims::Dimension...; kw...) => DimArray\nBase.rand(x, dims::Tuple{Vararg{Dimension}}; kw...) => DimArray\nBase.rand(r::AbstractRNG, x, dims::Tuple{Vararg{Dimension}}; kw...) => DimArray\nBase.rand(r::AbstractRNG, x, dims::Dimension...; kw...) => DimArray\n Create a  DimArray  of random values. There are two kinds of  Dimension  value acepted: A  Dimension  holding an  AbstractVector  will set the dimension index to  that  AbstractVector , and detect the dimension lookup. A  Dimension  holding an  Integer  will set the length of the axis, and set the dimension lookup to  NoLookup . Keywords are the same as for  DimArray . Example julia >   using   DimensionalData \n\n julia >   rand ( Bool ,   X ( 2 ),   Y ( 4 )) \n 2 × 4   DimArray { Bool , 2 }   with   dimensions :   X ,   Y \n   1    0    0    1 \n   1    0    1    1 \n\n julia >   rand ( X ([ :a ,   :b ,   :c ]),   Y ( 100.0 : 50 : 200.0 )) \n 3 × 3   DimArray { Float64 , 2 }   with   dimensions : \n    X :   Symbol [ a ,   b ,   c ]   Categorical :   Unordered , \n    Y :   100.0 : 50.0 : 200.0   Sampled :   Ordered   Regular   Points \n   0.43204     0.835111    0.624231 \n   0.752868    0.471638    0.193652 \n   0.484558    0.846559    0.455256 \n source # Base.zeros  —  Function . Base . zeros ( x ,   dims :: Dimension ... ;   kw ... )   =>   DimArray \n Base . zeros ( x ,   dims :: Tuple { Vararg { Dimension }};   kw ... )   =>   DimArray \n Create a  DimArray  of zeros. There are two kinds of  Dimension  value acepted: A  Dimension  holding an  AbstractVector  will set the dimension index to  that  AbstractVector , and detect the dimension lookup. A  Dimension  holding an  Integer  will set the length of the axis, and set the dimension lookup to  NoLookup . Keywords are the same as for  DimArray . Example ```@doctest\njulia> using DimensionalData julia> zeros(Bool, X(2), Y(4))\n2×4 DimArray{Bool,2} with dimensions: X, Y\n 0  0  0  0\n 0  0  0  0 julia> zeros(X([:a, :b, :c]), Y(100.0:50:200.0))\n3×3 DimArray{Float64,2} with dimensions:\n  X: Symbol[a, b, c] Categorical: Unordered,\n  Y: 100.0:50.0:200.0 Sampled: Ordered Regular Points\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n 0.0  0.0  0.0\n <a target='_blank' href='https://github.com/rafaqz/DimensionalData.jl/blob/b6a68dc2a607e1dc87a8f961483e4e4889cc6eeb/src/array/array.jl#L472-L504' class='documenter-source'>source</a><br>\n\n<a id='Base.ones' href='#Base.ones'>#</a>\n**`Base.ones`** &mdash; *Function*.\n\n\n\n```julia\nBase.ones(x, dims::Dimension...; kw...) => DimArray\nBase.ones(x, dims::Tuple{Vararg{Dimension}}; kw...) => DimArray\n Create a  DimArray  of ones. There are two kinds of  Dimension  value acepted: A  Dimension  holding an  AbstractVector  will set the dimension index to  that  AbstractVector , and detect the dimension lookup. A  Dimension  holding an  Integer  will set the length of the axis, and set the dimension lookup to  NoLookup . Keywords are the same as for  DimArray . Example ```@doctest\njulia> using DimensionalData julia> ones(Bool, X(2), Y(4))\n2×4 DimArray{Bool,2} with dimensions: X, Y\n 1  1  1  1\n 1  1  1  1 julia> ones(X([:a, :b, :c]), Y(100.0:50:200.0))\n3×3 DimArray{Float64,2} with dimensions:\n  X: Symbol[a, b, c] Categorical: Unordered,\n  Y: 100.0:50.0:200.0 Sampled: Ordered Regular Points\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n <a target='_blank' href='https://github.com/rafaqz/DimensionalData.jl/blob/b6a68dc2a607e1dc87a8f961483e4e4889cc6eeb/src/array/array.jl#L507-L539' class='documenter-source'>source</a><br>\n\n\n<a id='Dimensions'></a>\n\n<a id='Dimensions-1'></a>\n\n# Dimensions\n\n\nHandling of Dimensions is kept in a sub-module `Dimensions`.\n\n<a id='DimensionalData.Dimensions' href='#DimensionalData.Dimensions'>#</a>\n**`DimensionalData.Dimensions`** &mdash; *Module*.\n\n\n\nDimensions\n\nSub-module for [`Dimension`](reference.md#DimensionalData.Dimensions.Dimension)s wrappers, and operations on them used in DimensionalData.jl.\n\nTo load `Dimensions` types and methods into scope:\n\n```julia\nusing DimensionalData\nusing DimensionalData.Dimensions\n source Dimensions have a type-heirarchy that organises plotting and dimension matching. # DimensionalData.Dimensions.Dimension  —  Type . Dimension \n Abstract supertype of all dimension types. Example concrete implementations are  X ,  Y ,  Z ,  Ti  (Time), and the custom [ Dim ]@ref) dimension. Dimension s label the axes of an  AbstractDimArray , or other dimensional objects, and are used to index into the array. They may also provide an alternate index to lookup for each array axis. This may be any  AbstractVector  matching the array axis length, or a  Val  holding a tuple for compile-time index lookups. Dimension s also have  lookup  and  metadata  fields. lookup  gives more details about the dimension, such as that it is  Categorical  or  Sampled  as  Points  or  Intervals  along some transect. DimensionalData will attempt to guess the lookup from the passed-in index value. Example: using   DimensionalData ,   Dates \n\n x   =   X ( 2 : 2 : 10 ) \n y   =   Y ([ 'a' ,   'b' ,   'c' ]) \n ti   =   Ti ( DateTime ( 2021 ,   1 ) : Month ( 1 ) : DateTime ( 2021 ,   12 )) \n\n A   =   DimArray ( zeros ( 3 ,   5 ,   12 ),   ( y ,   x ,   ti )) \n\n # output \n\n 3 × 5 × 12   DimArray { Float64 , 3 }   with   dimensions : \n    Y   Categorical { Char }   Char [ 'a' ,   'b' ,   'c' ]   ForwardOrdered , \n    X   Sampled { Int64 }   2 : 2 : 10   ForwardOrdered   Regular   Points , \n    Ti   Sampled { DateTime }   DateTime ( \"2021-01-01T00:00:00\" ) : Month ( 1 ) : DateTime ( \"2021-12-01T00:00:00\" )   ForwardOrdered   Regular   Points \n [ : ,   : ,   1 ] \n         2      4      6      8      10 \n    'a'    0.0    0.0    0.0    0.0     0.0 \n    'b'    0.0    0.0    0.0    0.0     0.0 \n    'c'    0.0    0.0    0.0    0.0     0.0 \n [ and   11   more   slices ... ] \n For simplicity, the same  Dimension  types are also used as wrappers in  getindex , like: x   =   A [ X ( 2 ),   Y ( 3 )] \n\n # output \n\n 12 - element   DimArray { Float64 , 1 }   with   dimensions : \n    Ti   Sampled { DateTime }   DateTime ( \"2021-01-01T00:00:00\" ) : Month ( 1 ) : DateTime ( \"2021-12-01T00:00:00\" )   ForwardOrdered   Regular   Points \n and   reference   dimensions : \n    Y   Categorical { Char }   Char [ 'c' ]   ForwardOrdered , \n    X   Sampled { Int64 }   4 : 2 : 4   ForwardOrdered   Regular   Points \n   2021 - 01 - 01 T00 : 00 : 00    0.0 \n   2021 - 02 - 01 T00 : 00 : 00    0.0 \n   2021 - 03 - 01 T00 : 00 : 00    0.0 \n   2021 - 04 - 01 T00 : 00 : 00    0.0 \n   ⋮ \n   2021 - 10 - 01 T00 : 00 : 00    0.0 \n   2021 - 11 - 01 T00 : 00 : 00    0.0 \n   2021 - 12 - 01 T00 : 00 : 00    0.0 \n A  Dimension  can also wrap  Selector . x   =   A [ X ( Between ( 3 ,   4 )),   Y ( At ( 'b' ))] \n\n # output \n\n 1 × 12   DimArray { Float64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   4 : 2 : 4   ForwardOrdered   Regular   Points , \n    Ti   Sampled { DateTime }   DateTime ( \"2021-01-01T00:00:00\" ) : Month ( 1 ) : DateTime ( \"2021-12-01T00:00:00\" )   ForwardOrdered   Regular   Points \n and   reference   dimensions : \n    Y   Categorical { Char }   Char [ 'b' ]   ForwardOrdered \n       2021 - 01 - 01 T00 : 00 : 00    …     2021 - 12 - 01 T00 : 00 : 00 \n   4    0.0                                    0.0 \n Dimension  objects may have  lookup  and  metadata  fields to track additional information about the data and the index, and their relationship. source # DimensionalData.Dimensions.DependentDim  —  Type . DependentDim   <:   Dimension \n Abstract supertype for Dependent dimensions. These will plot on the Y axis. source # DimensionalData.Dimensions.IndependentDim  —  Type . IndependentDim   <:   Dimension \n Abstract supertype for independent dimensions. Thise will plot on the X axis. source # DimensionalData.Dimensions.XDim  —  Type . XDim   <:   IndependentDim \n Abstract supertype for all X dimensions. source # DimensionalData.Dimensions.YDim  —  Type . YDim   <:   DependentDim \n Abstract supertype for all Y dimensions. source # DimensionalData.Dimensions.ZDim  —  Type . ZDim   <:   DependentDim \n Abstract supertype for all Z dimensions. source # DimensionalData.Dimensions.TimeDim  —  Type . TimeDim   <:   IndependentDim \n Abstract supertype for all time dimensions. In a  TimeDime  with  Interval  sampling the locus will automatically be set to  Start() . Dates and times generally refer to the start of a month, hour, second etc., not the central point as is more common with spatial data. ` source # DimensionalData.Dimensions.X  —  Type . X   <:   XDim \n\n X ( val =: ) \n X  Dimension .  X <: XDim <: IndependentDim Example: xdim   =   X ( 2 : 2 : 10 ) \n # Or \n val   =   A [ X ( 1 )] \n # Or \n mean ( A ;   dims = X ) \n source # DimensionalData.Dimensions.Y  —  Type . Y   <:   YDim \n\n Y ( val =: ) \n Y  Dimension .  Y <: YDim <: DependentDim Example: ydim   =   Y ([ 'a' ,   'b' ,   'c' ]) \n # Or \n val   =   A [ Y ( 1 )] \n # Or \n mean ( A ;   dims = Y ) \n source # DimensionalData.Dimensions.Z  —  Type . Z   <:   ZDim \n\n Z ( val =: ) \n Z  Dimension .  Z <: ZDim <: Dimension Example: zdim   =   Z ( 10 : 10 : 100 ) \n # Or \n val   =   A [ Z ( 1 )] \n # Or \n mean ( A ;   dims = Z ) \n source # DimensionalData.Dimensions.Ti  —  Type . m     Ti <: TimeDim Ti(val=:)\n Time  Dimension .  Ti <: TimeDim <: IndependentDim Time  is already used by Dates, and  T  is a common type parameter, We use  Ti  to avoid clashes. Example: timedim   =   Ti ( DateTime ( 2021 ,   1 ) : Month ( 1 ) : DateTime ( 2021 ,   12 )) \n # Or \n val   =   A [ Ti ( 1 )] \n # Or \n mean ( A ;   dims = Ti ) \n source # DimensionalData.Dimensions.Dim  —  Type . Dim { S }( val =: ) \n A generic dimension. For use when custom dims are required when loading data from a file. Can be used as keyword arguments for indexing. Dimension types take precedence over same named  Dim  types when indexing with symbols, or e.g. creating Tables.jl keys. using   DimensionalData \n\n dim   =   Dim { :custom }([ 'a' ,   'b' ,   'c' ]) \n\n # output \n\n Dim { :custom }   Char [ 'a' ,   'b' ,   'c' ] \n source # DimensionalData.Dimensions.Coord  —  Type . Coord   <:   Dimension \n A coordinate dimension itself holds dimensions. This allows combining point data with other dimensions, such as time. Example julia >   using   DimensionalData \n\n julia >   dim   =   Coord ([( 1.0 , 1.0 , 1.0 ),   ( 1.0 , 2.0 , 2.0 ),   ( 3.0 , 4.0 , 4.0 ),   ( 1.0 , 3.0 , 4.0 )],   ( X (),   Y (),   Z ())) \n Coord   :: \n    val :   Tuple { Float64 ,   Float64 ,   Float64 }[( 1.0 ,   1.0 ,   1.0 ),   ( 1.0 ,   2.0 ,   2.0 ),   ( 3.0 ,   4.0 ,   4.0 ),   ( 1.0 ,   3.0 , \n 4.0 )] \n    lookup :   MergedLookup \n Coord { Vector { Tuple { Float64 ,   Float64 ,   Float64 }},   DimensionalData . MergedLookup { Tuple { X { Colon ,   AutoLookup { Auto \n Order },   NoMetadata },   Y { Colon ,   AutoLookup { AutoOrder },   NoMetadata },   Z { Colon ,   AutoLookup { AutoOrder },   NoMetada \n ta }}},   NoMetadata } \n\n julia >   da   =   DimArray ( 0.1 : 0.1 : 0.4 ,   dim ) \n 4 - element   DimArray { Float64 , 1 }   with   dimensions : \n    Coord   () :   Tuple { Float64 ,   Float64 ,   Float64 }[( 1.0 ,   1.0 ,   1.0 ),   ( 1.0 ,   2.0 ,   2.0 ),   ( 3.0 ,   4.0 ,   4.0 ),   ( 1.0 , \n 3.0 ,   4.0 )] \n      MergedLookup \n   0.1 \n   0.2 \n   0.3 \n   0.4 \n\n julia >   da [ Coord ( Z ( At ( 1.0 )),   Y ( Between ( 1 ,   3 )))] \n 1 - element   DimArray { Float64 , 1 }   with   dimensions : \n    Coord   () :   Tuple { Float64 ,   Float64 ,   Float64 }[( 1.0 ,   1.0 ,   1.0 )]   MergedLookup \n   0.1 \n\n julia >   da [ Coord ( 4 )]   ==   0.4 \n true \n\n julia >   da [ Coord ( Between ( 1 ,   5 ),   : ,   At ( 4.0 ))] \n 2 - element   DimArray { Float64 , 1 }   with   dimensions : \n    Coord   () :   Tuple { Float64 ,   Float64 ,   Float64 }[( 3.0 ,   4.0 ,   4.0 ),   ( 1.0 ,   3.0 ,   4.0 )]   MergedLookup \n   0.3 \n   0.4 \n source # DimensionalData.Dimensions.AnonDim  —  Type . AnonDim   <:   Dimension \n\n AnonDim () \n Anonymous dimension. Used when extra dimensions are created, such as during transpose of a vector. source # DimensionalData.Dimensions.@dim  —  Macro . @dim   typ   [ supertype = Dimension ]   [ name :: String = string ( typ )] \n Macro to easily define new dimensions. The supertype will be inserted into the type of the dim. The default is simply  YourDim <: Dimension . Making a Dimesion inherit from  XDim ,  YDim ,  ZDim  or  TimeDim  will affect automatic plot layout and other methods that dispatch on these types.  <: YDim  are plotted on the Y axis,  <: XDim  on the X axis, etc. Example: using   DimensionalData \n using   DimensionalData :   @dim ,   YDim ,   XDim \n @dim   Lat   YDim   \"latitude\" \n @dim   Lon   XDim   \"Longitude\" \n # output \n source"},{"id":386,"pagetitle":"Reference - DimensionalData.jl","title":"Exported methods ¤","ref":"/DimensionalData/stable/reference/#exported-methods","content":" Exported methods ¤ # DimensionalData.Dimensions.hasdim  —  Function . hasdim ([ f ],   x ,   query :: Tuple )   =>   NTUple { Bool } \n hasdim ([ f ],   x ,   query ... )   =>   NTUple { Bool } \n hasdim ([ f ],   x ,   query )   =>   Bool \n Check if an object  x  has dimensions that match or inherit from the  query  dimensions. Arguments x : any object with a  dims  method, a  Tuple  of  Dimension  or a single  Dimension . query : Tuple or single  Dimension  or dimension  Type . f :  <:  by default, but can be  >:  to match abstract types to concrete types. Check if an object or tuple contains an  Dimension , or a tuple of dimensions. Example julia>   using   DimensionalData \n\n julia>   A   =   DimArray ( ones ( 10 ,   10 ,   10 ),   ( X ,   Y ,   Z )); \n\n julia>   hasdim ( A ,   X ) \n true \n\n julia>   hasdim ( A ,   ( Z ,   X ,   Y )) \n (true, true, true) \n\n julia>   hasdim ( A ,   Ti ) \n false \n source # DimensionalData.Dimensions.dimnum  —  Function . dimnum ( x ,   query :: Tuple )   =>   NTuple { Int } \n dimnum ( x ,   query )   =>   Int \n Get the number(s) of  Dimension (s) as ordered in the dimensions of an object. Arguments x : any object with a  dims  method, a  Tuple  of  Dimension  or a single  Dimension . query : Tuple, Array or single  Dimension  or dimension  Type . The return type will be a Tuple of  Int  or a single  Int , depending on wether  query  is a  Tuple  or single  Dimension . Example julia>   using   DimensionalData \n\n julia>   A   =   DimArray ( ones ( 10 ,   10 ,   10 ),   ( X ,   Y ,   Z )); \n\n julia>   dimnum ( A ,   ( Z ,   X ,   Y )) \n (3, 1, 2) \n\n julia>   dimnum ( A ,   Y ) \n 2 \n source"},{"id":387,"pagetitle":"Reference - DimensionalData.jl","title":"Non-exported methods ¤","ref":"/DimensionalData/stable/reference/#non-exported-methods","content":" Non-exported methods ¤ # DimensionalData.Dimensions.lookup  —  Function . lookup ( x :: Dimension )   =>   LookupArray \n lookup ( x ,   [ dims :: Tuple ])   =>   Tuple { Vararg { LookupArray }} \n lookup ( x :: Tuple )   =>   Tuple { Vararg { LookupArray }} \n lookup ( x ,   dim )   =>   LookupArray \n Returns the  LookupArray  of a dimension. This dictates properties of the dimension such as array axis and index order, and sampling properties. dims  can be a  Dimension , a dimension type, or a tuple of either. source # DimensionalData.Dimensions.label  —  Function . label ( x )   =>   String \n label ( x ,   dims :: Tuple )   =>   NTuple { N , String } \n label ( x ,   dim )   =>   String \n label ( xs :: Tuple )   =>   NTuple { N , String } \n Get a plot label for data or a dimension. This will include the name and units if they exist, and anything else that should be shown on a plot. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.dim2key  —  Function . dim2key ( dim :: Dimension )   =>   Symbol \n dim2key ( dims :: Type { <: Dimension })   =>   Symbol \n dim2key ( dims :: Tuple )   =>   Tuple { Symbol , Vararg } \n Convert a dimension object to a simbol.  X() ,  Y() ,  Ti()  etc will be converted. to  :X ,  :Y ,  :Ti , as with any other dims generated with the  @dim  macro. All other  Dim{S}()  dimensions will generate  Symbol s  S . source # DimensionalData.Dimensions.key2dim  —  Function . key2dim ( s :: Symbol )   =>   Dimension \n key2dim ( dims ... )   =>   Tuple { Dimension , Vararg } \n key2dim ( dims :: Tuple )   =>   Tuple { Dimension , Vararg } \n Convert a symbol to a dimension object.  :X ,  :Y ,  :Ti  etc will be converted. to  X() ,  Y() ,  Ti() , as with any other dims generated with the  @dim  macro. All other  Symbol s  S  will generate  Dim{S}()  dimensions. source # DimensionalData.Dimensions.dims2indices  —  Function . dims2indices ( dim :: Dimension ,   I )   =>   NTuple { Union { Colon , AbstractArray , Int }} \n Convert a  Dimension  or  Selector I  to indices of  Int ,  AbstractArray  or  Colon . source # DimensionalData.Dimensions.LookupArrays.selectindices  —  Function . selectindices ( lookups ,   selectors ) \n Converts  Selector  to regular indices. source # DimensionalData.Dimensions.format  —  Function . format ( dims ,   x )   =>   Tuple { Vararg { Dimension , N }} \n Format the passed-in dimension(s)  dims  to match the object  x . Errors are thrown if dims don't match the array dims or size,  and any fields holding  Auto-  objects are filled with guessed objects. If a  LookupArray  hasn't been specified, a lookup is chosen based on the type and element type of the index. source # DimensionalData.Dimensions.reducedims  —  Function . reducedims ( x ,   dimstoreduce )   =>   Tuple { Vararg { Dimension }} \n Replace the specified dimensions with an index of length 1. This is usually to match a new array size where an axis has been reduced with a method like  mean  or  reduce  to a length of 1, but the number of dimensions has not changed. LookupArray  traits are also updated to correspond to the change in cell step, sampling type and order. source # DimensionalData.Dimensions.swapdims  —  Function . swapdims ( x :: T ,   newdims )   =>   T \n swapdims ( dims :: Tuple ,   newdims )   =>   Tuple { Vararg { Dimension }} \n Swap dimensions for the passed in dimensions, in the order passed. Passing in the  Dimension  types rewraps the dimension index, keeping the index values and metadata, while constructed  Dimension  objectes replace the original dimension.  nothing  leaves the original dimension as-is. Arguments x : any object with a  dims  method or a  Tuple  of  Dimension . newdim : Tuple of  Dimension  or dimension  Type . Example using   DimensionalData \n A   =   ones ( X ( 2 ),   Y ( 4 ),   Z ( 2 )) \n Dimensions . swapdims ( A ,   ( Dim { :a },   Dim { :b },   Dim { :c })) \n\n # output \n 2 × 4 × 2   DimArray { Float64 , 3 }   with   dimensions :   Dim { :a },   Dim { :b },   Dim { :c } \n [ : ,   : ,   1 ] \n   1.0    1.0    1.0    1.0 \n   1.0    1.0    1.0    1.0 \n [ and   1   more   slices ... ] \n source # DimensionalData.Dimensions.slicedims  —  Function . slicedims ( x ,   I )   =>   Tuple { Tuple , Tuple } \n slicedims ( f ,   x ,   I )   =>   Tuple { Tuple , Tuple } \n Slice the dimensions to match the axis values of the new array. All methods return a tuple conatining two tuples: the new dimensions, and the reference dimensions. The ref dimensions are no longer used in the new struct but are useful to give context to plots. Called at the array level the returned tuple will also include the previous reference dims attached to the array. Arguments f : a function  getindex ,   view  or  dotview . This will be used for slicing    getindex  is the default if  f  is not included. x : An  AbstractDimArray ,  Tuple  of  Dimension , or  Dimension I : A tuple of  Integer ,  Colon  or  AbstractArray source # DimensionalData.Dimensions.comparedims  —  Function . comparedims ( A :: AbstractDimArray ... ;   kw ... ) \n comparedims ( A :: Tuple ... ;   kw ... ) \n comparedims ( A :: Dimension ... ;   kw ... ) \n comparedims ( :: Type { Bool },   args ... ;   kw ... ) \n Check that dimensions or tuples of dimensions passed as each argument are the same, and return the first valid dimension.  If  AbstractDimArray s are passed as arguments their dimensions are compared. Empty tuples and  nothing  dimension values are ignored, returning the  Dimension  value if it exists. Passing  Bool  as the first argument means  true / false  will be returned, rather than throwing an error. Keywords These are all  Bool  flags: type : compare dimension type,  true  by default. valtype : compare wrapped value type,  false  by default. val : compare wrapped values,  false  by default. order : compare order,  false  by default. length : compare lengths,  true  by default. ignore_length_one : ignore length  1  in comparisons, and return whichever   dimension is not length 1, if any. This is useful in e.g. broadcasting comparisons.    false  by default. warn : a  String  or  nothing . Used only for  Bool  methods,   to give a warning for  false  values and include  warn  in the warning text. source # DimensionalData.Dimensions.combinedims  —  Function . combinedims ( xs ;   check = true ) \n Combine the dimensions of each object in  xs , in the order they are found. source # DimensionalData.Dimensions.otherdims  —  Function . otherdims ( x ,   query )   =>   Tuple { Vararg { Dimension , N }} \n Get the dimensions of an object  not  in  query . Arguments x : any object with a  dims  method, a  Tuple  of  Dimension . query : Tuple or single  Dimension  or dimension  Type . f :  <:  by default, but can be  >:  to match abstract types to concrete types. A tuple holding the unmatched dimensions is always returned. Example julia>   using   DimensionalData ,   DimensionalData . Dimensions \n\n julia>   A   =   DimArray ( ones ( 10 ,   10 ,   10 ),   ( X ,   Y ,   Z )); \n\n julia>   otherdims ( A ,   X ) \n Y, Z \n\n julia>   otherdims ( A ,   ( Y ,   Z )) \n X \n source # DimensionalData.Dimensions.commondims  —  Function . commondims ([ f ],   x ,   query )   =>   Tuple { Vararg { Dimension }} \n This is basically  dims(x, query)  where the order of the original is kept, unlike  dims  where the query tuple determines the order Also unlike  dims , commondims  always returns a  Tuple , no matter the input. No errors are thrown if dims are absent from either  x  or  query . f  is  <:  by default, but can be  >:  to sort abstract types by concrete types. julia>   using   DimensionalData ,   . Dimensions \n\n julia>   A   =   DimArray ( ones ( 10 ,   10 ,   10 ),   ( X ,   Y ,   Z )); \n\n julia>   commondims ( A ,   X ) \n X \n\n julia>   commondims ( A ,   ( X ,   Z )) \n X, Z \n\n julia>   commondims ( A ,   Ti ) \n () \n source # DimensionalData.Dimensions.sortdims  —  Function . sortdims ([ f ],   tosort ,   order )   =>   Tuple \n Sort dimensions  tosort  by  order . Dimensions in  order  but missing from  tosort  are replaced with  nothing . tosort  and  order  can be  Tuple s or  Vector s or Dimension or dimension type. Abstract supertypes like  TimeDim  can be used in  order . f  is  <:  by default, but can be  >:  to sort abstract types by concrete types. source # DimensionalData.Dimensions.LookupArrays.basetypeof  —  Function . basetypeof ( x )   =>   Type \n Get the \"base\" type of an object - the minimum required to define the object without it's fields. By default this is the full  UnionAll  for the type. But custom  basetypeof  methods can be defined for types with free type parameters. In DimensionalData this is primariliy used for comparing  Dimension s, where  Dim{:x}  is different from  Dim{:y} . source # DimensionalData.Dimensions.setdims  —  Function . setdims ( X ,   newdims )   =>   AbstractArray \n setdims ( :: Tuple ,   newdims )   =>   Tuple { Vararg { Dimension , N }} \n Replaces the first dim matching  <: basetypeof(newdim)  with newdim, and returns a new object or tuple with the dimension updated. Arguments x : any object with a  dims  method, a  Tuple  of  Dimension  or a single  Dimension . newdim : Tuple or single  Dimension ,  Type  or  Symbol . Example using   DimensionalData ,   DimensionalData . Dimensions ,   DimensionalData . LookupArrays \n A   =   ones ( X ( 10 ),   Y ( 10 : 10 : 100 )) \n B   =   setdims ( A ,   Y ( Categorical ( 'a' : 'j' ;   order = ForwardOrdered ()))) \n lookup ( B ,   Y ) \n # output \n Categorical { Char }   ForwardOrdered \n wrapping :   'a' : 1 : 'j' \n source # DimensionalData.Dimensions.dimsmatch  —  Function . dimsmatch ([ f ],   dim ,   query )   =>   Bool \n dimsmatch ([ f ],   dims :: Tuple ,   query :: Tuple )   =>   Bool \n Compare 2 dimensions or  Tuple  of  Dimension  are of the same base type, or are at least rotations/transformations of the same type. f  is  <:  by default, but can be  >:  to match abstract types to concrete types. source # DimensionalData.Dimensions.dimstride  —  Function . dimstride ( x ,   dim )   =>   Int \n Get the stride of the dimension relative to the other dimensions. This may or may not be equal to the stride of the related array, although it will be for  Array . Arguments x  is any object with a  dims  method, or a  Tuple  of  Dimension . dim  is a  Dimension ,  Dimension  type, or and  Int . Using an  Int  is not type-stable. source # DimensionalData.refdims_title  —  Function . refdims_title ( A :: AbstractDimArray ) \n refdims_title ( refdims :: Tuple ) \n refdims_title ( refdim :: Dimension ) \n Generate a title string based on reference dimension values. source # DimensionalData.rebuild_from_arrays  —  Function . rebuild_from_arrays ( s :: AbstractDimStack ,   das :: NamedTuple { <: Any , <: Tuple { Vararg { AbstractDimArray }}};   kw ... ) \n Rebuild an  AbstractDimStack  from a  Tuple  or  NamedTuple  of  AbstractDimArray  and an existing stack. Keywords Keywords are simply the fields of the stack object: data dims refdims metadata layerdims layermetadata source"},{"id":388,"pagetitle":"Reference - DimensionalData.jl","title":"LookupArrays ¤","ref":"/DimensionalData/stable/reference/#lookuparrays","content":" LookupArrays ¤ # DimensionalData.Dimensions.LookupArrays  —  Module . LookupArrays \n Module for  LookupArrays  and [ Selector ]s used in DimensionalData.jl LookupArrays  defines traits and  AbstractArray  wrappers that give specific behaviours for a lookup index when indexed with  Selector . For example, these allow tracking over array order so fast indexing works evne when  the array is reversed. To load LookupArrays types and methods into scope: using   DimensionalData \n using   DimensionalData . LookupArrays \n source"},{"id":389,"pagetitle":"Reference - DimensionalData.jl","title":"Selectors ¤","ref":"/DimensionalData/stable/reference/#selectors","content":" Selectors ¤ # DimensionalData.Dimensions.LookupArrays.Selector  —  Type . Selector \n Abstract supertype for all selectors. Selectors are wrappers that indicate that passed values are not the array indices, but values to be selected from the dimension index, such as  DateTime  objects for a  Ti  dimension. Selectors provided in DimensionalData are: At Between Touches Near Where Contains source # DimensionalData.Dimensions.LookupArrays.IntSelector  —  Type . IntSelector   <:   Selector \n Abstract supertype for  Selector s that return a single  Int  index. IntSelectors provided by DimensionalData are: At Contains Near source # DimensionalData.Dimensions.LookupArrays.ArraySelector  —  Type . ArraySelector   <:   Selector \n Abstract supertype for  Selector s that return an  AbstractArray . ArraySelectors provided by DimensionalData are: Between Touches Where source # DimensionalData.Dimensions.LookupArrays.At  —  Type . At   <:   IntSelector \n\n At ( x ,   atol ,   rtol ) \n At ( x ;   atol = nothing ,   rtol = nothing ) \n Selector that exactly matches the value on the passed-in dimensions, or throws an error. For ranges and arrays, every intermediate value must match an existing value - not just the end points. x  can be any value or  Vector  of values. atol  and  rtol  are passed to  isapprox . For  Number rtol  will be set to  Base.rtoldefault , otherwise  nothing , and wont be used. Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 5 : 7 ))) \n A [ X ( At ( 20 )),   Y ( At ( 6 ))] \n\n # output \n\n 5 \n source # DimensionalData.Dimensions.LookupArrays.Near  —  Type . Near   <:   IntSelector \n\n Near ( x ) \n Selector that selects the nearest index to  x . With  Points  this is simply the index values nearest to the  x , however with  Intervals  it is the interval  center  nearest to  x . This will be offset from the index value for  Start  and  End  loci. Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 5 : 7 ))) \n A [ X ( Near ( 23 )),   Y ( Near ( 5.1 ))] \n\n # output \n 4 \n source # DimensionalData.Dimensions.LookupArrays.Between  —  Type . Between   <:   ArraySelector \n\n Between ( a ,   b ) \n Depreciated: use  a..b  instead of  Between(a, b) . Other  Interval  objects from IntervalSets.jl, like `OpenInterval(a, b) will also work, giving the correct open/closed boundaries. Between  will e removed in furture to avoid clashes with  DataFrames.Between . Selector that retreive all indices located between 2 values, evaluated with  >=  for the lower value, and  <  for the upper value. This means the same value will not be counted twice in 2 adjacent  Between  selections. For  Intervals  the whole interval must be lie between the values. For  Points  the points must fall between the values. Different  Sampling  types may give different results with the same input - this is the intended behaviour. Between  for  Irregular  intervals is a little complicated. The interval is the distance between a value and the next (for  Start  locus) or previous (for  End  locus) value. For  Center , we take the mid point between two index values as the start and end of each interval. This may or may not make sense for the values in your indes, so use  Between  with  Irregular Intervals(Center())  with caution. Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 5 : 7 ))) \n A [ X ( Between ( 15 ,   25 )),   Y ( Between ( 4 ,   6.5 ))] \n\n # output \n\n 1 × 2   DimArray { Int64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   20 : 10 : 20   ForwardOrdered   Regular   Points , \n    Y   Sampled { Int64 }   5 : 6   ForwardOrdered   Regular   Points \n       5    6 \n   20    4    5 \n source # DimensionalData.Dimensions.LookupArrays.Touches  —  Type . Touches   <:   ArraySelector \n\n Touches ( a ,   b ) \n Selector that retreives all indices touching the closed interval 2 values, for the maximum possible area that could interact with the supplied range. This can be better than  ..  when e.g. subsetting an area to rasterize, as you may wish to include pixels that just touch the area, rather than those that fall within it. Touches is different to using closed intervals when the lookups also contain intervals - if any of the intervals touch, they are included. With  ..  they are discarded unless the whole cell interval falls inside the selector interval. Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 5 : 7 ))) \n A [ X ( Touches ( 15 ,   25 )),   Y ( Touches ( 4 ,   6.5 ))] \n\n # output \n 1 × 2   DimArray { Int64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   20 : 10 : 20   ForwardOrdered   Regular   Points , \n    Y   Sampled { Int64 }   5 : 6   ForwardOrdered   Regular   Points \n       5    6 \n   20    4    5 \n source # DimensionalData.Dimensions.LookupArrays.Contains  —  Type . Contains   <:   IntSelector \n\n Contains ( x ) \n Selector that selects the interval the value is contained by. If the interval is not present in the index, an error will be thrown. Can only be used for  Intervals  or  Categorical . For  Categorical  it falls back to using  At .  Contains  should not be confused with  Base.contains  - use  Where(contains(x))  to check for if values are contain in categorical values like strings. Example using   DimensionalData ;   const   DD   =   DimensionalData \n dims_   =   X ( 10 : 10 : 20 ;   sampling = DD . Intervals ( DD . Center ())), \n          Y ( 5 : 7 ;   sampling = DD . Intervals ( DD . Center ())) \n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   dims_ ) \n A [ X ( Contains ( 8 )),   Y ( Contains ( 6.8 ))] \n\n # output \n 3 \n source # DimensionalData.Dimensions.LookupArrays.Where  —  Type . Where   <:   ArraySelector \n\n Where ( f :: Function ) \n Selector that filters a dimension lookup by any function that accepts a single value and returns a  Bool . Example using   DimensionalData \n\n A   =   DimArray ([ 1   2   3 ;   4   5   6 ],   ( X ( 10 : 10 : 20 ),   Y ( 19 : 21 ))) \n A [ X ( Where ( x   ->   x   >   15 )),   Y ( Where ( x   ->   x   in   ( 19 ,   21 )))] \n\n # output \n\n 1 × 2   DimArray { Int64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   Int64 [ 20 ]   ForwardOrdered   Regular   Points , \n    Y   Sampled { Int64 }   Int64 [ 19 ,   21 ]   ForwardOrdered   Regular   Points \n       19    21 \n   20     4     6 \n source # DimensionalData.Dimensions.LookupArrays.All  —  Type . All   <:   Selector \n\n All ( selectors :: Selector ... ) \n Selector that combines the results of other selectors.  The indices used will be the union of all result sorted in ascending order. Example using   DimensionalData ,   Unitful \n\n dimz   =   X ( 10.0 : 20 : 200.0 ),   Ti ( 1 u \"s\" : 5 u \"s\" : 100 u \"s\" ) \n A   =   DimArray (( 1 : 10 )   *   ( 1 : 20 ) ' ,   dimz ) \n A [ X = All ( At ( 10.0 ),   At ( 50.0 )),   Ti = All ( 1 u \"s\" .. 10 u \"s\" ,   90 u \"s\" .. 100 u \"s\" )] \n\n # output \n\n 2 × 4   DimArray { Int64 , 2 }   with   dimensions : \n    X   Sampled { Float64 }   Float64 [ 10.0 ,   50.0 ]   ForwardOrdered   Regular   Points , \n    Ti   Sampled { Quantity { Int64 ,   𝐓 ,   Unitful . FreeUnits {( s ,),   𝐓 ,   nothing }}}   Quantity { Int64 ,   𝐓 ,   Unitful . FreeUnits {( s ,),   𝐓 ,   nothing }}[ 1   s ,   6   s ,   91   s ,   96   s ]   ForwardOrdered   Regular   Points \n         1   s    6   s    91   s    96   s \n   10.0      1      2      19      20 \n   50.0      3      6      57      60 \n source Lookup properties: # DimensionalData.Dimensions.LookupArrays.bounds  —  Function . bounds ( xs ,   [ dims :: Tuple ])   =>   Tuple { Vararg { Tuple { T , T }}} \n bounds ( xs :: Tuple )   =>   Tuple { Vararg { Tuple { T , T }}} \n bounds ( x ,   dim )   =>   Tuple { T , T } \n bounds ( dim :: Union { Dimension , LookupArray })   =>   Tuple { T , T } \n Return the bounds of all dimensions of an object, of a specific dimension, or of a tuple of dimensions. If bounds are not known, one or both values may be  nothing . dims  can be a  Dimension , a dimension type, or a tuple of either. source # DimensionalData.Dimensions.LookupArrays.val  —  Function . val ( x ) \n val ( dims :: Tuple )   =>   Tuple \n Return the contained value of a wrapper object. dims  can be  Dimension ,  Dimension  types, or  Symbols  for  Dim{Symbol} . Objects that don't define a  val  method are returned unaltered. source # DimensionalData.Dimensions.LookupArrays.LookupArray  —  Type . LookupArray \n Types defining the behaviour of a lookup index, how it is plotted and how  Selector s like  Between  work. A  LookupArray  may be  NoLookup  indicating that the index is just the underlying array axis,  Categorical  for ordered or unordered categories,  or a  Sampled  index for  Points  or  Intervals . source # DimensionalData.Dimensions.LookupArrays.Aligned  —  Type . Aligned   <:   LookupArray \n Abstract supertype for  LookupArray s where the index is aligned with the array axes. This is by far the most common supertype for  LookupArray . source # DimensionalData.Dimensions.LookupArrays.AbstractSampled  —  Type . AbstractSampled   <:   Aligned \n Abstract supertype for  LookupArray s where the index is aligned with the array, and is independent of other dimensions.  Sampled  is provided by this package. AbstractSampled  must have   order ,  span  and  sampling  fields, or a  rebuild  method that accpts them as keyword arguments. source # DimensionalData.Dimensions.LookupArrays.Sampled  —  Type . Sampled   <:   AbstractSampled \n\n Sampled ( data :: AbstractVector ,   order :: Order ,   span :: Span ,   sampling :: Sampling ,   metadata ) \n Sampled (;   data = AutoIndex (),   order = AutoOrder (),   span = AutoSpan (),   sampling = Points (),   metadata = NoMetadata ()) \n A concrete implementation of the  LookupArray AbstractSampled . It can be used to represent  Points  or  Intervals . Sampled  is capable of representing gridded data from a wide range of sources, allowing correct  bounds  and  Selector s for points or intervals of regular, irregular, forward and reverse indexes. On  AbstractDimArray  construction,  Sampled  lookup is assigned for all lookups of   AbstractRange  not assigned to  Categorical . Arguments data : An  AbstractVector  of index values, matching the length of the curresponding   array axis. order :  Order ) indicating the order of the index,    AutoOrder  by default, detected from the order of  data    to be  ForwardOrdered ,  ReverseOrdered  or  Unordered .   These can be provided explicitly if they are known and performance is important. span : indicates the size of intervals or distance between points, and will be set to    Regular  for  AbstractRange  and  Irregular  for  AbstractArray ,   unless assigned manually. sampling : is assigned to  Points , unless set to  Intervals  manually.    Using  Intervals  will change the behaviour of  bounds  and  Selectors s   to take account for the full size of the interval, rather than the point alone. metadata : a  Dict  or  Metadata  wrapper that holds any metadata object adding more   information about the array axis - useful for extending DimensionalData for specific   contexts, like geospatial data in GeoData.jl. By default it is  NoMetadata() . Example Create an array with [ Interval ] sampling, and  Regular  span for a vector with known spacing. We set the  Locus  of the  Intervals  to  Start  specifying that the index values are for the positions at the start of each interval. using   DimensionalData ,   DimensionalData . LookupArrays \n\n x   =   X ( Sampled ( 100 :- 20 : 10 ;   sampling = Intervals ( Start ()))) \n y   =   Y ( Sampled ([ 1 ,   4 ,   7 ,   10 ];   span = Regular ( 3 ),   sampling = Intervals ( Start ()))) \n A   =   ones ( x ,   y ) \n\n # output \n 5 × 4   DimArray { Float64 , 2 }   with   dimensions : \n    X   Sampled { Int64 }   100 :- 20 : 20   ReverseOrdered   Regular   Intervals , \n    Y   Sampled { Int64 }   Int64 [ 1 ,   4 ,   7 ,   10 ]   ForwardOrdered   Regular   Intervals \n        1      4      7      10 \n   100    1.0    1.0    1.0     1.0 \n    80    1.0    1.0    1.0     1.0 \n    60    1.0    1.0    1.0     1.0 \n    40    1.0    1.0    1.0     1.0 \n    20    1.0    1.0    1.0     1.0 \n source # DimensionalData.Dimensions.LookupArrays.AbstractCategorical  —  Type . AbstractCategorical   <:   Aligned \n LookupArray s where the values are categories. Categorical  is the provided concrete implementation.  but this can easily be extended - all methods are defined for  AbstractCategorical . All  AbstractCategorical  must provide a  rebuild  method with  data ,  order  and  metadata  keyword arguments. source # DimensionalData.Dimensions.LookupArrays.Categorical  —  Type . Categorical   <:   AbstractCategorical \n\n Categorical ( o :: Order ) \n Categorical (;   order = Unordered ()) \n An LookupArray where the values are categories. This will be automatically assigned if the index contains  AbstractString ,  Symbol  or  Char . Otherwise it can be assigned manually. Order  will be determined automatically where possible. Arguments data : An  AbstractVector  of index values, matching the length of the curresponding   array axis. order :  Order ) indicating the order of the index,    AutoOrder  by default, detected from the order of  data    to be  ForwardOrdered ,  ReverseOrdered  or  Unordered .   Can be provided if this is known and performance is important. metadata : a  Dict  or  Metadata  wrapper that holds any metadata object adding more   information about the array axis - useful for extending DimensionalData for specific   contexts, like geospatial data in GeoData.jl. By default it is  NoMetadata() . Example Create an array with [ Interval ] sampling. using   DimensionalData \n\n ds   =   X ([ \"one\" ,   \"two\" ,   \"three\" ]),   Y ([ :a ,   :b ,   :c ,   :d ]) \n A   =   DimArray ( rand ( 3 ,   4 ),   ds ) \n Dimensions . lookup ( A ) \n\n # output \n\n Categorical { String }   String [ \"one\" ,   \"two\" ,   \"three\" ]   Unordered , \n Categorical { Symbol }   Symbol [ :a ,   :b ,   :c ,   :d ]   ForwardOrdered \n source # DimensionalData.Dimensions.LookupArrays.Unaligned  —  Type . Unaligned   <:   LookupArray \n Abstract supertype for  LookupArray  where the index is not aligned to the grid. Indexing an  Unaligned  with  Selector s must provide all other  Unaligned  dimensions. source # DimensionalData.Dimensions.LookupArrays.Transformed  —  Type . Transformed   <:   Unaligned \n\n Transformed ( f ,   dim :: Dimension ;   metadata = NoMetadata ()) \n LookupArray  that uses an affine transformation to convert dimensions from  dims(lookup)  to  dims(array) . This can be useful when the dimensions are e.g. rotated from a more commonly used axis. Any function can be used to do the transformation, but transformations from CoordinateTransformations.jl may be useful. Arguments f : transformation function dim : a dimension to transform to. Keyword Arguments metdata : Example using   DimensionalData ,   DimensionalData . LookupArrays ,   CoordinateTransformations \n\n m   =   LinearMap ([ 0.5   0.0 ;   0.0   0.5 ]) \n A   =   [ 1   2    3    4 \n       5   6    7    8 \n       9   10   11   12 ]; \n da   =   DimArray ( A ,   ( t1 = Transformed ( m ,   X ),   t2 = Transformed ( m ,   Y ))) \n\n da [ X ( At ( 6 )),   Y ( At ( 2 ))] \n\n # output \n 9 \n source # DimensionalData.Dimensions.MergedLookup  —  Type . MergedLookup   <:   LookupArray \n\n MergedLookup ( data ,   dims ;   [ metadata ]) \n A  LookupArray  that holds multiple combined dimensions. MergedLookup  can be indexed with  Selector s like  At ,   Between , and  Where  although  Near  has undefined meaning. Arguments data : A  Vector  of  Tuple . dims : A  Tuple  of  Dimension  indicating the dimensions in the tuples in  data . Keywords metadata : a  Dict  or  Metadata  object to attach dimension metadata. source # DimensionalData.Dimensions.LookupArrays.NoLookup  —  Type . NoLookup   <:   LookupArray \n\n NoLookup () \n A  LookupArray  that is identical to the array axis.   Selector s can't be used on this lookup. Example Defining a  DimArray  without passing an index to the dimensions, it will be assigned  NoLookup : using   DimensionalData \n\n A   =   DimArray ( rand ( 3 ,   3 ),   ( X ,   Y )) \n Dimensions . lookup ( A ) \n\n # output \n\n NoLookup ,   NoLookup \n Which is identical to: using   . LookupArrays \n A   =   DimArray ( rand ( 3 ,   3 ),   ( X ( NoLookup ()),   Y ( NoLookup ()))) \n Dimensions . lookup ( A ) \n\n # output \n\n NoLookup ,   NoLookup \n source # DimensionalData.Dimensions.LookupArrays.AutoLookup  —  Type . AutoLookup   <:   LookupArray \n\n AutoLookup () \n AutoLookup ( index = AutoIndex ();   kw ... ) \n Automatic  LookupArray , the default lookup. It will be converted automatically to another  LookupArray  when it is possible to detect it from the index. Keywords will be used in the detected  LookupArray  constructor. source # DimensionalData.Dimensions.LookupArrays.AutoIndex  —  Type . AutoIndex \n Detect a  LookupArray  index from the context. This is used in  NoLookup  to simply use the array axis as the index when the array is constructed, and in  set  to change the  LookupArray  type without changing the index values. source"},{"id":390,"pagetitle":"Reference - DimensionalData.jl","title":"Metadata ¤","ref":"/DimensionalData/stable/reference/#metadata","content":" Metadata ¤ # DimensionalData.Dimensions.LookupArrays.AbstractMetadata  —  Type . AbstractMetadata { X , T } \n Abstract supertype for all metadata wrappers. Metadata wrappers allow tracking the contents and origin of metadata. This can  facilitate conversion between metadata types (for saving a file to a differenet format) or simply saving data back to the same file type with identical metadata. Using a wrapper instead of  Dict  or  NamedTuple  also lets us pass metadata  objects to  set  without ambiguity about where to put them. source # DimensionalData.Dimensions.LookupArrays.Metadata  —  Type . Metadata   <:   AbstractMetadata \n\n Metadata { X }( val :: Union { Dict , NamedTuple }) \n Metadata { X }( pairs :: Pair ... )   =>   Metadata { Dict } \n Metadata { X }(;   kw ... )   =>   Metadata { NamedTuple } \n General  Metadata  object. The  X  type parameter categorises the metadata for method dispatch, if required.  source # DimensionalData.Dimensions.LookupArrays.NoMetadata  —  Type . NoMetadata   <:   AbstractMetadata \n\n NoMetadata () \n Indicates an object has no metadata. But unlike using  nothing ,   get ,  keys  and  haskey  will still work on it,  get  always returning the fallback argument.  keys  returns  ()  while  haskey  always returns  false . source"},{"id":391,"pagetitle":"Reference - DimensionalData.jl","title":"LookupArray traits ¤","ref":"/DimensionalData/stable/reference/#lookuparray-traits","content":" LookupArray traits ¤ # DimensionalData.Dimensions.LookupArrays.LookupArrayTrait  —  Type . LookupArrayTrait \n Abstract supertype of all traits of a  LookupArray . These modify the behaviour of the lookup index. The term \"Trait\" is used loosely - these may be fields of an object of traits hard-coded to specific types. source"},{"id":392,"pagetitle":"Reference - DimensionalData.jl","title":"Order ¤","ref":"/DimensionalData/stable/reference/#order","content":" Order ¤ # DimensionalData.Dimensions.LookupArrays.Order  —  Type . Order   <:   LookupArrayTrait \n Traits for the order of a  LookupArray . These determine how  searchsorted  finds values in the index, and how objects are plotted. source # DimensionalData.Dimensions.LookupArrays.Ordered  —  Type . Ordered   <:   Order \n Supertype for the order of an ordered  LookupArray , including  ForwardOrdered  and  ReverseOrdered . source # DimensionalData.Dimensions.LookupArrays.ForwardOrdered  —  Type . ForwardOrdered   <:   Ordered \n\n ForwardOrdered () \n Indicates that the  LookupArray  index is in the normal forward order. source # DimensionalData.Dimensions.LookupArrays.ReverseOrdered  —  Type . ReverseOrdered   <:   Ordered \n\n ReverseOrdered () \n Indicates that the  LookupArray  index is in the reverse order. source # DimensionalData.Dimensions.LookupArrays.Unordered  —  Type . Unordered   <:   Order \n\n Unordered () \n Indicates that  LookupArray  is unordered. This means the index cannot be searched with  searchsortedfirst  or similar optimised methods - instead it will use  findfirst . source # DimensionalData.Dimensions.LookupArrays.AutoOrder  —  Type . AutoOrder   <:   Order \n\n AutoOrder () \n Specifies that the  Order  of a  LookupArray  will be found automatically where possible. source"},{"id":393,"pagetitle":"Reference - DimensionalData.jl","title":"Span ¤","ref":"/DimensionalData/stable/reference/#span","content":" Span ¤ # DimensionalData.Dimensions.LookupArrays.Span  —  Type . Span   <:   LookupArrayTrait \n Defines the type of span used in a  Sampling  index. These are  Regular  or  Irregular . source # DimensionalData.Dimensions.LookupArrays.Regular  —  Type . Regular   <:   Span \n\n Regular ( step = AutoStep ()) \n Points  or  Intervals  that have a fixed, regular step. source # DimensionalData.Dimensions.LookupArrays.Irregular  —  Type . Irregular   <:   Span \n\n Irregular ( bounds :: Tuple ) \n Irregular ( lowerbound ,   upperbound ) \n Points  or  Intervals  that have an  Irrigular  step size. To enable bounds tracking and accuract selectors, the starting bounds are provided as a 2 tuple, or 2 arguments.  (nothing, nothing)  is acceptable input, the bounds will be guessed from the index, but may be innaccurate. source # DimensionalData.Dimensions.LookupArrays.Explicit  —  Type . Explicit ( bounds :: AbstractMatix ) \n Intervals where the span is explicitly listed for every interval. This uses a matrix where with length 2 columns for each index value, holding the lower and upper bounds for that specific index. source # DimensionalData.Dimensions.LookupArrays.AutoSpan  —  Type . AutoSpan   <:   Span \n\n AutoSpan () \n The span will be guessed and replaced in  format  or  set . source"},{"id":394,"pagetitle":"Reference - DimensionalData.jl","title":"Sampling ¤","ref":"/DimensionalData/stable/reference/#sampling","content":" Sampling ¤ # DimensionalData.Dimensions.LookupArrays.Sampling  —  Type . Sampling   <:   LookupArrayTrait \n Indicates the sampling method used by the index:  Points  or  Intervals . source # DimensionalData.Dimensions.LookupArrays.Points  —  Type . Points   <:   Sampling \n\n Points () \n Sampling  lookup where single samples at exact points. These are always plotted at the center of array cells. source # DimensionalData.Dimensions.LookupArrays.Intervals  —  Type . Intervals   <:   Sampling \n\n Intervals ( locus :: Locus ) \n Sampling  specifying that sampled values are the mean (or similar) value over an  interval , rather than at one specific point. Intervals require a  Locus  of  Start ,  Center  or  End  to define the location in the interval that the index values refer to. source"},{"id":395,"pagetitle":"Reference - DimensionalData.jl","title":"Loci ¤","ref":"/DimensionalData/stable/reference/#loci","content":" Loci ¤ # DimensionalData.Dimensions.LookupArrays.Locus  —  Type . Locus <: LookupArrayTrait Abstract supertype of types that indicate the position of index values  where they represent  Intervals . These allow for values array cells to align with the  Start ,  Center , or  End  of values in the lookup index. This means they can be plotted with correct axis markers, and allows automatic converrsions to between formats with different standards (such as NetCDF and GeoTiff). source # DimensionalData.Dimensions.LookupArrays.Center  —  Type . Center   <:   Locus \n\n Center () \n Indicates a lookup value is for the center of its corresponding array cell. source # DimensionalData.Dimensions.LookupArrays.Start  —  Type . Start   <:   Locus \n\n Start () \n Indicates a lookup value is for the start of its corresponding array cell, in the direction of the lookup index order. source # DimensionalData.Dimensions.LookupArrays.End  —  Type . End   <:   Locus \n\n End () \n Indicates a lookup value is for the end of its corresponding array cell, in the direction of the lookup index order. source # DimensionalData.Dimensions.LookupArrays.AutoLocus  —  Type . AutoLocus   <:   Locus \n\n AutoLocus () \n Indicates a interval where the index position is not yet known. This will be filled with a default value on object construction. source"},{"id":396,"pagetitle":"Reference - DimensionalData.jl","title":"LookupArrays methods ¤","ref":"/DimensionalData/stable/reference/#lookuparrays-methods","content":" LookupArrays methods ¤ # DimensionalData.Dimensions.LookupArrays.hasselection  —  Function . hasselection ( x ,   selector )   =>   Bool \n hasselection ( x ,   selectors :: Tuple )   =>   Bool \n Check if indexing into x with  selectors  can be performed, where x is some object with a  dims  method, and  selectors  is a  Selector  or  Dimension  or a tuple of either. source # DimensionalData.Dimensions.LookupArrays.shiftlocus  —  Function . shiftlocus ( locus :: Locus ,   x ) \n Shift the index of  x  from the current locus to the new locus. We only shift  Sampled ,  Regular  or  Explicit ,  Intervals .  source # DimensionalData.Dimensions.LookupArrays.sampling  —  Function . sampling ( x ,   [ dims :: Tuple ])   =>   Tuple \n sampling ( x ,   dim )   =>   Sampling \n sampling ( xs :: Tuple )   =>   Tuple { Vararg { Sampling }} \n sampling ( x : Union { Dimension , LookupArray })   =>   Sampling \n Return the  Sampling  for each dimension. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.LookupArrays.span  —  Function . span ( x ,   [ dims :: Tuple ])   =>   Tuple \n span ( x ,   dim )   =>   Span \n span ( xs :: Tuple )   =>   Tuple { Vararg { Span , N }} \n span ( x :: Union { Dimension , LookupArray })   =>   Span \n Return the  Span  for each dimension. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.LookupArrays.order  —  Function . order ( x ,   [ dims :: Tuple ])   =>   Tuple \n order ( xs :: Tuple )   =>   Tuple \n order ( x :: Union { Dimension , LookupArray })   =>   Order \n Return the  Ordering  of the dimension index for each dimension:  ForwardOrdered ,  ReverseOrdered , or  Unordered Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.LookupArrays.index  —  Function . index ( x )   =>   Tuple { Vararg { AbstractArray }} \n index ( x ,   dims :: Tuple )   =>   Tuple { Vararg { AbstractArray }} \n index ( dims :: Tuple )   =>   Tuple { Vararg { AbstractArray }} } \n index ( x ,   dim )   =>   AbstractArray \n index ( dim :: Dimension )   =>   AbstractArray \n Return the contained index of a  Dimension . Only valid when a  Dimension  contains an  AbstractArray  or a Val tuple like  Val{(:a, :b)}() . The  Val  is unwrapped to return just the  Tuple dims  can be a  Dimension , or a tuple of  Dimension . source # DimensionalData.Dimensions.LookupArrays.locus  —  Function . locus ( x ,   [ dims :: Tuple ])   =>   Tuple \n locus ( x ,   dim )   =>   Locus \n locus ( xs :: Tuple )   =>   Tuple { Vararg { Locus , N }} \n locus ( x :: Union { Dimension , LookupArray })   =>   Locus \n Return the  Locus  for each dimension. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source # DimensionalData.Dimensions.LookupArrays.units  —  Function . units ( x )   =>   Union { Nothing , Any } \n units ( xs : Tuple )   =>   Tuple \n unit ( A :: AbstractDimArray ,   dims :: Tuple )   =>   Tuple \n unit ( A :: AbstractDimArray ,   dim )   =>   Union { Nothing , Any } \n Get the units of an array or  Dimension , or a tuple of of either. Units do not have a set field, and may or may not be included in  metadata . This method is to facilitate use in labels and plots when units are available, not a guarantee that they will be. If not available,  nothing  is returned. Second argument  dims  can be  Dimension s,  Dimension  types, or  Symbols  for  Dim{Symbol} . source"},{"id":397,"pagetitle":"Reference - DimensionalData.jl","title":"Name ¤","ref":"/DimensionalData/stable/reference/#name","content":" Name ¤ # DimensionalData.AbstractName  —  Type . AbstractName \n Abstract supertype for name wrappers. source # DimensionalData.Name  —  Type . Name   <:   AbstractName \n\n Name ( name :: Union { Symbol , Name )   =>   Name \n Name ( name :: NoName )   =>   NoName \n Name wrapper. This lets arrays keep symbol names when the array wrapper neeeds to be `isbits, like for use on GPUs. It makes the name a property of the type. It's not necessary to use in normal use, a symbol is probably easier. source # DimensionalData.NoName  —  Type . NoName   <:   AbstractName \n\n NoName () \n NoName specifies an array is not named, and is the default  name  value for all  AbstractDimArray s. source"}]