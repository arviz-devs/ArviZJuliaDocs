[{"id":3,"pagetitle":"Home","title":"ArviZ.jl: Exploratory analysis of Bayesian models in Julia","ref":"/ArviZ/stable/#arvizjl","content":" ArviZ.jl: Exploratory analysis of Bayesian models in Julia ArviZ.jl is a Julia package for exploratory analysis of Bayesian models. It is codeveloped with the Python package  ArviZ  and in some cases temporarily relies on the Python package for functionality."},{"id":4,"pagetitle":"Home","title":"Installation","ref":"/ArviZ/stable/#installation","content":" Installation To install ArviZ.jl, we first need to install Python ArviZ. To use with the default Python environment, first  install Python ArviZ . From the Julia REPL, type  ]  to enter the Pkg REPL mode and run pkg> add ArviZ To install ArviZ.jl with its Python dependencies in Julia's private conda environment, in the console run PYTHON=\"\" julia -e 'using Pkg; Pkg.add(\"PyCall\"); Pkg.build(\"PyCall\"); Pkg.add(\"ArviZ\")' For specifying other Python versions, see the  PyCall documentation ."},{"id":5,"pagetitle":"Home","title":"Usage","ref":"/ArviZ/stable/#usage","content":" Usage See the  Quickstart  for example usage and the  API Overview  for description of functions."},{"id":6,"pagetitle":"Home","title":"Extending ArviZ.jl","ref":"/ArviZ/stable/#extendingarviz","content":" Extending ArviZ.jl To use a custom data type with ArviZ.jl, simply overload  convert_to_inference_data  to convert your input(s) to an  InferenceData ."},{"id":7,"pagetitle":"Home","title":"Known Issues","ref":"/ArviZ/stable/#knownissues","content":" Known Issues ArviZ.jl uses  PyCall.jl  to wrap ArviZ. At the moment, Julia segfaults if Numba is imported, which ArviZ does if it is available. For the moment, the workaround is to  specify a Python version  that doesn't have Numba installed. See  this issue  for more details."},{"id":10,"pagetitle":"API Overview","title":"API Overview","ref":"/ArviZ/stable/api/#api","content":" API Overview Data Dataset Diagnostics InferenceData Plots Stats"},{"id":13,"pagetitle":"Data","title":"Data","ref":"/ArviZ/stable/api/data/#data-api","content":" Data ArviZ.concat ArviZ.extract ArviZ.from_cmdstan ArviZ.from_dict ArviZ.from_json ArviZ.from_mcmcchains ArviZ.from_netcdf ArviZ.from_samplechains ArviZ.load_example_data ArviZ.to_netcdf"},{"id":14,"pagetitle":"Data","title":"Inference library converters","ref":"/ArviZ/stable/api/data/#Inference-library-converters","content":" Inference library converters"},{"id":15,"pagetitle":"Data","title":"ArviZ.from_cmdstan","ref":"/ArviZ/stable/api/data/#ArviZ.from_cmdstan","content":" ArviZ.from_cmdstan  —  Function Convert CmdStan data into an InferenceData object. Note This function is forwarded to Python's  arviz.from_cmdstan . The docstring of that function is included below. \n    For a usage example read the\n    :ref:`Creating InferenceData section on from_cmdstan <creating_InferenceData>`\n\n    Parameters\n    ----------\n    posterior : str or list of str, optional\n        List of paths to output.csv files.\n    posterior_predictive : str or list of str, optional\n        Posterior predictive samples for the fit. If endswith \".csv\" assumes file.\n    predictions : str or list of str, optional\n        Out of sample predictions samples for the fit. If endswith \".csv\" assumes file.\n    prior : str or list of str, optional\n        List of paths to output.csv files\n    prior_predictive : str or list of str, optional\n        Prior predictive samples for the fit. If endswith \".csv\" assumes file.\n    observed_data : str, optional\n        Observed data used in the sampling. Path to data file in Rdump or JSON format.\n    observed_data_var : str or list of str, optional\n        Variable(s) used for slicing observed_data. If not defined, all\n        data variables are imported.\n    constant_data : str, optional\n        Constant data used in the sampling. Path to data file in Rdump or JSON format.\n    constant_data_var : str or list of str, optional\n        Variable(s) used for slicing constant_data. If not defined, all\n        data variables are imported.\n    predictions_constant_data : str, optional\n        Constant data for predictions used in the sampling.\n        Path to data file in Rdump or JSON format.\n    predictions_constant_data_var : str or list of str, optional\n        Variable(s) used for slicing predictions_constant_data.\n        If not defined, all data variables are imported.\n    log_likelihood : dict of {str: str}, list of str or str, optional\n        Pointwise log_likelihood for the data. log_likelihood is extracted from the\n        posterior. It is recommended to use this argument as a dictionary whose keys\n        are observed variable names and its values are the variables storing log\n        likelihood arrays in the Stan code. In other cases, a dictionary with keys\n        equal to its values is used. By default, if a variable ``log_lik`` is\n        present in the Stan model, it will be retrieved as pointwise log\n        likelihood values. Use ``False`` to avoid this behaviour.\n    index_origin : int, optional\n        Starting value of integer coordinate values. Defaults to the value in rcParam\n        ``data.index_origin``.\n    coords : dict of {str: array_like}, optional\n        A dictionary containing the values that are used as index. The key\n        is the name of the dimension, the values are the index values.\n    dims : dict of {str: list of str}, optional\n        A mapping from variables to a list of coordinate names for the variable.\n    disable_glob : bool\n        Don't use glob for string input. This means that all string input is\n        assumed to be variable names (samples) or a path (data).\n    save_warmup : bool\n        Save warmup iterations into InferenceData object, if found in the input files.\n        If not defined, use default defined by the rcParams.\n    dtypes : dict or str\n        A dictionary containing dtype information (int, float) for parameters.\n        If input is a string, it is assumed to be a model code or path to model code file.\n\n    Returns\n    -------\n    InferenceData object\n     source"},{"id":16,"pagetitle":"Data","title":"ArviZ.from_mcmcchains","ref":"/ArviZ/stable/api/data/#ArviZ.from_mcmcchains","content":" ArviZ.from_mcmcchains  —  Function from_mcmcchains(posterior::MCMCChains.Chains; kwargs...) -> InferenceData\nfrom_mcmcchains(; kwargs...) -> InferenceData\nfrom_mcmcchains(\n    posterior::MCMCChains.Chains,\n    posterior_predictive,\n    predictions,\n    log_likelihood;\n    kwargs...\n) -> InferenceData Convert data in an  MCMCChains.Chains  format into an  InferenceData . Any keyword argument below without an an explicitly annotated type above is allowed, so long as it can be passed to  convert_to_inference_data . Arguments posterior::MCMCChains.Chains : Draws from the posterior Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution or   name(s) of predictive variables in  posterior predictions : Out-of-sample predictions for the posterior. prior : Draws from the prior prior_predictive : Draws from the prior predictive distribution or name(s) of predictive   variables in  prior observed_data : Observed data on which the  posterior  is conditional. It should only   contain data which is modeled as a random variable. Keys are parameter names and values. constant_data : Model constants, data included in the model that are not modeled as   random variables. Keys are parameter names. predictions_constant_data : Constants relevant to the model predictions (i.e. new  x    values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this   argument as a named tuple whose keys are observed variable names and whose values are log   likelihood arrays. Alternatively, provide the name of variable in  posterior  containing   log likelihoods. library=MCMCChains : Name of library that generated the chains coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions eltypes : Map from variable names to eltypes. This is primarily used to assign discrete   eltypes to discrete variables that were stored in  Chains  as floats. Returns InferenceData : The data with groups corresponding to the provided data source"},{"id":17,"pagetitle":"Data","title":"ArviZ.from_samplechains","ref":"/ArviZ/stable/api/data/#ArviZ.from_samplechains","content":" ArviZ.from_samplechains  —  Function from_samplechains(\n    posterior=nothing;\n    prior=nothing,\n    library=SampleChains,\n    kwargs...,\n) -> InferenceData Convert SampleChains samples to an  InferenceData . Either  posterior  or  prior  may be a  SampleChains.AbstractChain  or  SampleChains.MultiChain  object. For descriptions of remaining  kwargs , see  from_namedtuple . source"},{"id":18,"pagetitle":"Data","title":"IO / Conversion","ref":"/ArviZ/stable/api/data/#IO-/-Conversion","content":" IO / Conversion"},{"id":19,"pagetitle":"Data","title":"ArviZ.from_dict","ref":"/ArviZ/stable/api/data/#ArviZ.from_dict","content":" ArviZ.from_dict  —  Function Convert Dictionary data into an InferenceData object. Note This function is forwarded to Python's  arviz.from_dict . The docstring of that function is included below. \n    For a usage example read the\n    :ref:`Creating InferenceData section on from_dict <creating_InferenceData>`\n\n    Parameters\n    ----------\n    posterior : dict, optional\n    posterior_predictive : dict, optional\n    predictions: dict, optional\n    sample_stats : dict, optional\n    log_likelihood : dict, optional\n        For stats functions, log likelihood data should be stored here.\n    prior : dict, optional\n    prior_predictive : dict, optional\n    observed_data : dict, optional\n    constant_data : dict, optional\n    predictions_constant_data: dict, optional\n    warmup_posterior : dict, optional\n    warmup_posterior_predictive : dict, optional\n    warmup_predictions : dict, optional\n    warmup_log_likelihood : dict, optional\n    warmup_sample_stats : dict, optional\n    save_warmup : bool, optional\n        Save warmup iterations InferenceData object. If not defined, use default\n        defined by the rcParams.\n    index_origin : int, optional\n    coords : dict of {str : list}, optional\n        A dictionary containing the values that are used as index. The key\n        is the name of the dimension, the values are the index values.\n    dims : dict of {str : list of str}, optional\n        A mapping from variables to a list of coordinate names for the variable.\n    pred_dims : dict of {str : list of str}, optional\n        A mapping from variables to a list of coordinate names for predictions.\n    pred_coords : dict of {str : list}, optional\n        A mapping from variables to a list of coordinate values for predictions.\n    attrs : dict, optional\n        A dictionary containing attributes for different groups.\n    kwargs : dict, optional\n        A dictionary containing group attrs. Accepted kwargs are:\n\n        - posterior_attrs, posterior_warmup_attrs : attrs for posterior group\n        - sample_stats_attrs, sample_stats_warmup_attrs : attrs for sample_stats group\n        - log_likelihood_attrs, log_likelihood_warmup_attrs : attrs for log_likelihood group\n        - posterior_predictive_attrs, posterior_predictive_warmup_attrs : attrs for\n          posterior_predictive group\n        - predictions_attrs, predictions_warmup_attrs : attrs for predictions group\n        - prior_attrs : attrs for prior group\n        - sample_stats_prior_attrs : attrs for sample_stats_prior group\n        - prior_predictive_attrs : attrs for prior_predictive group\n\n    Returns\n    -------\n    InferenceData\n     source"},{"id":20,"pagetitle":"Data","title":"ArviZ.from_json","ref":"/ArviZ/stable/api/data/#ArviZ.from_json","content":" ArviZ.from_json  —  Function Initialize object from a json file. Note This function is forwarded to Python's  arviz.from_json . The docstring of that function is included below. \n    Will use the faster `ujson` (https://github.com/ultrajson/ultrajson) if it is available.\n\n    Parameters\n    ----------\n    filename : str\n        location of json file\n\n    Returns\n    -------\n    InferenceData object\n     source"},{"id":21,"pagetitle":"Data","title":"ArviZ.from_netcdf","ref":"/ArviZ/stable/api/data/#ArviZ.from_netcdf","content":" ArviZ.from_netcdf  —  Function Load netcdf file back into an arviz.InferenceData. Note This function is forwarded to Python's  arviz.from_netcdf . The docstring of that function is included below. \n    Parameters\n    ----------\n    filename : str\n        name or path of the file to load trace\n    group_kwargs : dict of {str: dict}\n        Keyword arguments to be passed into each call of :func:`xarray.open_dataset`.\n        The keys of the higher level should be group names or regex matching group\n        names, the inner dicts re passed to ``open_dataset``.\n        This feature is currently experimental\n    regex : str\n        Specifies where regex search should be used to extend the keyword arguments.\n\n    Returns\n    -------\n        InferenceData object\n\n    Notes\n    -----\n    By default, the datasets of the InferenceData object will be lazily loaded instead\n    of loaded into memory. This behaviour is regulated by the value of\n    ``az.rcParams[\"data.load\"]``.\n     source"},{"id":22,"pagetitle":"Data","title":"ArviZ.to_netcdf","ref":"/ArviZ/stable/api/data/#ArviZ.to_netcdf","content":" ArviZ.to_netcdf  —  Function Save dataset as a netcdf file. Note This function is forwarded to Python's  arviz.to_netcdf . The docstring of that function is included below. \n    WARNING: Only idempotent in case `data` is InferenceData\n\n    Parameters\n    ----------\n    data : InferenceData, or any object accepted by `convert_to_inference_data`\n        Object to be saved\n    filename : str\n        name or path of the file to load trace\n    group : str (optional)\n        In case `data` is not InferenceData, this is the group it will be saved to\n    coords : dict (optional)\n        See `convert_to_inference_data`\n    dims : dict (optional)\n        See `convert_to_inference_data`\n\n    Returns\n    -------\n    str\n        filename saved to\n     source"},{"id":23,"pagetitle":"Data","title":"General functions","ref":"/ArviZ/stable/api/data/#General-functions","content":" General functions"},{"id":24,"pagetitle":"Data","title":"ArviZ.concat","ref":"/ArviZ/stable/api/data/#ArviZ.concat","content":" ArviZ.concat  —  Function Concatenate InferenceData objects. Note This function is forwarded to Python's  arviz.concat . The docstring of that function is included below. \n    Concatenates over `group`, `chain` or `draw`.\n    By default concatenates over unique groups.\n    To concatenate over `chain` or `draw` function\n    needs identical groups and variables.\n\n    The `variables` in the `data` -group are merged if `dim` are not found.\n\n\n    Parameters\n    ----------\n    *args : InferenceData\n        Variable length InferenceData list or\n        Sequence of InferenceData.\n    dim : str, optional\n        If defined, concatenated over the defined dimension.\n        Dimension which is concatenated. If None, concatenates over\n        unique groups.\n    copy : bool\n        If True, groups are copied to the new InferenceData object.\n        Used only if `dim` is None.\n    inplace : bool\n        If True, merge args to first object.\n    reset_dim : bool\n        Valid only if dim is not None.\n\n    Returns\n    -------\n    InferenceData\n        A new InferenceData object by default.\n        When `inplace==True` merge args to first arg and return `None`\n\n    See Also\n    --------\n    add_groups : Add new groups to InferenceData object.\n    extend : Extend InferenceData with groups from another InferenceData.\n\n    Examples\n    --------\n    Use ``concat`` method to concatenate InferenceData objects. This will concatenates over\n    unique groups by default. We first create an ``InferenceData`` object:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: import numpy as np\n           ...: data = {\n           ...:     \"a\": np.random.normal(size=(4, 100, 3)),\n           ...:     \"b\": np.random.normal(size=(4, 100)),\n           ...: }\n           ...: coords = {\"a_dim\": [\"x\", \"y\", \"z\"]}\n           ...: dataA = az.from_dict(data, coords=coords, dims={\"a\": [\"a_dim\"]})\n           ...: dataA\n\n    We have created an ``InferenceData`` object with default group 'posterior'. Now, we will\n    create another ``InferenceData`` object:\n\n    .. ipython::\n\n        In [1]: dataB = az.from_dict(prior=data, coords=coords, dims={\"a\": [\"a_dim\"]})\n           ...: dataB\n\n    We have created another ``InferenceData`` object with group 'prior'. Now, we will concatenate\n    these two ``InferenceData`` objects:\n\n    .. ipython::\n\n        In [1]: az.concat(dataA, dataB)\n\n    Now, we will concatenate over chain (or draw). It requires identical groups and variables.\n    Here we are concatenating two identical ``InferenceData`` objects over dimension chain:\n\n    .. ipython::\n\n        In [1]: az.concat(dataA, dataA, dim=\"chain\")\n\n    It will create an ``InferenceData`` with the original group 'posterior'. In similar way,\n    we can also concatenate over draws.\n\n     source"},{"id":25,"pagetitle":"Data","title":"ArviZ.extract","ref":"/ArviZ/stable/api/data/#ArviZ.extract","content":" ArviZ.extract  —  Function Extract an InferenceData group or subset of it. Note This function is forwarded to Python's  arviz.extract . The docstring of that function is included below. \n    Parameters\n    ----------\n    idata : InferenceData or InferenceData_like\n        InferenceData from which to extract the data.\n    group : str, optional\n        Which InferenceData data group to extract data from.\n    combined : bool, optional\n        Combine ``chain`` and ``draw`` dimensions into ``sample``. Won't work if\n        a dimension named ``sample`` already exists.\n    var_names : str or list of str, optional\n        Variables to be extracted. Prefix the variables by `~` when you want to exclude them.\n    filter_vars: {None, \"like\", \"regex\"}, optional\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        `pandas.filter`.\n        Like with plotting, sometimes it's easier to subset saying what to exclude\n        instead of what to include\n    num_samples : int, optional\n        Extract only a subset of the samples. Only valid if ``combined=True``\n    keep_dataset : bool, optional\n        If true, always return a DataSet. If false (default) return a DataArray\n        when there is a single variable.\n    rng : bool, int, numpy.Generator, optional\n        Shuffle the samples, only valid if ``combined=True``. By default,\n        samples are shuffled if ``num_samples`` is not ``None``, and are left\n        in the same order otherwise. This ensures that subsetting the samples doesn't return\n        only samples from a single chain and consecutive draws.\n\n    Returns\n    -------\n    xarray.DataArray or xarray.Dataset\n\n    Examples\n    --------\n    The default behaviour is to return the posterior group after stacking the chain and\n    draw dimensions.\n\n    .. jupyter-execute::\n\n        import arviz as az\n        idata = az.load_arviz_data(\"centered_eight\")\n        az.extract(idata)\n\n    You can also indicate a subset to be returned, but in variables and in samples:\n\n    .. jupyter-execute::\n\n        az.extract(idata, var_names=\"theta\", num_samples=100)\n\n    To keep the chain and draw dimensions, use ``combined=False``.\n\n    .. jupyter-execute::\n\n        az.extract(idata, group=\"prior\", combined=False)\n\n     source"},{"id":26,"pagetitle":"Data","title":"Example data","ref":"/ArviZ/stable/api/data/#Example-data","content":" Example data"},{"id":27,"pagetitle":"Data","title":"ArviZ.load_example_data","ref":"/ArviZ/stable/api/data/#ArviZ.load_example_data","content":" ArviZ.load_example_data  —  Function load_example_data(name; kwargs...) -> InferenceData\nload_example_data() -> Dict{String,AbstractFileMetadata} Load a local or remote pre-made dataset. kwargs  are forwarded to  from_netcdf . Pass no parameters to get a  Dict  listing all available datasets. Data files are handled by DataDeps.jl. A file is downloaded only when it is requested and then cached for future use. Examples julia> using ArviZ\n\njulia> keys(load_example_data())\nKeySet for a Dict{String, ArviZ.AbstractFileMetadata} with 9 entries. Keys:\n  \"centered_eight\"\n  \"radon\"\n  \"glycan_torsion_angles\"\n  \"rugby\"\n  \"non_centered_eight\"\n  \"classification1d\"\n  \"regression10d\"\n  \"classification10d\"\n  \"regression1d\"\n\njulia> load_example_data(\"centered_eight\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data source"},{"id":30,"pagetitle":"Dataset","title":"Dataset","ref":"/ArviZ/stable/api/dataset/#dataset-api","content":" Dataset InferenceObjects.Dataset InferenceObjects.convert_to_dataset InferenceObjects.namedtuple_to_dataset"},{"id":31,"pagetitle":"Dataset","title":"Type definition","ref":"/ArviZ/stable/api/dataset/#Type-definition","content":" Type definition"},{"id":32,"pagetitle":"Dataset","title":"InferenceObjects.Dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.Dataset","content":" InferenceObjects.Dataset  —  Type Dataset{L} <: DimensionalData.AbstractDimStack{L} Container of dimensional arrays sharing some dimensions. This type is an  DimensionalData.AbstractDimStack  that implements the same interface as  DimensionalData.DimStack  and has identical usage. When a  Dataset  is passed to Python, it is converted to an  xarray.Dataset  without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an  xarray.Dataset  is passed to Julia, its data must be copied. Constructors Dataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n) In most cases, use  convert_to_dataset  to create a  Dataset  instead of directly using a constructor."},{"id":33,"pagetitle":"Dataset","title":"General conversion","ref":"/ArviZ/stable/api/dataset/#General-conversion","content":" General conversion"},{"id":34,"pagetitle":"Dataset","title":"InferenceObjects.convert_to_dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.convert_to_dataset","content":" InferenceObjects.convert_to_dataset  —  Function convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset Convert a supported object to a  Dataset . In most cases, this function calls  convert_to_inference_data  and returns the corresponding  group ."},{"id":35,"pagetitle":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.namedtuple_to_dataset","content":" InferenceObjects.namedtuple_to_dataset  —  Function namedtuple_to_dataset(data; kwargs...) -> Dataset Convert  NamedTuple  mapping variable names to arrays to a  Dataset . Keywords attrs : a Symbol-indexable collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable. library::Union{String,Module} : library used for performing inference. Will be attached to the  attrs  metadata. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated."},{"id":36,"pagetitle":"Dataset","title":"DimensionalData","ref":"/ArviZ/stable/api/dataset/#DimensionalData","content":" DimensionalData As a  DimensionalData.AbstractDimStack ,  Dataset  also implements the  AbstractDimStack  API and can be used like a  DimStack . See  DimensionalData's documentation  for example usage."},{"id":37,"pagetitle":"Dataset","title":"Tables inteface","ref":"/ArviZ/stable/api/dataset/#Tables-inteface","content":" Tables inteface Dataset  implements the  Tables  interface. This allows  Dataset s to be used as sources for any function that can accept a table. For example, it's straightforward to: write to CSV with CSV.jl flatten to a DataFrame with DataFrames.jl plot with StatsPlots.jl plot with AlgebraOfGraphics.jl"},{"id":40,"pagetitle":"Diagnostics","title":"Diagnostics","ref":"/ArviZ/stable/api/diagnostics/#diagnostics-api","content":" Diagnostics ArviZ.bfmi ArviZ.ess ArviZ.mcse ArviZ.rhat"},{"id":41,"pagetitle":"Diagnostics","title":"Reference","ref":"/ArviZ/stable/api/diagnostics/#Reference","content":" Reference"},{"id":42,"pagetitle":"Diagnostics","title":"ArviZ.bfmi","ref":"/ArviZ/stable/api/diagnostics/#ArviZ.bfmi","content":" ArviZ.bfmi  —  Function Calculate the estimated Bayesian fraction of missing information (BFMI). Note This function is forwarded to Python's  arviz.bfmi . The docstring of that function is included below. \n    BFMI quantifies how well momentum resampling matches the marginal energy distribution. For more\n    information on BFMI, see https://arxiv.org/pdf/1604.00695v1.pdf. The current advice is that\n    values smaller than 0.3 indicate poor sampling. However, this threshold is\n    provisional and may change. See\n    `pystan_workflow <http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html>`_\n    for more information.\n\n    Parameters\n    ----------\n    data : obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n        If InferenceData, energy variable needs to be found.\n\n    Returns\n    -------\n    z : array\n        The Bayesian fraction of missing information of the model and trace. One element per\n        chain in the trace.\n\n    See Also\n    --------\n    plot_energy : Plot energy transition distribution and marginal energy\n                  distribution in HMC algorithms.\n\n    Examples\n    --------\n    Compute the BFMI of an InferenceData object\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data('radon')\n           ...: az.bfmi(data)\n\n     source"},{"id":43,"pagetitle":"Diagnostics","title":"ArviZ.ess","ref":"/ArviZ/stable/api/diagnostics/#ArviZ.ess","content":" ArviZ.ess  —  Function Calculate estimate of the effective sample size (ess). Note This function is forwarded to Python's  arviz.ess . The docstring of that function is included below. \n    Parameters\n    ----------\n    data : obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n        For ndarray: shape = (chain, draw).\n        For n-dimensional ndarray transform first to dataset with :func:`arviz.convert_to_dataset`.\n    var_names : str or list of str\n        Names of variables to include in the return value Dataset.\n    method : str, optional, default \"bulk\"\n        Select ess method. Valid methods are:\n\n        - \"bulk\"\n        - \"tail\"     # prob, optional\n        - \"quantile\" # prob\n        - \"mean\" (old ess)\n        - \"sd\"\n        - \"median\"\n        - \"mad\" (mean absolute deviance)\n        - \"z_scale\"\n        - \"folded\"\n        - \"identity\"\n        - \"local\"\n    relative : bool\n        Return relative ess\n        ``ress = ess / n``\n    prob : float, or tuple of two floats, optional\n        probability value for \"tail\", \"quantile\" or \"local\" ess functions.\n    dask_kwargs : dict, optional\n        Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.\n\n    Returns\n    -------\n    xarray.Dataset\n        Return the effective sample size, :math:`\\hat{N}_{eff}`\n\n    Notes\n    -----\n    The basic ess (:math:`N_{\\mathit{eff}}`) diagnostic is computed by:\n\n    .. math:: \\hat{N}_{\\mathit{eff}} = \\frac{MN}{\\hat{\\tau}}\n\n    .. math:: \\hat{\\tau} = -1 + 2 \\sum_{t'=0}^K \\hat{P}_{t'}\n\n    where :math:`M` is the number of chains, :math:`N` the number of draws,\n    :math:`\\hat{\\rho}_t` is the estimated _autocorrelation at lag :math:`t`, and\n    :math:`K` is the last integer for which :math:`\\hat{P}_{K} = \\hat{\\rho}_{2K} +\n    \\hat{\\rho}_{2K+1}` is still positive.\n\n    The current implementation is similar to Stan, which uses Geyer's initial monotone sequence\n    criterion (Geyer, 1992; Geyer, 2011).\n\n    References\n    ----------\n    * Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n    * https://mc-stan.org/docs/2_18/reference-manual/effective-sample-size-section.html\n      Section 15.4.2\n    * Gelman et al. BDA (2014) Formula 11.8\n\n    See Also\n    --------\n    arviz.rhat : Compute estimate of rank normalized splitR-hat for a set of traces.\n    arviz.mcse : Calculate Markov Chain Standard Error statistic.\n    plot_ess : Plot quantile, local or evolution of effective sample sizes (ESS).\n    arviz.summary : Create a data frame with summary statistics.\n\n    Examples\n    --------\n    Calculate the effective_sample_size using the default arguments:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data('non_centered_eight')\n           ...: az.ess(data)\n\n    Calculate the ress of some of the variables\n\n    .. ipython::\n\n        In [1]: az.ess(data, relative=True, var_names=[\"mu\", \"theta_t\"])\n\n    Calculate the ess using the \"tail\" method, leaving the `prob` argument at its default\n    value.\n\n    .. ipython::\n\n        In [1]: az.ess(data, method=\"tail\")\n\n     source"},{"id":44,"pagetitle":"Diagnostics","title":"ArviZ.mcse","ref":"/ArviZ/stable/api/diagnostics/#ArviZ.mcse","content":" ArviZ.mcse  —  Function Calculate Markov Chain Standard Error statistic. Note This function is forwarded to Python's  arviz.mcse . The docstring of that function is included below. \n    Parameters\n    ----------\n    data : obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n        For ndarray: shape = (chain, draw).\n        For n-dimensional ndarray transform first to dataset with ``az.convert_to_dataset``.\n    var_names : list\n        Names of variables to include in the rhat report\n    method : str\n        Select mcse method. Valid methods are:\n        - \"mean\"\n        - \"sd\"\n        - \"median\"\n        - \"quantile\"\n\n    prob : float\n        Quantile information.\n    dask_kwargs : dict, optional\n        Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.\n\n    Returns\n    -------\n    xarray.Dataset\n        Return the msce dataset\n\n    See Also\n    --------\n    ess : Compute autocovariance estimates for every lag for the input array.\n    summary : Create a data frame with summary statistics.\n    plot_mcse : Plot quantile or local Monte Carlo Standard Error.\n\n    Examples\n    --------\n    Calculate the Markov Chain Standard Error using the default arguments:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data(\"non_centered_eight\")\n           ...: az.mcse(data)\n\n    Calculate the Markov Chain Standard Error using the quantile method:\n\n    .. ipython::\n\n        In [1]: az.mcse(data, method=\"quantile\", prob=0.7)\n\n     source"},{"id":45,"pagetitle":"Diagnostics","title":"ArviZ.rhat","ref":"/ArviZ/stable/api/diagnostics/#ArviZ.rhat","content":" ArviZ.rhat  —  Function Compute estimate of rank normalized splitR-hat for a set of traces. Note This function is forwarded to Python's  arviz.rhat . The docstring of that function is included below. \n    The rank normalized R-hat diagnostic tests for lack of convergence by comparing the variance\n    between multiple chains to the variance within each chain. If convergence has been achieved,\n    the between-chain and within-chain variances should be identical. To be most effective in\n    detecting evidence for nonconvergence, each chain should have been initialized to starting\n    values that are dispersed relative to the target distribution.\n\n    Parameters\n    ----------\n    data : obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n        At least 2 posterior chains are needed to compute this diagnostic of one or more\n        stochastic parameters.\n        For ndarray: shape = (chain, draw).\n        For n-dimensional ndarray transform first to dataset with ``az.convert_to_dataset``.\n    var_names : list\n        Names of variables to include in the rhat report\n    method : str\n        Select R-hat method. Valid methods are:\n        - \"rank\"        # recommended by Vehtari et al. (2019)\n        - \"split\"\n        - \"folded\"\n        - \"z_scale\"\n        - \"identity\"\n    dask_kwargs : dict, optional\n        Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.\n\n    Returns\n    -------\n    xarray.Dataset\n      Returns dataset of the potential scale reduction factors, :math:`\\hat{R}`\n\n    See Also\n    --------\n    ess : Calculate estimate of the effective sample size (ess).\n    mcse : Calculate Markov Chain Standard Error statistic.\n    plot_forest : Forest plot to compare HDI intervals from a number of distributions.\n\n    Notes\n    -----\n    The diagnostic is computed by:\n\n      .. math:: \\hat{R} = \\frac{\\hat{V}}{W}\n\n    where :math:`W` is the within-chain variance and :math:`\\hat{V}` is the posterior variance\n    estimate for the pooled rank-traces. This is the potential scale reduction factor, which\n    converges to unity when each of the traces is a sample from the target posterior. Values\n    greater than one indicate that one or more chains have not yet converged.\n\n    Rank values are calculated over all the chains with ``scipy.stats.rankdata``.\n    Each chain is split in two and normalized with the z-transform following Vehtari et al. (2019).\n\n    References\n    ----------\n    * Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n    * Gelman et al. BDA (2014)\n    * Brooks and Gelman (1998)\n    * Gelman and Rubin (1992)\n\n    Examples\n    --------\n    Calculate the R-hat using the default arguments:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data(\"non_centered_eight\")\n           ...: az.rhat(data)\n\n    Calculate the R-hat of some variables using the folded method:\n\n    .. ipython::\n\n        In [1]: az.rhat(data, var_names=[\"mu\", \"theta_t\"], method=\"folded\")\n\n     source"},{"id":48,"pagetitle":"InferenceData","title":"InferenceData","ref":"/ArviZ/stable/api/inference_data/#inferencedata-api","content":" InferenceData InferenceObjects.InferenceData Base.getindex Base.getproperty Base.merge Base.propertynames Base.setindex InferenceObjects.convert_to_inference_data InferenceObjects.from_namedtuple"},{"id":49,"pagetitle":"InferenceData","title":"Type definition","ref":"/ArviZ/stable/api/inference_data/#Type-definition","content":" Type definition"},{"id":50,"pagetitle":"InferenceData","title":"InferenceObjects.InferenceData","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.InferenceData","content":" InferenceObjects.InferenceData  —  Type InferenceData{group_names,group_types} Container for inference data storage using DimensionalData. This object implements the  InferenceData schema . Internally, groups are stored in a  NamedTuple , which can be accessed using  parent(::InferenceData) . Constructors InferenceData(groups::NamedTuple)\nInferenceData(; groups...) Construct an inference data from either a  NamedTuple  or keyword arguments of groups. Groups must be  Dataset  objects. Instead of directly creating an  InferenceData , use the exported  from_xyz  functions or  convert_to_inference_data ."},{"id":51,"pagetitle":"InferenceData","title":"Property interface","ref":"/ArviZ/stable/api/inference_data/#Property-interface","content":" Property interface"},{"id":52,"pagetitle":"InferenceData","title":"Base.getproperty","ref":"/ArviZ/stable/api/inference_data/#Base.getproperty","content":" Base.getproperty  —  Function getproperty(data::InferenceData, name::Symbol) -> Dataset Get group with the specified  name ."},{"id":53,"pagetitle":"InferenceData","title":"Base.propertynames","ref":"/ArviZ/stable/api/inference_data/#Base.propertynames","content":" Base.propertynames  —  Function propertynames(data::InferenceData) -> Tuple{Symbol} Get names of groups"},{"id":54,"pagetitle":"InferenceData","title":"Indexing interface","ref":"/ArviZ/stable/api/inference_data/#Indexing-interface","content":" Indexing interface"},{"id":55,"pagetitle":"InferenceData","title":"Base.getindex","ref":"/ArviZ/stable/api/inference_data/#Base.getindex","content":" Base.getindex  —  Function Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData Return a new  InferenceData  containing the specified groups sliced to the specified coords. coords  specifies a dimension name mapping to an index, a  DimensionalData.Selector , or an  IntervalSets.AbstractInterval . If one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised. Examples Select data from all groups for just the specified id values. julia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 1 entry:\n  :created_at => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 1 entry:\n  :created_at => \"2022-08-11T11:15:21.4\" Select data from just the posterior, returning a  Dataset  if the indices index more than one element from any of the variables: julia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 1 entry:\n  :created_at => \"2022-08-11T11:19:25.982\" Note that if a single index is provided, the behavior is still to slice so that the dimension is preserved."},{"id":56,"pagetitle":"InferenceData","title":"Base.setindex","ref":"/ArviZ/stable/api/inference_data/#Base.setindex","content":" Base.setindex  —  Function Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData Create a new  InferenceData  containing the  group  with the specified  name . If a group with  name  is already in  data , it is replaced."},{"id":57,"pagetitle":"InferenceData","title":"Iteration interface","ref":"/ArviZ/stable/api/inference_data/#Iteration-interface","content":" Iteration interface InferenceData  also implements the same iteration interface as its underlying  NamedTuple . That is, iterating over an  InferenceData  iterates over its groups."},{"id":58,"pagetitle":"InferenceData","title":"General conversion","ref":"/ArviZ/stable/api/inference_data/#General-conversion","content":" General conversion"},{"id":59,"pagetitle":"InferenceData","title":"InferenceObjects.convert_to_inference_data","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.convert_to_inference_data","content":" InferenceObjects.convert_to_inference_data  —  Function convert_to_inference_data(obj; group, kwargs...) -> InferenceData Convert a supported object to an  InferenceData  object. If  obj  converts to a single dataset,  group  specifies which dataset in the resulting  InferenceData  that is. See  convert_to_dataset Arguments obj  can be many objects. Basic supported types are: InferenceData : return unchanged Dataset / DimensionalData.AbstractDimStack : add to  InferenceData  as the only group NamedTuple / AbstractDict : create a  Dataset  as the only group AbstractArray{<:Real} : create a  Dataset  as the only group, given an arbitrary name, if the name is not set More specific types may be documented separately. Keywords group::Symbol = :posterior : If  obj  converts to a single dataset, assign the resulting dataset to this group. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. kwargs : remaining keywords forwarded to converter functions"},{"id":60,"pagetitle":"InferenceData","title":"InferenceObjects.from_namedtuple","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.from_namedtuple","content":" InferenceObjects.from_namedtuple  —  Function from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{<:NamedTuple}; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Matrix{<:NamedTuple}; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData Convert a  NamedTuple  or container of  NamedTuple s to an  InferenceData . If containers are passed, they are flattened into a single  NamedTuple  with array elements whose first dimensions correspond to the dimensions of the containers. Arguments posterior : The data to be converted. It may be of the following types: ::NamedTuple : The keys are the variable names and the values are arrays with dimensions  (nchains, ndraws, sizes...) . ::Matrix{<:NamedTuple} : Each element is a single draw from a single chain, with array/scalar values with dimensions  sizes . The dimensions of the matrix container are  (nchains, ndraws) ::Vector{Vector{<:NamedTuple}} : The same as the above case. Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior::Any=nothing : Draws from the prior prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Examples using InferenceObjects\nnchains, ndraws = 2, 10\n\ndata1 = (\n    x=rand(nchains, ndraws), y=randn(nchains, ndraws, 2), z=randn(nchains, ndraws, 3, 2)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [(x=rand(ndraws), y=randn(ndraws, 2), z=randn(ndraws, 3, 2)) for _ in 1:nchains];\nidata2 = from_namedtuple(data2)\n\ndata3 = [(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:nchains, _ in 1:ndraws];\nidata3 = from_namedtuple(data3)\n\ndata4 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata4 = from_namedtuple(data4)"},{"id":61,"pagetitle":"InferenceData","title":"General functions","ref":"/ArviZ/stable/api/inference_data/#General-functions","content":" General functions"},{"id":62,"pagetitle":"InferenceData","title":"Base.merge","ref":"/ArviZ/stable/api/inference_data/#Base.merge","content":" Base.merge  —  Function merge(data::InferenceData, others::InferenceData...) -> InferenceData Merge  InferenceData  objects. The result contains all groups in  data  and  others . If a group appears more than once, the one that occurs first is kept."},{"id":65,"pagetitle":"Plots","title":"Plots","ref":"/ArviZ/stable/api/plots/#plots-api","content":" Plots ArviZ.plot_autocorr ArviZ.plot_bpv ArviZ.plot_compare ArviZ.plot_density ArviZ.plot_dist ArviZ.plot_dist_comparison ArviZ.plot_elpd ArviZ.plot_energy ArviZ.plot_ess ArviZ.plot_forest ArviZ.plot_hdi ArviZ.plot_kde ArviZ.plot_khat ArviZ.plot_loo_pit ArviZ.plot_mcse ArviZ.plot_pair ArviZ.plot_parallel ArviZ.plot_posterior ArviZ.plot_ppc ArviZ.plot_rank ArviZ.plot_separation ArviZ.plot_trace ArviZ.plot_violin"},{"id":66,"pagetitle":"Plots","title":"Reference","ref":"/ArviZ/stable/api/plots/#Reference","content":" Reference"},{"id":67,"pagetitle":"Plots","title":"ArviZ.plot_autocorr","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_autocorr","content":" ArviZ.plot_autocorr  —  Function Bar plot of the autocorrelation function for a sequence of data. Note This function is forwarded to Python's  arviz.plot_autocorr . The docstring of that function is included below. \n    Useful in particular for posteriors from MCMC samples which may display correlation.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variables are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot. Vector-value\n        stochastics are handled automatically.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    max_lag: int, optional\n        Maximum lag to calculate autocorrelation. Defaults to 100 or num draws,\n        whichever is smaller.\n    combined: bool, default=False\n        Flag for combining multiple chains into a single chain. If False, chains will be\n        plotted separately.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n        Note this is not used if ``ax`` is supplied.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in ``rcParams``.\n    backend_kwargs: dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    autocov : Compute autocovariance estimates for every lag for the input array.\n    autocorr : Compute autocorrelation using FFT for every lag for the input array.\n\n    Examples\n    --------\n    Plot default autocorrelation\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_autocorr(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'] )\n\n\n    Combine chains by variable and select variables by excluding some with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['~thet'], filter_vars=\"like\", combined=True)\n\n\n    Specify maximum lag (x axis bound)\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'], max_lag=200, combined=True)\n     source"},{"id":68,"pagetitle":"Plots","title":"ArviZ.plot_bpv","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_bpv","content":" ArviZ.plot_bpv  —  Function Note This function is forwarded to Python's  arviz.plot_bpv . The docstring of that function is included below.     Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n\n    Parameters\n    ----------\n    data : az.InferenceData object\n        :class:`arviz.InferenceData` object containing the observed and\n        posterior/prior predictive data.\n    kind : str\n        Type of plot to display (\"p_value\", \"u_value\", \"t_stat\"). Defaults to u_value.\n        For \"p_value\" we compute p := p(y* ≤ y | y). This is the probability of the data y being\n        larger or equal than the predicted data y*. The ideal value is 0.5 (half the predictions\n        below and half above the data).\n        For \"u_value\" we compute pi := p(yi* ≤ yi | y). i.e. like a p_value but per observation yi.\n        This is also known as marginal p_value. The ideal distribution is uniform. This is similar\n        to the LOO-pit calculation/plot, the difference is than in LOO-pit plot we compute\n        pi = p(yi* r ≤ yi | y-i ), where y-i, is all other data except yi.\n        For \"t_stat\" we compute := p(T(y)* ≤ T(y) | y) where T is any test statistic. See t_stat\n        argument below for details of available options.\n    t_stat : str, float, or callable\n        Test statistics to compute from the observations and predictive distributions.\n        Allowed strings are \"mean\", \"median\" or \"std\". Defaults to \"median\".\n        Alternative a quantile can be passed as a float (or str) in the\n        interval (0, 1). Finally a user defined function is also\n        acepted, see examples section for details.\n    bpv : bool\n        If True (default) add the Bayesian p_value to the legend when ``kind = t_stat``.\n    plot_mean : bool\n        Whether or not to plot the mean test statistic. Defaults to True.\n    reference : str\n        How to compute the distributions used as reference for u_values or p_values. Allowed values\n        are \"analytical\" (default) and \"samples\". Use `None` to do not plot any reference.\n        Defaults to \"samples\".\n    mse :bool\n        Show scaled mean square error between uniform distribution and marginal p_value\n        distribution. Defaults to False.\n    n_ref : int, optional\n        Number of reference distributions to sample when ``reference=samples``. Defaults to 100.\n    hdi_prob: float, optional\n        Probability for the highest density interval for the analytical reference distribution when\n        computing u_values. Should be in the interval (0, 1]. Defaults to\n        0.94.\n    color : str\n        Matplotlib color\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    textsize : float\n        Text size scaling factor for labels, titles and lines. If None it will be\n        autoscaled based on ``figsize``.\n    data_pairs : dict\n        Dictionary containing relations between observed data and posterior/prior predictive data.\n        Dictionary structure:\n\n        - key = data var_name\n        - value = posterior/prior predictive var_name\n\n        For example, ``data_pairs = {'y' : 'y_hat'}``\n        If None, it will assume that the observed data and the posterior/prior\n        predictive data have the same variable name.\n    labeller : labeller instance, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    var_names : list of variable names\n        Variables to be plotted, if `None` all variable are plotted. Prefix the variables by ``~``\n        when you want to exclude them from the plot.\n    filter_vars : {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords : dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None.\n    flatten : list\n        List of dimensions to flatten in observed_data. Only flattens across the coordinates\n        specified in the coords argument. Defaults to flattening all of the dimensions.\n    flatten_pp : list\n        List of dimensions to flatten in posterior_predictive/prior_predictive. Only flattens\n        across the coordinates specified in the coords argument. Defaults to flattening all\n        of the dimensions. Dimensions should match flatten excluding dimensions for data_pairs\n        parameters. If flatten is defined and flatten_pp is None, then ``flatten_pp=flatten``.\n    legend : bool\n        Add legend to figure. By default True.\n    ax : numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    plot_ref_kwargs :  dict, optional\n        Extra keyword arguments to control how reference is represented.\n        Passed to :meth:`matplotlib.axes.Axes.plot` or\n        :meth:`matplotlib.axes.Axes.axhspan` (when ``kind=u_value``\n        and ``reference=analytical``).\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    group : {\"prior\", \"posterior\"}, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n        Other value can be 'prior'.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_ppc : Plot for posterior/prior predictive checks.\n    plot_loo_pit : Plot Leave-One-Out probability integral transformation (PIT) predictive checks.\n    plot_dist_comparison : Plot to compare fitted and unfitted distributions.\n\n    References\n    ----------\n    * Gelman et al. (2013) see http://www.stat.columbia.edu/~gelman/book/ pages 151-153 for details\n\n    Examples\n    --------\n    Plot Bayesian p_values.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data(\"regression1d\")\n        >>> az.plot_bpv(data, kind=\"p_value\")\n\n    Plot custom test statistic comparison.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data(\"regression1d\")\n        >>> az.plot_bpv(data, kind=\"t_stat\", t_stat=lambda x:np.percentile(x, q=50, axis=-1))\n     source"},{"id":69,"pagetitle":"Plots","title":"ArviZ.plot_compare","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_compare","content":" ArviZ.plot_compare  —  Function Note This function is forwarded to Python's  arviz.plot_compare . The docstring of that function is included below.     Summary plot for model comparison.\n\n    Models are compared based on their expected log pointwise predictive density (ELPD),\n    the ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend LOO in line with the work presented by Vehtari et al. (2016) available\n    here: https://arxiv.org/abs/1507.04544.\n\n    This plot is in the style of the one used in the book Statistical Rethinking by\n    Richard McElreath.Chapter 6 in the first edition or 7 in the second.\n\n\n    Parameters\n    ----------\n    comp_df : pd.DataFrame\n        Result of the :func:`arviz.compare` method\n    insample_dev : bool, optional\n        Plot in-sample ELPD, that is the value of the information criteria without the\n        penalization given by the effective number of parameters (p_loo or p_waic).\n        Defaults to False\n    plot_standard_error : bool, optional\n        Plot the standard error of the ELPD. Defaults to True\n    plot_ic_diff : bool, optional\n        Plot standard error of the difference in ELPD between each model\n        and the top-ranked model. Defaults to True\n    order_by_rank : bool\n        If True (default) ensure the best model is used as reference.\n    legend : bool\n        Add legend to figure. By default True.\n    figsize : tuple, optional\n        If None, size is (6, num of models) inches\n    title : bool:\n        Show a tittle with a description of how to interpret the plot. Defaults to True.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    labeller : labeller instance, optional\n        Class providing the method ``model_name_to_str`` to generate the labels in\n        the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    plot_kwargs : dict, optional\n        Optional arguments for plot elements. Currently accepts 'color_ic',\n        'marker_ic', 'color_insample_dev', 'marker_insample_dev', 'color_dse',\n        'marker_dse', 'ls_min_ic' 'color_ls_min_ic',  'fontsize'\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_elpd : Plot pointwise elpd differences between two or more models.\n    compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n    waic : Compute the widely applicable information criterion.\n\n    Examples\n    --------\n    Show default compare plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> model_compare = az.compare({'Centered 8 schools': az.load_arviz_data('centered_eight'),\n        >>>                  'Non-centered 8 schools': az.load_arviz_data('non_centered_eight')})\n        >>> az.plot_compare(model_compare)\n\n    Include the in-sample ELDP\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_compare(model_compare, insample_dev=True)\n\n     source"},{"id":70,"pagetitle":"Plots","title":"ArviZ.plot_density","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_density","content":" ArviZ.plot_density  —  Function Generate KDE plots for continuous variables and histograms for discrete ones. Note This function is forwarded to Python's  arviz.plot_density . The docstring of that function is included below. \n    Plots are truncated at their 100*(1-alpha)% highest density intervals. Plots are grouped per\n    variable and colors assigned to models.\n\n    Parameters\n    ----------\n    data : Union[Object, Iterator[Object]]\n        Any object that can be converted to an :class:`arviz.InferenceData` object, or an Iterator\n        returning a sequence of such objects.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details about such objects.\n    group: Optional[str]\n        Specifies which :class:`arviz.InferenceData` group should be plotted.\n        Defaults to 'posterior'.\n        Alternative values include 'prior' and any other strings used as dataset keys in the\n        :class:`arviz.InferenceData`.\n    data_labels : Optional[List[str]]\n        List with names for the datasets passed as \"data.\" Useful when plotting more than one\n        dataset.  Must be the same shape as the data parameter.  Defaults to None.\n    var_names: Optional[List[str]]\n        List of variables to plot.  If multiple datasets are supplied and var_names is not None,\n        will print the same set of variables for each dataset.  Defaults to None, which results in\n        all the variables being plotted.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    transform : callable\n        Function to transform data (defaults to None i.e. the identity function)\n    hdi_prob : float\n        Probability for the highest density interval. Should be in the interval (0, 1].\n        Defaults to 0.94.\n    point_estimate : Optional[str]\n        Plot point estimate per variable. Values should be 'mean', 'median', 'mode' or None.\n        Defaults to 'auto' i.e. it falls back to default set in ``rcParams``.\n    colors : Optional[Union[List[str],str]]\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplotlib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    outline : bool\n        Use a line to draw KDEs and histograms. Default to True\n    hdi_markers : str\n        A valid `matplotlib.markers` like 'v', used to indicate the limits of the highest density\n        interval. Defaults to empty string (no marker).\n    shade : Optional[float]\n        Alpha blending value for the shaded area under the curve, between 0 (no shade) and 1\n        (opaque). Defaults to 0.\n    bw: Optional[float or str]\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when `circular` is False\n        and \"taylor\" (for now) when `circular` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    circular: Optional[bool]\n        If True, it interprets the values passed are from a circular variable measured in radians\n        and a circular KDE is used. Only valid for 1D KDE. Defaults to False.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize : Optional[Tuple[int, int]]\n        Figure size. If None it will be defined automatically.\n    textsize: Optional[float]\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n\n    Examples\n    --------\n    Plot default density plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> az.plot_density([centered, non_centered])\n\n    Plot variables in a 4x5 grid\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], grid=(4, 5))\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"])\n\n    Plot a specific `az.InferenceData` group\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], group=\"prior\")\n\n    Specify highest density interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], hdi_prob=.5)\n\n    Shade plots and/or remove outlines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], outline=False, shade=.8)\n\n    Specify binwidth for kernel density estimation\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], bw=.9)\n     source"},{"id":71,"pagetitle":"Plots","title":"ArviZ.plot_dist","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_dist","content":" ArviZ.plot_dist  —  Function Plot distribution as histogram or kernel density estimates. Note This function is forwarded to Python's  arviz.plot_dist . The docstring of that function is included below. \n    By default continuous variables are plotted using KDEs and discrete ones using histograms\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot.\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE or a hexbin will be estimated.\n    color : string\n        valid matplotlib color.\n    kind : string\n        By default (\"auto\") continuous variables will use the kind defined by rcParam\n        ``plot.density_kind`` and discrete ones will use histograms.\n        To override this use \"hist\" to plot histograms and \"kde\" for KDEs.\n    cumulative : bool\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE.\n    label : string\n        Text to include as part of the legend.\n    rotated : bool\n        Whether to rotate the 1D KDE plot 90 degrees.\n    rug : bool\n        If True adds a rugplot. Defaults to False. Ignored for 2D KDE.\n    bw: Optional[float or str]\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``is_circular`` is False\n        and \"taylor\" (for now) when ``is_circular`` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    quantiles : list\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n        Defaults to None.\n    contour : bool\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE. Defaults to True.\n    fill_last : bool\n        If True fill the last contour of the 2D KDE plot. Defaults to True.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``. Not implemented for bokeh backend.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE.\n        Passed to :func:`arviz.plot_kde` as ``plot_kwargs``.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use fill_kwargs={'alpha': 0} to disable fill).\n        Ignored for 2D KDE. Passed to :func:`arviz.plot_kde` as ``fill_kwargs``.\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if rug=False or for 2D KDE\n        Use ``space`` keyword (float) to control the position of the rugplot.\n        The larger this number the lower the rugplot. Passed to\n        :func:`arviz.plot_kde` as ``rug_kwargs``.\n    contour_kwargs : dict\n        Keywords passed to the contourplot. Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to :meth:`matplotlib.axes.Axes.contourf`. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to :meth:`matplotlib.axes.Axes.pcolormesh`. Ignored for 1D KDE.\n    hist_kwargs : dict\n        Keyword arguments used to customize the histogram. Ignored when plotting a KDE.\n        They are passed to :meth:`matplotlib.axes.Axes.hist` if using matplotlib,\n        or to :meth:`bokeh.plotting.Figure.quad` if using bokeh. In bokeh case,\n        the following extra keywords are also supported:\n\n        * ``color``: replaces the ``fill_color`` and ``line_color`` of the ``quad`` method\n        * ``bins``: taken from ``hist_kwargs`` and passed to :func:`numpy.histogram` instead\n        * ``density``: normalize histogram to represent a probability density function,\n          Defaults to ``True``\n\n        * ``cumulative``: plot the cumulative counts. Defaults to ``False``\n\n    is_circular : {False, True, \"radians\", \"degrees\"}. Default False.\n        Select input type {\"radians\", \"degrees\"} for circular histogram or KDE plot. If True,\n        default input type is \"radians\". When this argument is present, it interprets the\n        values passed are from a circular variable measured in radians and a circular KDE is\n        used. Inputs in \"degrees\" will undergo an internal conversion to radians. Only valid\n        for 1D KDE. Defaults to False.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    plot_kde : 1D or 2D KDE plot taking into account boundary conditions.\n\n    Examples\n    --------\n    Plot an integer distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> a = np.random.poisson(4, 1000)\n        >>> az.plot_dist(a)\n\n    Plot a continuous distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> b = np.random.normal(0, 1, 1000)\n        >>> az.plot_dist(b)\n\n    Add a rug under the Gaussian distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True)\n\n    Segment into quantiles\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True, quantiles=[.25, .5, .75])\n\n    Plot as the cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True, quantiles=[.25, .5, .75], cumulative=True)\n     source"},{"id":72,"pagetitle":"Plots","title":"ArviZ.plot_dist_comparison","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_dist_comparison","content":" ArviZ.plot_dist_comparison  —  Function Plot to compare fitted and unfitted distributions. Note This function is forwarded to Python's  arviz.plot_dist_comparison . The docstring of that function is included below. \n    The resulting plots will show the compared distributions both on\n    separate axes (particularly useful when one of them is substantially tighter\n    than another), and plotted together, so three plots per distribution\n\n    Parameters\n    ----------\n    data : InferenceData object\n        :class:`arviz.InferenceData` object containing the posterior/prior data.\n    kind : str\n        kind of plot to display {\"latent\", \"observed\"}, defaults to 'latent'.\n        \"latent\" includes {\"prior\", \"posterior\"} and \"observed\" includes\n        {\"observed_data\", \"prior_predictive\", \"posterior_predictive\"}\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be\n        autoscaled based on ``figsize``.\n    var_names : str, list, list of lists\n        if str, plot the variable. if list, plot all the variables in list\n        of all groups. if list of lists, plot the vars of groups in respective lists.\n    coords : dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n\n    transform : callable\n        Function to transform data (defaults to None i.e. the identity function)\n    legend : bool\n        Add legend to figure. By default True.\n    labeller : labeller instance, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes: The ax argument should have shape (nvars, 3), where the\n        last column is for the combined before/after plots and columns 0 and 1 are\n        for the before and after plots, respectively.\n    prior_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for prior/predictive groups.\n    posterior_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for posterior/predictive groups.\n    observed_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for observed_data group.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : a numpy 2D array of matplotlib axes. Returned object will have shape (nvars, 3),\n    where the last column is the combined plot and the first columns are the single plots.\n\n    See Also\n    --------\n    plot_bpv : Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n\n    Examples\n    --------\n    Plot the prior/posterior plot for specified vars and coords.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('rugby')\n        >>> az.plot_dist_comparison(data, var_names=[\"defs\"], coords={\"team\" : [\"Italy\"]})\n\n     source"},{"id":73,"pagetitle":"Plots","title":"ArviZ.plot_elpd","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_elpd","content":" ArviZ.plot_elpd  —  Function Note This function is forwarded to Python's  arviz.plot_elpd . The docstring of that function is included below.     Plot pointwise elpd differences between two or more models.\n\n    Parameters\n    ----------\n    compare_dict : mapping of {str : ELPDData or InferenceData}\n        A dictionary mapping the model name to the object containing inference data or the result\n        of :func:`arviz.loo` or :func:`arviz.waic` functions.\n        Refer to :func:`arviz.convert_to_inference_data` for details on possible dict items.\n    color : str or array_like, optional\n        Colors of the scatter plot. If color is a str all dots will have the same color.\n        If it is the size of the observations, each dot will have the specified color.\n        Otherwise, it will be interpreted as a list of the dims to be used for the color code.\n    xlabels : bool, optional\n        Use coords as xticklabels. Defaults to False.\n    figsize : figure size tuple, optional\n        If None, size is (8 + numvars, 8 + numvars).\n    textsize: int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    coords : mapping, optional\n        Coordinates of points to plot. **All** values are used for computation, but only a\n        subset can be plotted for convenience.\n    legend : bool, optional\n        Include a legend to the plot. Only taken into account when color argument is a dim name.\n    threshold : float\n        If some elpd difference is larger than ``threshold * elpd.std()``, show its label. If\n        `None`, no observations will be highlighted.\n    ic : str, optional\n        Information Criterion (\"loo\" for PSIS-LOO, \"waic\" for WAIC) used to compare models.\n        Defaults to ``rcParams[\"stats.information_criterion\"]``.\n        Only taken into account when input is :class:`arviz.InferenceData`.\n    scale : str, optional\n        Scale argument passed to :func:`arviz.loo` or :func:`arviz.waic`, see their docs for\n        details. Only taken into account when values in ``compare_dict`` are\n        :class:`arviz.InferenceData`.\n    var_name : str, optional\n        Argument passed to to :func:`arviz.loo` or :func:`arviz.waic`, see their docs for\n        details. Only taken into account when values in ``compare_dict`` are\n        :class:`arviz.InferenceData`.\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter`.\n    ax: axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\", \"bokeh\"}. Defaults to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_compare : Summary plot for model comparison.\n\n    Examples\n    --------\n    Compare pointwise PSIS-LOO for centered and non centered models of the 8-schools problem\n    using matplotlib.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata1 = az.load_arviz_data(\"centered_eight\")\n        >>> idata2 = az.load_arviz_data(\"non_centered_eight\")\n        >>> az.plot_elpd(\n        >>>     {\"centered model\": idata1, \"non centered model\": idata2},\n        >>>     xlabels=True\n        >>> )\n\n    .. bokeh-plot::\n        :source-position: above\n\n        import arviz as az\n        idata1 = az.load_arviz_data(\"centered_eight\")\n        idata2 = az.load_arviz_data(\"non_centered_eight\")\n        az.plot_elpd(\n            {\"centered model\": idata1, \"non centered model\": idata2},\n            backend=\"bokeh\"\n        )\n\n     source"},{"id":74,"pagetitle":"Plots","title":"ArviZ.plot_energy","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_energy","content":" ArviZ.plot_energy  —  Function Plot energy transition distribution and marginal energy distribution in HMC algorithms. Note This function is forwarded to Python's  arviz.plot_energy . The docstring of that function is included below. \n    This may help to diagnose poor exploration by gradient-based algorithms like HMC or NUTS.\n\n    Parameters\n    ----------\n    data : obj\n        :class:`xarray.Dataset`, or any object that can be converted (must represent\n        ``sample_stats`` and have an ``energy`` variable).\n    kind : str\n        Type of plot to display (\"kde\", \"hist\").\n    bfmi : bool\n        If True add to the plot the value of the estimated Bayesian fraction of missing information\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    legend : bool\n        Flag for plotting legend. Defaults to True.\n    fill_alpha : tuple of floats\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque). Defaults to (1, .75).\n    fill_color : tuple of valid matplotlib color\n        Color for Marginal energy distribution and Energy transition distribution.\n        Defaults to ('C0', 'C5').\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\". Defaults to \"experimental\".\n        Only works if ``kind='kde'``.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    fill_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_kde` (to control the shade).\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_kde` or :func:`matplotlib.pyplot.hist`\n        (if ``type='hist'``).\n    ax: axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\", \"bokeh\"}. Defaults to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    bfmi : Calculate the estimated Bayesian fraction of missing information (BFMI).\n\n    Examples\n    --------\n    Plot a default energy plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_energy(data)\n\n    Represent energy plot via histograms\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_energy(data, kind='hist')\n\n     source"},{"id":75,"pagetitle":"Plots","title":"ArviZ.plot_ess","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_ess","content":" ArviZ.plot_ess  —  Function Plot quantile, local or evolution of effective sample sizes (ESS). Note This function is forwarded to Python's  arviz.plot_ess . The docstring of that function is included below. \n    Parameters\n    ----------\n    idata: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    kind: str, optional\n        Options: ``local``, ``quantile`` or ``evolution``, specify the kind of plot.\n    relative: bool\n        Show relative ess in plot ``ress = ess / N``.\n    coords: dict, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple, optional\n        Figure size. If None it will be defined automatically.\n    textsize: float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    rug: bool\n        Plot rug plot of values diverging or that reached the max tree depth.\n    rug_kind: bool\n        Variable in sample stats to use as rug mask. Must be a boolean variable.\n    n_points: int\n        Number of points for which to plot their quantile/local ess or number of subsets\n        in the evolution plot.\n    extra_methods: bool, optional\n        Plot mean and sd ESS as horizontal lines. Not taken into account in evolution kind\n    min_ess: int\n        Minimum number of ESS desired. If ``relative=True`` the line is plotted at\n        ``min_ess / n_samples`` for local and quantile kinds and as a curve following\n        the ``min_ess / n`` dependency in evolution kind.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    extra_kwargs: dict, optional\n        If evolution plot, extra_kwargs is used to plot ess tail and differentiate it\n        from ess bulk. Otherwise, passed to extra methods lines.\n    text_kwargs: dict, optional\n        Only taken into account when ``extra_methods=True``. kwargs passed to ax.annotate\n        for extra methods lines labels. It accepts the additional\n        key ``x`` to set ``xy=(text_kwargs[\"x\"], mcse)``\n    hline_kwargs: dict, optional\n        kwargs passed to :func:`~matplotlib.axes.Axes.axhline` or to :class:`~bokeh.models.Span`\n        depending on the backend for the horizontal minimum ESS line.\n        For relative ess evolution plots the kwargs are passed to\n        :func:`~matplotlib.axes.Axes.plot` or to :class:`~bokeh.plotting.figure.line`\n    rug_kwargs: dict\n        kwargs passed to rug plot.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :meth:`mpl:matplotlib.axes.Axes.hist` or\n        :meth:`mpl:matplotlib.axes.Axes.plot` function depending on the\n        value of ``kind``.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    ess: Calculate estimate of the effective sample size.\n\n    References\n    ----------\n    * Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n\n    Examples\n    --------\n    Plot local ESS. This plot, together with the quantile ESS plot, is recommended to check\n    that there are enough samples for all the explored regions of parameter space. Checking\n    local and quantile ESS is particularly relevant when working with HDI intervals as\n    opposed to ESS bulk, which is relevant for point estimates.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"centered_eight\")\n        >>> coords = {\"school\": [\"Choate\", \"Lawrenceville\"]}\n        >>> az.plot_ess(\n        ...     idata, kind=\"local\", var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    Plot quantile ESS and exclude variables with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"quantile\", var_names=['~thet'], filter_vars=\"like\", coords=coords\n        ... )\n\n    Plot ESS evolution as the number of samples increase. When the model is converging properly,\n    both lines in this plot should be roughly linear.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"evolution\", var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    Customize local ESS plot to look like reference paper.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"local\", var_names=[\"mu\"], drawstyle=\"steps-mid\", color=\"k\",\n        ...     linestyle=\"-\", marker=None, rug=True, rug_kwargs={\"color\": \"r\"}\n        ... )\n\n    Customize ESS evolution plot to look like reference paper.\n\n    .. plot::\n        :context: close-figs\n\n        >>> extra_kwargs = {\"color\": \"lightsteelblue\"}\n        >>> az.plot_ess(\n        ...     idata, kind=\"evolution\", var_names=[\"mu\"],\n        ...     color=\"royalblue\", extra_kwargs=extra_kwargs\n        ... )\n\n     source"},{"id":76,"pagetitle":"Plots","title":"ArviZ.plot_forest","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_forest","content":" ArviZ.plot_forest  —  Function Forest plot to compare HDI intervals from a number of distributions. Note This function is forwarded to Python's  arviz.plot_forest . The docstring of that function is included below. \n    Generates a forest plot of 100*(hdi_prob)% HDI intervals from a trace or list of traces.\n\n    Parameters\n    ----------\n    data: obj or list[obj]\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    kind: str\n        Choose kind of plot for main axis. Supports \"forestplot\" or \"ridgeplot\".\n    model_names: list[str], optional\n        List with names for the models in the list of data. Useful when plotting more that one\n        dataset.\n    var_names: list[str], optional\n        List of variables to plot (defaults to None, which results in all variables plotted)\n        Prefix the variables by ``~`` when you want to exclude them from the plot.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If None(default), interpret var_names as the real variables names. If \"like\", interpret\n        var_names as substrings of the real variables names. If \"regex\", interpret var_names as\n        regular expressions on the real variables names. A la ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: dict, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n    combined: bool\n        Flag for combining multiple chains into a single chain. If False(default), chains will\n        be plotted separately.\n    hdi_prob: float, optional\n        Plots highest posterior density interval for chosen percentage of density.\n        Defaults to `0.94`.\n    rope: tuple or dictionary of tuples\n        Lower and upper values of the Region Of Practical Equivalence. If a list with one interval\n        only is provided, the ROPE will be displayed across the y-axis. If more than one\n        interval is provided the length of the list should match the number of variables.\n    quartiles: bool, optional\n        Flag for plotting the interquartile range, in addition to the ``hdi_prob`` intervals.\n        Defaults to True.\n    r_hat: bool, optional\n        Flag for plotting Split R-hat statistics. Requires 2 or more chains. Defaults to False\n    ess: bool, optional\n        Flag for plotting the effective sample size. Defaults to False.\n    colors: list or string, optional\n        list with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically chose a color per model from the matplotlibs\n        cycle. If a single color is passed, eg 'k', 'C2', 'red' this color will be used for all\n        models. Defaults to 'cycle'.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    linewidth: int\n        Line width throughout. If None it will be autoscaled based on ``figsize``.\n    markersize: int\n        Markersize throughout. If None it will be autoscaled based on ``figsize``.\n    legend : bool, optional\n        Show a legend with the color encoded model information.\n        Defaults to True, if there are multiple models.\n    labeller : labeller instance, optional\n        Class providing the method ``make_model_label`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ridgeplot_alpha: float\n        Transparency for ridgeplot fill.  If **0**, border is colored by model, otherwise\n        a `black` outline is used.\n    ridgeplot_overlap: float\n        Overlap height for ridgeplots.\n    ridgeplot_kind: string\n        By default (\"auto\") continuous variables are plotted using KDEs and discrete ones using\n        histograms. To override this use \"hist\" to plot histograms and \"density\" for KDEs.\n    ridgeplot_truncate: bool\n        Whether to truncate densities according to the value of ``hdi_prob``. Defaults to True.\n    ridgeplot_quantiles: list\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n        Defaults to None.\n    figsize: tuple\n        Figure size. If None, it will be defined automatically.\n    ax: axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Defaults to \"matplotlib\".\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in ``rcParams``.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    gridspec: matplotlib GridSpec or bokeh figures\n\n    See Also\n    --------\n    plot_posterior: Plot Posterior densities in the style of John K. Kruschke's book.\n    plot_density: Generate KDE plots for continuous variables and histograms for discrete ones.\n\n    Examples\n    --------\n    Forestplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered_data = az.load_arviz_data('non_centered_eight')\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='forestplot',\n        >>>                            var_names=[\"^the\"],\n        >>>                            filter_vars=\"regex\",\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Forestplot with multiple datasets\n\n    .. plot::\n        :context: close-figs\n\n        >>> centered_data = az.load_arviz_data('centered_eight')\n        >>> axes = az.plot_forest([non_centered_data, centered_data],\n        >>>                            model_names = [\"non centered eight\", \"centered eight\"],\n        >>>                            kind='forestplot',\n        >>>                            var_names=[\"^the\"],\n        >>>                            filter_vars=\"regex\",\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools models')\n\n    Forestplot with ropes\n\n    .. plot::\n        :context: close-figs\n\n        >>> rope = {'theta': [{'school': 'Choate', 'rope': (2, 4)}], 'mu': [{'rope': (-2, 2)}]}\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            rope=rope,\n        >>>                            var_names='~tau',\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n\n    Ridgeplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_overlap=3,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Ridgeplot non-truncated and with quantiles\n\n    .. plot::\n        :context: close-figs\n\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_truncate=False,\n        >>>                            ridgeplot_quantiles=[.25, .5, .75],\n        >>>                            ridgeplot_overlap=0.7,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n     source"},{"id":77,"pagetitle":"Plots","title":"ArviZ.plot_hdi","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_hdi","content":" ArviZ.plot_hdi  —  Function Note This function is forwarded to Python's  arviz.plot_hdi . The docstring of that function is included below.     Plot HDI intervals for regression data.\n\n    Parameters\n    ----------\n    x : array-like\n        Values to plot.\n    y : array-like, optional\n        Values from which to compute the HDI. Assumed shape ``(chain, draw, \\*shape)``.\n        Only optional if ``hdi_data`` is present.\n    hdi_data : array_like, optional\n        Precomputed HDI values to use. Assumed shape is ``(*x.shape, 2)``.\n    hdi_prob : float, optional\n        Probability for the highest density interval. Defaults to ``stats.hdi_prob`` rcParam.\n    color : str, optional\n        Color used for the limits of the HDI and fill. Should be a valid matplotlib color.\n    circular : bool, optional\n        Whether to compute the HDI taking into account ``x`` is a circular variable\n        (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n    smooth : boolean, optional\n        If True the result will be smoothed by first computing a linear interpolation of the data\n        over a regular grid and then applying the Savitzky-Golay filter to the interpolated data.\n        Defaults to True.\n    smooth_kwargs : dict, optional\n        Additional keywords modifying the Savitzky-Golay filter. See\n        :func:`scipy:scipy.signal.savgol_filter` for details.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    fill_kwargs : dict, optional\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.fill_between`\n        (use ``fill_kwargs={'alpha': 0}`` to disable fill) or to\n        :meth:`bokeh.plotting.Figure.patch`.\n    plot_kwargs : dict, optional\n        HDI limits keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh.plotting.Figure.patch`.\n    hdi_kwargs : dict, optional\n        Keyword arguments passed to :func:`~arviz.hdi`. Ignored if ``hdi_data`` is present.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    backend : {\"matplotlib\",\"bokeh\"}, optional\n        Select plotting backend.\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh.plotting.Figure.patch`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    hdi : Calculate highest density interval (HDI) of array for given probability.\n\n    Examples\n    --------\n    Plot HDI interval of simulated regression data using `y` argument:\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> x_data = np.random.normal(0, 1, 100)\n        >>> y_data = np.random.normal(2 + x_data * 0.5, 0.5, size=(2, 50, 100))\n        >>> az.plot_hdi(x_data, y_data)\n\n    ``plot_hdi`` can also be given precalculated values with the argument ``hdi_data``. This example\n    shows how to use :func:`~arviz.hdi` to precalculate the values and pass these values to\n    ``plot_hdi``. Similarly to an example in ``hdi`` we are using the ``input_core_dims``\n    argument of :func:`~arviz.wrap_xarray_ufunc` to manually define the dimensions over which\n    to calculate the HDI.\n\n    .. plot::\n        :context: close-figs\n\n        >>> hdi_data = az.hdi(y_data, input_core_dims=[[\"draw\"]])\n        >>> ax = az.plot_hdi(x_data, hdi_data=hdi_data[0], color=\"r\", fill_kwargs={\"alpha\": .2})\n        >>> az.plot_hdi(x_data, hdi_data=hdi_data[1], color=\"k\", ax=ax, fill_kwargs={\"alpha\": .2})\n\n    ``plot_hdi`` can also be used with Inference Data objects. Here we use the posterior predictive\n    to plot the HDI interval.\n\n    .. plot::\n        :context: close-figs\n\n        >>> X = np.random.normal(0,1,100)\n        >>> Y = np.random.normal(2 + X * 0.5, 0.5, size=(2,10,100))\n        >>> idata = az.from_dict(posterior={\"y\": Y}, constant_data={\"x\":X})\n        >>> x_data = idata.constant_data.x\n        >>> y_data = idata.posterior.y\n        >>> az.plot_hdi(x_data, y_data)\n\n     source"},{"id":78,"pagetitle":"Plots","title":"ArviZ.plot_kde","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_kde","content":" ArviZ.plot_kde  —  Function 1D or 2D KDE plot taking into account boundary conditions. Note This function is forwarded to Python's  arviz.plot_kde . The docstring of that function is included below. \n    Parameters\n    ----------\n    values : array-like\n        Values to plot\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE will be estimated\n    cumulative : bool\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE\n    rug : bool\n        If True adds a rugplot. Defaults to False. Ignored for 2D KDE\n    label : string\n        Text to include as part of the legend\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``is_circular`` is False\n        and \"taylor\" (for now) when ``is_circular`` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    adaptive: bool, optional.\n        If True, an adaptative bandwidth is used. Only valid for 1D KDE.\n        Defaults to False.\n    quantiles : list\n        Quantiles in ascending order used to segment the KDE.\n        Use [.25, .5, .75] for quartiles. Defaults to None.\n    rotated : bool\n        Whether to rotate the 1D KDE plot 90 degrees.\n    contour : bool\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE.\n        Defaults to True.\n    hdi_probs : list\n        Plots highest density credibility regions for the provided probabilities for a 2D KDE.\n        Defaults to matplotlib chosen levels with no fixed probability associated.\n    fill_last : bool\n        If True fill the last contour of the 2D KDE plot. Defaults to False.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``. Not implemented for bokeh backend.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE. See :meth:`mpl:matplotlib.axes.Axes.plot`\n        or :meth:`bokeh:bokeh.plotting.Figure.line` for a description of accepted values.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use ``fill_kwargs={'alpha': 0}``\n        to disable fill). Ignored for 2D KDE. Passed to\n        :meth:`bokeh.plotting.Figure.patch`.\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if ``rug=False`` or for 2D KDE\n        Use ``space`` keyword (float) to control the position of the rugplot. The larger this number\n        the lower the rugplot. Passed to :class:`bokeh:bokeh.models.glyphs.Scatter`.\n    contour_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.contour`\n        to draw contour lines or :meth:`bokeh.plotting.Figure.patch`.\n        Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.contourf`\n        to draw filled contours. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.pcolormesh` or\n        :meth:`bokeh.plotting.Figure.image`.\n        Ignored for 1D KDE.\n    is_circular : {False, True, \"radians\", \"degrees\"}. Default False.\n        Select input type {\"radians\", \"degrees\"} for circular histogram or KDE plot. If True,\n        default input type is \"radians\". When this argument is present, it interprets ``values``\n        is a circular variable measured in radians and a circular KDE is used. Inputs in\n        \"degrees\" will undergo an internal conversion to radians.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    legend : bool\n        Add legend to the figure. By default True.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n    return_glyph : bool, optional\n        Internal argument to return glyphs for bokeh\n\n    Returns\n    -------\n    axes : matplotlib.Axes or bokeh.plotting.Figure\n        Object containing the kde plot\n    glyphs : list, optional\n        Bokeh glyphs present in plot.  Only provided if ``return_glyph`` is True.\n\n    See Also\n    --------\n    kde : One dimensional density estimation.\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n\n    Examples\n    --------\n    Plot default KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> mu_posterior = np.concatenate(non_centered.posterior[\"mu\"].values)\n        >>> tau_posterior = np.concatenate(non_centered.posterior[\"tau\"].values)\n        >>> az.plot_kde(mu_posterior)\n\n\n    Plot KDE with rugplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rug=True)\n\n    Plot KDE with adaptive bandwidth\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, adaptive=True)\n\n    Plot KDE with a different bandwidth estimator\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, bw=\"scott\")\n\n    Plot KDE with a bandwidth specified manually\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, bw=0.4)\n\n    Plot KDE for a circular variable\n\n    .. plot::\n        :context: close-figs\n\n        >>> rvs = np.random.vonmises(mu=np.pi, kappa=2, size=500)\n        >>> az.plot_kde(rvs, is_circular=True)\n\n\n    Plot a cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, cumulative=True)\n\n\n\n    Rotate plot 90 degrees\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rotated=True)\n\n\n    Plot 2d contour KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior)\n\n\n    Plot 2d contour KDE, without filling and contour lines using viridis cmap\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior,\n        ...             contour_kwargs={\"colors\":None, \"cmap\":plt.cm.viridis},\n        ...             contourf_kwargs={\"alpha\":0});\n\n    Plot 2d contour KDE, set the number of levels to 3.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(\n        ...     mu_posterior, values2=tau_posterior,\n        ...     contour_kwargs={\"levels\":3}, contourf_kwargs={\"levels\":3}\n        ... );\n\n    Plot 2d contour KDE with 30%, 60% and 90% HDI contours.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, hdi_probs=[0.3, 0.6, 0.9])\n\n    Plot 2d smooth KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, contour=False)\n\n     source"},{"id":79,"pagetitle":"Plots","title":"ArviZ.plot_khat","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_khat","content":" ArviZ.plot_khat  —  Function Note This function is forwarded to Python's  arviz.plot_khat . The docstring of that function is included below.     Plot Pareto tail indices for diagnosing convergence.\n\n    Parameters\n    ----------\n    khats : ELPDData containing Pareto shapes information or array of\n        Pareto tail indices.\n    color : str or array_like, optional\n        Colors of the scatter plot, if color is a str all dots will\n        have the same color, if it is the size of the observations,\n        each dot will have the specified color, otherwise, it will be\n        interpreted as a list of the dims to be used for the color\n        code. If Matplotlib c argument is passed, it will override\n        the color argument\n    xlabels : bool, optional\n        Use coords as xticklabels\n    show_hlines : bool, optional\n        Show the horizontal lines, by default at the values [0, 0.5, 0.7, 1].\n    show_bins : bool, optional\n        Show the percentage of khats falling in each bin, as delimited by hlines.\n    bin_format : str, optional\n        The string is used as formatting guide calling ``bin_format.format(count, pct)``.\n    threshold : float, optional\n        Show the labels of k values larger than threshold. Defaults to `None`,\n        no observations will be highlighted.\n    hover_label : bool, optional\n        Show the datapoint label when hovering over it with the mouse. Requires an interactive\n        backend.\n    hover_format : str, optional\n        String used to format the hover label via ``hover_format.format(idx, coord_label)``\n    figsize : tuple, optional\n        Figure size. If None it will be defined automatically.\n    textsize: float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    coords : mapping, optional\n        Coordinates of points to plot. **All** values are used for computation, but only a\n        a subset can be plotted for convenience.\n    legend : bool, optional\n        Include a legend to the plot. Only taken into account when color argument is a dim name.\n    markersize: int, optional\n        markersize for scatter plot. Defaults to `None` in which case it will\n        be chosen based on autoscaling for figsize.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    hlines_kwargs: dictionary, optional\n        Additional keywords passed to\n        :meth:`matplotlib.axes.Axes.hlines`.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n    kwargs :\n        Additional keywords passed to\n        :meth:`matplotlib.axes.Axes.scatter`.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    psislw : Pareto smoothed importance sampling (PSIS).\n\n    Examples\n    --------\n    Plot estimated pareto shape parameters showing how many fall in each category.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> radon = az.load_arviz_data(\"radon\")\n        >>> loo_radon = az.loo(radon, pointwise=True)\n        >>> az.plot_khat(loo_radon, show_bins=True)\n\n    Show xlabels\n\n    .. plot::\n        :context: close-figs\n\n        >>> centered_eight = az.load_arviz_data(\"centered_eight\")\n        >>> khats = az.loo(centered_eight, pointwise=True).pareto_k\n        >>> az.plot_khat(khats, xlabels=True, threshold=1)\n\n    Use custom color scheme\n\n    .. plot::\n        :context: close-figs\n\n        >>> counties = radon.posterior.County[radon.constant_data.county_idx].values\n        >>> colors = [\n        ...     \"blue\" if county[-1] in (\"A\", \"N\") else \"green\" for county in counties\n        ... ]\n        >>> az.plot_khat(loo_radon, color=colors)\n\n    Notes\n    -----\n    The Generalized Pareto distribution (GPD) may be used to diagnose\n    convergence rates for importance sampling.  GPD has parameters\n    offset, scale, and shape. The shape parameter is usually denoted\n    with ``k``. ``k`` also tells how many finite moments the\n    distribution has. The pre-asymptotic convergence rate of\n    importance sampling can be estimated based on the fractional\n    number of finite moments of the importance ratio distribution. GPD\n    is fitted to the largest importance ratios and the estimated shape\n    parameter ``k``, i.e., ``\\hat{k}`` can then be used as a diagnostic\n    (most importantly if ``\\hat{k} > 0.7``, then the convergence rate\n    is impractically low). See [1]_.\n\n    References\n    ----------\n    .. [1] Vehtari, A., Simpson, D., Gelman, A., Yao, Y., Gabry, J.,\n        2019. Pareto Smoothed Importance Sampling. arXiv:1507.02646 [stat].\n\n     source"},{"id":80,"pagetitle":"Plots","title":"ArviZ.plot_loo_pit","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_loo_pit","content":" ArviZ.plot_loo_pit  —  Function Plot Leave-One-Out (LOO) probability integral transformation (PIT) predictive checks. Note This function is forwarded to Python's  arviz.plot_loo_pit . The docstring of that function is included below. \n    Parameters\n    ----------\n    idata : InferenceData\n        :class:`arviz.InferenceData` object.\n    y : array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat : array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as y plus an\n        extra dimension at the end of size n_samples (chains and draws stacked). If str or\n        None, ``idata`` must contain the posterior predictive group. If None, ``y_hat`` is taken\n        equal to y, thus, y must be str too.\n    log_weights : array or DataArray\n        Smoothed log_weights. It must have the same shape as ``y_hat``\n    ecdf : bool, optional\n        Plot the difference between the LOO-PIT Empirical Cumulative Distribution Function\n        (ECDF) and the uniform CDF instead of LOO-PIT kde.\n        In this case, instead of overlaying uniform distributions, the beta ``hdi_prob``\n        around the theoretical uniform CDF is shown. This approximation only holds\n        for large S and ECDF values not very close to 0 nor 1. For more information, see\n        `Vehtari et al. (2019)`, `Appendix G <https://avehtari.github.io/rhat_ess/rhat_ess.html>`_.\n    ecdf_fill : bool, optional\n        Use :meth:`matplotlib.axes.Axes.fill_between` to mark the area\n        inside the credible interval. Otherwise, plot the\n        border lines.\n    n_unif : int, optional\n        Number of datasets to simulate and overlay from the uniform distribution.\n    use_hdi : bool, optional\n        Compute expected hdi values instead of overlaying the sampled uniform distributions.\n    hdi_prob : float, optional\n        Probability for the highest density interval. Works with ``use_hdi=True`` or ``ecdf=True``.\n    figsize : figure size tuple, optional\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    color : str or array_like, optional\n        Color of the LOO-PIT estimated pdf plot. If ``plot_unif_kwargs`` has no \"color\" key,\n        a slightly lighter color than this argument will be used for the uniform kde lines.\n        This will ensure that LOO-PIT kde and uniform kde have different default colors.\n    legend : bool, optional\n        Show the legend of the figure.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    plot_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot`\n        for LOO-PIT line (kde or ECDF)\n    plot_unif_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot` for\n        overlaid uniform distributions or for beta credible interval\n        lines if ``ecdf=True``\n    hdi_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.axhspan`\n    fill_kwargs : dict, optional\n        Additional kwargs passed to :meth:`matplotlib.axes.Axes.fill_between`\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_bpv : Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n    loo_pit : Compute leave one out (PSIS-LOO) probability integral transform (PIT) values.\n\n    References\n    ----------\n    * Gabry et al. (2017) see https://arxiv.org/abs/1709.01449\n    * https://mc-stan.org/bayesplot/reference/PPC-loo.html\n    * Gelman et al. BDA (2014) Section 6.3\n\n    Examples\n    --------\n    Plot LOO-PIT predictive checks overlaying the KDE of the LOO-PIT values to several\n    realizations of uniform variable sampling with the same number of observations.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"radon\")\n        >>> az.plot_loo_pit(idata=idata, y=\"y\")\n\n    Fill the area containing the 94% highest density interval of the difference between uniform\n    variables empirical CDF and the real uniform CDF. A LOO-PIT ECDF clearly outside of these\n    theoretical boundaries indicates that the observations and the posterior predictive\n    samples do not follow the same distribution.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_loo_pit(idata=idata, y=\"y\", ecdf=True)\n\n     source"},{"id":81,"pagetitle":"Plots","title":"ArviZ.plot_mcse","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_mcse","content":" ArviZ.plot_mcse  —  Function Plot quantile or local Monte Carlo Standard Error. Note This function is forwarded to Python's  arviz.plot_mcse . The docstring of that function is included below. \n    Parameters\n    ----------\n    idata: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        `pandas.filter`.\n    coords: dict, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    errorbar: bool, optional\n        Plot quantile value +/- mcse instead of plotting mcse.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple, optional\n        Figure size. If None it will be defined automatically.\n    textsize: float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    extra_methods: bool, optional\n        Plot mean and sd MCSE as horizontal lines. Only taken into account when\n        ``errorbar=False``.\n    rug: bool\n        Plot rug plot of values diverging or that reached the max tree depth.\n    rug_kind: bool\n        Variable in sample stats to use as rug mask. Must be a boolean variable.\n    n_points: int\n        Number of points for which to plot their quantile/local ess or number of subsets\n        in the evolution plot.\n    labeller : labeller instance, optional\n        Class providing the method `make_label_vert` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    rug_kwargs: dict\n        kwargs passed to rug plot in\n        :meth:`mpl:matplotlib.axes.Axes.plot` or :class:`bokeh:bokeh.models.glyphs.Scatter`.\n    extra_kwargs: dict, optional\n        kwargs passed as extra method lines in\n        :meth:`mpl:matplotlib.axes.Axes.axhline` or :class:`bokeh:bokeh.models.Span`\n    text_kwargs: dict, optional\n        kwargs passed to :meth:`mpl:matplotlib.axes.Axes.annotate` for extra methods lines labels.\n        It accepts the additional key ``x`` to set ``xy=(text_kwargs[\"x\"], mcse)``.\n        text_kwargs are ignored for the bokeh plotting backend.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :meth:`mpl:matplotlib.axes.Axes.hist` or\n        :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib depending on the value of `kind`.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    :func:`arviz.mcse`: Calculate Markov Chain Standard Error statistic.\n\n    References\n    ----------\n    * Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n\n    Examples\n    --------\n    Plot quantile Monte Carlo Standard Error.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"centered_eight\")\n        >>> coords = {\"school\": [\"Deerfield\", \"Lawrenceville\"]}\n        >>> az.plot_mcse(\n        ...     idata, var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n     source"},{"id":82,"pagetitle":"Plots","title":"ArviZ.plot_pair","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_pair","content":" ArviZ.plot_pair  —  Function Note This function is forwarded to Python's  arviz.plot_pair . The docstring of that function is included below.     Plot a scatter, kde and/or hexbin matrix with (optional) marginals on the diagonal.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    group: str, optional\n        Specifies which InferenceData group should be plotted.  Defaults to 'posterior'.\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n    marginals: bool, optional\n        If True pairplot will include marginal distributions for every variable\n    figsize: figure size tuple\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    kind : str or List[str]\n        Type of plot to display (scatter, kde and/or hexbin)\n    gridsize: int or (int, int), optional\n        Only works for ``kind=hexbin``. The number of hexagons in the x-direction.\n        The corresponding number of hexagons in the y-direction is chosen\n        such that the hexagons are approximately regular. Alternatively, gridsize\n        can be a tuple with two elements specifying the number of hexagons\n        in the x-direction and the y-direction.\n    divergences: Boolean\n        If True divergences will be plotted in a different color, only if group is either 'prior'\n        or 'posterior'.\n    colorbar: bool\n        If True a colorbar will be included as part of the plot (Defaults to False).\n        Only works when ``kind=hexbin``\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    divergences_kwargs: dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter` for divergences\n    scatter_kwargs:\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter` when using scatter kind\n    kde_kwargs: dict, optional\n        Additional keywords passed to :func:`arviz.plot_kde` when using kde kind\n    hexbin_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.hexbin` when\n        using hexbin kind\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    marginal_kwargs: dict, optional\n        Additional keywords passed to :func:`arviz.plot_dist`, modifying the\n        marginal distributions plotted in the diagonal.\n    point_estimate: str, optional\n        Select point estimate from 'mean', 'mode' or 'median'. The point estimate will be\n        plotted using a scatter marker and vertical/horizontal lines.\n    point_estimate_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.axvline`,\n        :meth:`matplotlib.axes.Axes.axhline` (matplotlib) or\n        :class:`bokeh:bokeh.models.Span` (bokeh)\n    point_estimate_marker_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter`\n        or :meth:`bokeh:bokeh.plotting.Figure.square` in point\n        estimate plot. Not available in bokeh\n    reference_values: dict, optional\n        Reference values for the plotted variables. The Reference values will be plotted\n        using a scatter marker\n    reference_values_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.circle` in reference values plot\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    Examples\n    --------\n    KDE Pair Plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> coords = {'school': ['Choate', 'Deerfield']}\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu', 'tau'],\n        >>>             kind='kde',\n        >>>             coords=coords,\n        >>>             divergences=True,\n        >>>             textsize=18)\n\n    Hexbin pair plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu'],\n        >>>             coords=coords,\n        >>>             textsize=18,\n        >>>             kind='hexbin')\n\n    Pair plot showing divergences and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        ...             var_names=['^t', 'mu'],\n        ...             filter_vars=\"regex\",\n        ...             coords=coords,\n        ...             divergences=True,\n        ...             textsize=18)\n     source"},{"id":83,"pagetitle":"Plots","title":"ArviZ.plot_parallel","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_parallel","content":" ArviZ.plot_parallel  —  Function Note This function is forwarded to Python's  arviz.plot_parallel . The docstring of that function is included below.     Plot parallel coordinates plot showing posterior points with and without divergences.\n\n    Described by https://arxiv.org/abs/1709.01449\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names\n        Variables to be plotted, if `None` all variables are plotted. Can be used to change the\n        order of the plotted variables. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: mapping, optional\n        Coordinates of ``var_names`` to be plotted.\n        Passed to :meth:`xarray.Dataset.sel`.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    legend: bool\n        Flag for plotting legend (defaults to True)\n    colornd: valid matplotlib color\n        color for non-divergent points. Defaults to 'k'\n    colord: valid matplotlib color\n        color for divergent points. Defaults to 'C1'\n    shadend: float\n        Alpha blending value for non-divergent points, between 0 (invisible) and 1 (opaque).\n        Defaults to .025\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    norm_method: str\n        Method for normalizing the data. Methods include normal, minmax and rank.\n        Defaults to none.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes.\n        Defaults to value set in ``rcParams``.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_pair : Plot a scatter, kde and/or hexbin matrix with (optional) marginals on the diagonal.\n    plot_trace : Plot distribution (histogram or kernel density estimates) and sampled values\n                 or rank plot\n\n    Examples\n    --------\n    Plot default parallel plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_parallel(data, var_names=[\"mu\", \"tau\"])\n\n\n    Plot parallel plot with normalization\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"normal\")\n\n    Plot parallel plot with minmax\n\n    .. plot::\n        :context: close-figs\n\n        >>> ax = az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"minmax\")\n        >>> ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n\n    Plot parallel plot with rank\n\n    .. plot::\n        :context: close-figs\n\n        >>> ax = az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"rank\")\n        >>> ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n     source"},{"id":84,"pagetitle":"Plots","title":"ArviZ.plot_posterior","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_posterior","content":" ArviZ.plot_posterior  —  Function Plot Posterior densities in the style of John K. Kruschke's book. Note This function is forwarded to Python's  arviz.plot_posterior . The docstring of that function is included below. \n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to the documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names\n        Variables to be plotted, two variables are required. Prefix the variables with ``~``\n        when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    hdi_prob: float, optional\n        Plots highest density interval for chosen percentage of density.\n        Use 'hide' to hide the highest density interval. Defaults to 0.94.\n    multimodal: bool\n        If true (default) it may compute more than one credible interval if the distribution is\n        multimodal and the modes are well separated.\n    skipna : bool\n        If true ignores nan values when computing the hdi and point estimates. Defaults to false.\n    round_to: int, optional\n        Controls formatting of floats. Defaults to 2 or the integer part, whichever is bigger.\n    point_estimate: Optional[str]\n        Plot point estimate per variable. Values should be 'mean', 'median', 'mode' or None.\n        Defaults to 'auto' i.e. it falls back to default set in rcParams.\n    group: str, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n    rope: tuple or dictionary of tuples\n        Lower and upper values of the Region Of Practical Equivalence. If a list is provided, its\n        length should match the number of variables.\n    ref_val: float or dictionary of floats\n        display the percentage below and above the values in ref_val. Must be None (default),\n        a constant, a list or a dictionary like see an example below. If a list is provided, its\n        length should match the number of variables.\n    rope_color: str, optional\n        Specifies the color of ROPE and displayed percentage within ROPE\n    ref_val_color: str, optional\n        Specifies the color of the displayed percentage\n    kind: str\n        Type of plot to display (kde or hist) For discrete variables this argument is ignored and\n        a histogram is always used. Defaults to rcParam ``plot.density_kind``\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when `circular` is False\n        and \"taylor\" (for now) when `circular` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is. Only works if `kind == kde`.\n    circular: bool, optional\n        If True, it interprets the values passed are from a circular variable measured in radians\n        and a circular KDE is used. Only valid for 1D KDE. Defaults to False.\n        Only works if `kind == kde`.\n    bins: integer or sequence or 'auto', optional\n        Controls the number of bins,accepts the same keywords :func:`matplotlib.pyplot.hist` does.\n        Only works if `kind == hist`. If None (default) it will use `auto` for continuous variables\n        and `range(xmin, xmax + 1)` for discrete variables.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :func:`matplotlib.pyplot.hist` or :func:`matplotlib.pyplot.plot` function\n        depending on the value of `kind`.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    plot_forest : Forest plot to compare HDI intervals from a number of distributions.\n\n    Examples\n    --------\n    Show a default kernel density plot following style of John Kruschke\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_posterior(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'])\n\n    Plot Region of Practical Equivalence (rope) and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', '^the'], filter_vars=\"regex\", rope=(-1, 1))\n\n    Plot Region of Practical Equivalence for selected distributions\n\n    .. plot::\n        :context: close-figs\n\n        >>> rope = {'mu': [{'rope': (-2, 2)}], 'theta': [{'school': 'Choate', 'rope': (2, 4)}]}\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], rope=rope)\n\n    Using `coords` argument to plot only a subset of data\n\n    .. plot::\n        :context: close-figs\n\n        >>> coords = {\"school\": [\"Choate\",\"Phillips Exeter\"]}\n        >>> az.plot_posterior(data, var_names=[\"mu\", \"theta\"], coords=coords)\n\n    Add reference lines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], ref_val=0)\n\n    Show point estimate of distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], point_estimate='mode')\n\n    Show reference values using variable names and coordinates\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, ref_val= {\"theta\": [{\"school\": \"Deerfield\", \"ref_val\": 4},\n        ...                                             {\"school\": \"Choate\", \"ref_val\": 3}]})\n\n    Show reference values using a list\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, ref_val=[1] + [5] * 8 + [1])\n\n\n    Plot posterior as a histogram\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'], kind='hist')\n\n    Change size of highest density interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'], hdi_prob=.75)\n     source"},{"id":85,"pagetitle":"Plots","title":"ArviZ.plot_ppc","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_ppc","content":" ArviZ.plot_ppc  —  Function Note This function is forwarded to Python's  arviz.plot_ppc . The docstring of that function is included below.     Plot for posterior/prior predictive checks.\n\n    Parameters\n    ----------\n    data: az.InferenceData object\n        :class:`arviz.InferenceData` object containing the observed and posterior/prior\n        predictive data.\n    kind: str\n        Type of plot to display (\"kde\", \"cumulative\", or \"scatter\"). Defaults to `kde`.\n    alpha: float\n        Opacity of posterior/prior predictive density curves.\n        Defaults to 0.2 for ``kind = kde`` and cumulative, for scatter defaults to 0.7.\n    mean: bool\n        Whether or not to plot the mean posterior/prior predictive distribution.\n        Defaults to ``True``.\n    observed: bool, default True\n        Whether or not to plot the observed data.\n    color: str\n        Valid matplotlib ``color``. Defaults to ``C0``.\n    color: list\n        List with valid matplotlib colors corresponding to the posterior/prior predictive\n        distribution, observed data and mean of the posterior/prior predictive distribution.\n        Defaults to [\"C0\", \"k\", \"C1\"].\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None, it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None, it will be\n        autoscaled based on ``figsize``.\n    data_pairs: dict\n        Dictionary containing relations between observed data and posterior/prior predictive data.\n        Dictionary structure:\n\n        - key = data var_name\n        - value = posterior/prior predictive var_name\n\n        For example, ``data_pairs = {'y' : 'y_hat'}``\n        If None, it will assume that the observed data and the posterior/prior\n        predictive data have the same variable name.\n    var_names: list of variable names\n        Variables to be plotted, if `None` all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None.\n    flatten: list\n        List of dimensions to flatten in ``observed_data``. Only flattens across the coordinates\n        specified in the ``coords`` argument. Defaults to flattening all of the dimensions.\n    flatten_pp: list\n        List of dimensions to flatten in posterior_predictive/prior_predictive. Only flattens\n        across the coordinates specified in the ``coords`` argument. Defaults to flattening all\n        of the dimensions. Dimensions should match flatten excluding dimensions for ``data_pairs``\n        parameters. If ``flatten`` is defined and ``flatten_pp`` is None, then\n        ``flatten_pp = flatten``.\n    num_pp_samples: int\n        The number of posterior/prior predictive samples to plot. For ``kind`` = 'scatter' and\n        ``animation = False`` if defaults to a maximum of 5 samples and will set jitter to 0.7.\n        unless defined. Otherwise it defaults to all provided samples.\n    random_seed: int\n        Random number generator seed passed to ``numpy.random.seed`` to allow\n        reproducibility of the plot. By default, no seed will be provided\n        and the plot will change each call if a random sample is specified\n        by ``num_pp_samples``.\n    jitter: float\n        If ``kind`` is \"scatter\", jitter will add random uniform noise to the height\n        of the ppc samples and observed data. By default 0.\n    animated: bool\n        Create an animation of one posterior/prior predictive sample per frame.\n        Defaults to ``False``. Only works with matploblib backend.\n        To run animations inside a notebook you have to use the `nbAgg` matplotlib's backend.\n        Try with `%matplotlib notebook` or  `%matplotlib  nbAgg`. You can switch back to the\n        default matplotlib's backend with `%matplotlib  inline` or `%matplotlib  auto`.\n        If switching back and forth between matplotlib's backend, you may need to run twice the cell\n        with the animation.\n        If you experience problems rendering the animation try setting\n        `animation_kwargs({'blit':False}`) or changing the matplotlib's backend (e.g. to TkAgg)\n        If you run the animation from a script write `ax, ani = az.plot_ppc(.)`\n    animation_kwargs : dict\n        Keywords passed to  :class:`matplotlib.animation.FuncAnimation`. Ignored with\n        matplotlib backend.\n    legend : bool\n        Add legend to figure. By default ``True``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    group: {\"prior\", \"posterior\"}, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n        Other value can be 'prior'.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_bpv: Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n    plot_lm: Posterior predictive and mean plots for regression-like data.\n    plot_ppc: plot for posterior/prior predictive checks.\n    plot_ts: Plot timeseries data.\n\n    Examples\n    --------\n    Plot the observed data KDE overlaid on posterior predictive KDEs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('radon')\n        >>> az.plot_ppc(data, data_pairs={\"y\":\"y\"})\n\n    Plot the overlay with empirical CDFs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='cumulative')\n\n    Use the ``coords`` and ``flatten`` parameters to plot selected variable dimensions\n    across multiple plots. We will now modify the dimension ``obs_id`` to contain\n    indicate the name of the county where the measure was taken. The change has to\n    be done on both ``posterior_predictive`` and ``observed_data`` groups, which is\n    why we will use :meth:`~arviz.InferenceData.map` to apply the same function to\n    both groups. Afterwards, we will select the counties to be plotted with the\n    ``coords`` arg.\n\n    .. plot::\n        :context: close-figs\n\n        >>> obs_county = data.posterior[\"County\"][data.constant_data[\"county_idx\"]]\n        >>> data = data.assign_coords(obs_id=obs_county, groups=\"observed_vars\")\n        >>> az.plot_ppc(data, coords={'obs_id': ['ANOKA', 'BELTRAMI']}, flatten=[])\n\n    Plot the overlay using a stacked scatter plot that is particularly useful\n    when the sample sizes are small.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='scatter', flatten=[],\n        >>>             coords={'obs_id': ['AITKIN', 'BELTRAMI']})\n\n    Plot random posterior predictive sub-samples.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, num_pp_samples=30, random_seed=7)\n     source"},{"id":86,"pagetitle":"Plots","title":"ArviZ.plot_rank","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_rank","content":" ArviZ.plot_rank  —  Function Plot rank order statistics of chains. Note This function is forwarded to Python's  arviz.plot_rank . The docstring of that function is included below. \n    From the paper: Rank plots are histograms of the ranked posterior draws (ranked over all\n    chains) plotted separately for each chain.\n    If all of the chains are targeting the same posterior, we expect the ranks in each chain to be\n    uniform, whereas if one chain has a different location or scale parameter, this will be\n    reflected in the deviation from uniformity. If rank plots of all chains look similar, this\n    indicates good mixing of the chains.\n\n    This plot was introduced by Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter,\n    Paul-Christian Burkner (2019): Rank-normalization, folding, and localization: An improved R-hat\n    for assessing convergence of MCMC. arXiv preprint https://arxiv.org/abs/1903.08008\n\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of  :func:`arviz.convert_to_dataset` for details\n    var_names: string or list of variable names\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    bins: None or passed to np.histogram\n        Binning strategy used for histogram. By default uses twice the result of Sturges' formula.\n        See :func:`numpy.histogram` documentation for, other available arguments.\n    kind: string\n        If bars (defaults), ranks are represented as stacked histograms (one per chain). If vlines\n        ranks are represented as vertical lines above or below ``ref_line``.\n    colors: string or list of strings\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplotlib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    ref_line: boolean\n        Whether to include a dashed line showing where a uniform distribution would lie\n    labels: bool\n        whether to plot or not the x and y labels, defaults to True\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    ref_line_kwargs : dict, optional\n        Reference line keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.axhline` or\n        :class:`bokeh:bokeh.models.Span`.\n    bar_kwargs : dict, optional\n        Bars keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.bar` or\n        :meth:`bokeh:bokeh.plotting.Figure.vbar`.\n    vlines_kwargs : dict, optional\n        Vlines keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.vlines` or\n        :meth:`bokeh:bokeh.plotting.Figure.multi_line`.\n    marker_vlines_kwargs : dict, optional\n        Marker for the vlines keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.circle`.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_trace : Plot distribution (histogram or kernel density estimates) and\n                 sampled values or rank plot.\n\n    Examples\n    --------\n    Show a default rank plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_rank(data)\n\n    Recreate Figure 13 from the arxiv preprint\n\n    .. plot::\n        :context: close-figs\n\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_rank(data, var_names='tau')\n\n    Use vlines to compare results for centered vs noncentered models\n\n    .. plot::\n        :context: close-figs\n\n        >>> import matplotlib.pyplot as plt\n        >>> centered_data = az.load_arviz_data('centered_eight')\n        >>> noncentered_data = az.load_arviz_data('non_centered_eight')\n        >>> _, ax = plt.subplots(1, 2, figsize=(12, 3))\n        >>> az.plot_rank(centered_data, var_names=\"mu\", kind='vlines', ax=ax[0])\n        >>> az.plot_rank(noncentered_data, var_names=\"mu\", kind='vlines', ax=ax[1])\n\n    Change the aesthetics using kwargs\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_rank(noncentered_data, var_names=\"mu\", kind=\"vlines\",\n        >>>              vlines_kwargs={'lw':0}, marker_vlines_kwargs={'lw':3});\n     source"},{"id":87,"pagetitle":"Plots","title":"ArviZ.plot_separation","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_separation","content":" ArviZ.plot_separation  —  Function Separation plot for binary outcome models. Note This function is forwarded to Python's  arviz.plot_separation . The docstring of that function is included below. \n    Model predictions are sorted and plotted using a color code according to\n    the observed data.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        :class:`arviz.InferenceData` object.\n    y : array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat : array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as ``y``. If str or\n        None, ``idata`` must contain the posterior predictive group.\n    y_hat_line : bool, optional\n        Plot the sorted ``y_hat`` predictions.\n    expected_events : bool, optional\n        Plot the total number of expected events.\n    figsize : figure size tuple, optional\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    color : str, optional\n        Color to assign to the positive class. The negative class will be plotted using the\n        same color and an `alpha=0.3` transparency.\n    legend : bool, optional\n        Show the legend of the figure.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    plot_kwargs : dict, optional\n        Additional keywords passed to :meth:`mpl:matplotlib.axes.Axes.bar` or\n        :meth:`bokeh:bokeh.plotting.Figure.vbar` for separation plot.\n    y_hat_line_kwargs : dict, optional\n        Additional keywords passed to ax.plot for ``y_hat`` line.\n    exp_events_kwargs : dict, optional\n        Additional keywords passed to ax.scatter for ``expected_events`` marker.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_ppc : Plot for posterior/prior predictive checks.\n\n    References\n    ----------\n    .. [1] Greenhill, B. *et al.*, The Separation Plot: A New Visual Method\n       for Evaluating the Fit of Binary Models, *American Journal of\n       Political Science*, (2011) see https://doi.org/10.1111/j.1540-5907.2011.00525.x\n\n    Examples\n    --------\n    Separation plot for a logistic regression model.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data('classification10d')\n        >>> az.plot_separation(idata=idata, y='outcome', y_hat='outcome', figsize=(8, 1))\n\n     source"},{"id":88,"pagetitle":"Plots","title":"ArviZ.plot_trace","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_trace","content":" ArviZ.plot_trace  —  Function Plot distribution (histogram or kernel density estimates) and sampled values or rank plot. Note This function is forwarded to Python's  arviz.plot_trace . The docstring of that function is included below. \n    If `divergences` data is available in `sample_stats`, will plot the location of divergences as\n    dashed vertical lines.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: str or list of str, optional\n        One or more variables to be plotted. Prefix the variables by ``~`` when you want\n        to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: dict of {str: slice or array_like}, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    divergences: {\"bottom\", \"top\", None}, optional\n        Plot location of divergences on the traceplots.\n    kind: {\"trace\", \"rank_bars\", \"rank_vlines\"}, optional\n        Choose between plotting sampled values per iteration and rank plots.\n    transform: callable, optional\n        Function to transform data (defaults to None i.e.the identity function)\n    figsize: tuple of (float, float), optional\n        If None, size is (12, variables * 2)\n    rug: bool, optional\n        If True adds a rugplot of samples. Defaults to False. Ignored for 2D KDE.\n        Only affects continuous variables.\n    lines: list of tuple of (str, dict, array_like), optional\n        List of (var_name, {'coord': selection}, [line, positions]) to be overplotted as\n        vertical lines on the density and horizontal lines on the trace.\n    circ_var_names : str or list of str, optional\n        List of circular variables to account for when plotting KDE.\n    circ_var_units : str\n        Whether the variables in ``circ_var_names`` are in \"degrees\" or \"radians\".\n    compact: bool, optional\n        Plot multidimensional variables in a single plot.\n    compact_prop: str or dict {str: array_like}, optional\n        Tuple containing the property name and the property values to distinguish different\n        dimensions with compact=True\n    combined: bool, optional\n        Flag for combining multiple chains into a single line. If False (default), chains will be\n        plotted separately.\n    chain_prop: str or dict {str: array_like}, optional\n        Tuple containing the property name and the property values to distinguish different chains\n    legend: bool, optional\n        Add a legend to the figure with the chain color code.\n    plot_kwargs, fill_kwargs, rug_kwargs, hist_kwargs: dict, optional\n        Extra keyword arguments passed to :func:`arviz.plot_dist`. Only affects continuous\n        variables.\n    trace_kwargs: dict, optional\n        Extra keyword arguments passed to :meth:`matplotlib.axes.Axes.plot`\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    rank_kwargs : dict, optional\n        Extra keyword arguments passed to :func:`arviz.plot_rank`\n    axes: axes, optional\n        Matplotlib axes or bokeh figures.\n    backend: {\"matplotlib\", \"bokeh\"}, optional\n        Select plotting backend.\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in rcParams.\n    backend_kwargs: dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_rank : Plot rank order statistics of chains.\n\n    Examples\n    --------\n    Plot a subset variables and select them with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('non_centered_eight')\n        >>> coords = {'school': ['Choate', 'Lawrenceville']}\n        >>> az.plot_trace(data, var_names=('theta'), filter_vars=\"like\", coords=coords)\n\n    Show all dimensions of multidimensional variables in the same plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(data, compact=True)\n\n    Display a rank plot instead of trace\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(data, var_names=[\"mu\", \"tau\"], kind=\"rank_bars\")\n\n    Combine all chains into one distribution and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(\n        >>>     data, var_names=('^theta'), filter_vars=\"regex\", coords=coords, combined=True\n        >>> )\n\n\n    Plot reference lines against distribution and trace\n\n    .. plot::\n        :context: close-figs\n\n        >>> lines = (('theta_t',{'school': \"Choate\"}, [-1]),)\n        >>> az.plot_trace(data, var_names=('theta_t', 'theta'), coords=coords, lines=lines)\n\n     source"},{"id":89,"pagetitle":"Plots","title":"ArviZ.plot_violin","ref":"/ArviZ/stable/api/plots/#ArviZ.plot_violin","content":" ArviZ.plot_violin  —  Function Plot posterior of traces as violin plot. Note This function is forwarded to Python's  arviz.plot_violin . The docstring of that function is included below. \n    Notes\n    -----\n    If multiple chains are provided for a variable they will be combined\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e. the identity function).\n    quartiles: bool, optional\n        Flag for plotting the interquartile range, in addition to the ``hdi_prob`` * 100%\n        intervals. Defaults to ``True``.\n    rug: bool\n        If ``True`` adds a jittered rugplot. Defaults to ``False``.\n    side : {\"both\", \"left\", \"right\"}, default \"both\"\n        If ``both``, both sides of the violin plot are rendered. If ``left`` or ``right``, only\n        the respective side is rendered. By separately plotting left and right halfs with\n        different data, split violin plots can be achieved.\n    hdi_prob: float, optional\n        Plots highest posterior density interval for chosen percentage of density.\n        Defaults to 0.94.\n    shade: float\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque). Defaults to 0.\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``circular`` is ``False``\n        and \"taylor\" (for now) when ``circular`` is ``True``.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    circular: bool, optional.\n        If ``True``, it interprets `values` is a circular variable measured in radians\n        and a circular KDE is used. Defaults to ``False``.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: int\n        Text size of the point_estimates, axis ticks, and highest density interval. If None it will\n        be autoscaled based on ``figsize``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    sharex: bool\n        Defaults to ``True``, violinplots share a common x-axis scale.\n    sharey: bool\n        Defaults to ``True``, violinplots share a common y-axis scale.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    shade_kwargs: dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.fill_between`, or\n        :meth:`matplotlib.axes.Axes.barh` to control the shade.\n    rug_kwargs: dict\n        Keywords passed to the rug plot. If true only the right half side of the violin will be\n        plotted.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_forest: Forest plot to compare HDI intervals from a number of distributions.\n\n    Examples\n    --------\n    Show a default violin plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_violin(data)\n\n     source"},{"id":92,"pagetitle":"Stats","title":"Stats","ref":"/ArviZ/stable/api/stats/#stats-api","content":" Stats PSIS.PSISResult ArviZ.compare ArviZ.hdi ArviZ.loo ArviZ.loo_pit ArviZ.r2_score ArviZ.summary ArviZ.waic PSIS.psis PSIS.psis! StatsBase.summarystats"},{"id":93,"pagetitle":"Stats","title":"General statistics","ref":"/ArviZ/stable/api/stats/#General-statistics","content":" General statistics"},{"id":94,"pagetitle":"Stats","title":"ArviZ.hdi","ref":"/ArviZ/stable/api/stats/#ArviZ.hdi","content":" ArviZ.hdi  —  Function Note This function is forwarded to Python's  arviz.hdi . The docstring of that function is included below.     Calculate highest density interval (HDI) of array for given probability.\n\n    The HDI is the minimum width Bayesian credible interval (BCI).\n\n    Parameters\n    ----------\n    ary: obj\n        object containing posterior samples.\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    hdi_prob: float, optional\n        Prob for which the highest density interval will be computed. Defaults to\n        ``stats.hdi_prob`` rcParam.\n    circular: bool, optional\n        Whether to compute the hdi taking into account `x` is a circular variable\n        (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n        Only works if multimodal is False.\n    multimodal: bool, optional\n        If true it may compute more than one hdi if the distribution is multimodal and the\n        modes are well separated.\n    skipna: bool, optional\n        If true ignores nan values when computing the hdi. Defaults to false.\n    group: str, optional\n        Specifies which InferenceData group should be used to calculate hdi.\n        Defaults to 'posterior'\n    var_names: list, optional\n        Names of variables to include in the hdi report. Prefix the variables by ``~``\n        when you want to exclude them from the report: `[\"~beta\"]` instead of `[\"beta\"]`\n        (see :func:`arviz.summary` for more details).\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: mapping, optional\n        Specifies the subset over to calculate hdi.\n    max_modes: int, optional\n        Specifies the maximum number of modes for multimodal case.\n    dask_kwargs : dict, optional\n        Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.\n    kwargs: dict, optional\n        Additional keywords passed to :func:`~arviz.wrap_xarray_ufunc`.\n\n    Returns\n    -------\n    np.ndarray or xarray.Dataset, depending upon input\n        lower(s) and upper(s) values of the interval(s).\n\n    See Also\n    --------\n    plot_hdi : Plot highest density intervals for regression data.\n    xarray.Dataset.quantile : Calculate quantiles of array for given probabilities.\n\n    Examples\n    --------\n    Calculate the HDI of a Normal random variable:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: import numpy as np\n           ...: data = np.random.normal(size=2000)\n           ...: az.hdi(data, hdi_prob=.68)\n\n    Calculate the HDI of a dataset:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data('centered_eight')\n           ...: az.hdi(data)\n\n    We can also calculate the HDI of some of the variables of dataset:\n\n    .. ipython::\n\n        In [1]: az.hdi(data, var_names=[\"mu\", \"theta\"])\n\n    By default, ``hdi`` is calculated over the ``chain`` and ``draw`` dimensions. We can use the\n    ``input_core_dims`` argument of :func:`~arviz.wrap_xarray_ufunc` to change this. In this example\n    we calculate the HDI also over the ``school`` dimension:\n\n    .. ipython::\n\n        In [1]: az.hdi(data, var_names=\"theta\", input_core_dims = [[\"chain\",\"draw\", \"school\"]])\n\n    We can also calculate the hdi over a particular selection:\n\n    .. ipython::\n\n        In [1]: az.hdi(data, coords={\"chain\":[0, 1, 3]}, input_core_dims = [[\"draw\"]])\n\n     source"},{"id":95,"pagetitle":"Stats","title":"ArviZ.summary","ref":"/ArviZ/stable/api/stats/#ArviZ.summary","content":" ArviZ.summary  —  Function summary(\n    data; group = :posterior, coords dims, kwargs...,\n) -> Union{Dataset,DataFrames.DataFrame} Compute summary statistics on any object that can be passed to  convert_to_dataset . Keywords coords : Map from named dimension to named indices. dims : Map from variable name to names of its dimensions. kwargs : Keyword arguments passed to  summarystats . source"},{"id":96,"pagetitle":"Stats","title":"StatsBase.summarystats","ref":"/ArviZ/stable/api/stats/#StatsBase.summarystats","content":" StatsBase.summarystats  —  Function summarystats(\n    data::InferenceData;\n    group = :posterior,\n    kwargs...,\n) -> Union{Dataset,DataFrames.DataFrame}\nsummarystats(data::Dataset; kwargs...) -> Union{Dataset,DataFrames.DataFrame} Compute summary statistics on  data . Arguments data::Union{Dataset,InferenceData} : The data on which to compute summary statistics. If    data  is an  InferenceData , only the dataset corresponding to  group  is used. Keywords var_names : Collection of names of variables as  Symbol s to include in summary include_circ::Bool=false : Whether to include circular statistics digits::Int : Number of decimals used to round results. If not provided, numbers are not   rounded. stat_funcs::Union{Dict{String,Function},Vector{Function}}=nothing : A vector of functions   or a dict of functions with function names as keys used to calculate statistics. By   default, the mean, standard deviation, simulation standard error, and highest posterior   density intervals are included.   The functions will be given one argument, the samples for a variable as an array, The   functions should operate on an array, returning a single number. For example,    Statistics.mean , or  Statistics.var  would both work. extend::Bool=true : If  true , use the statistics returned by  stat_funcs  in addition   to, rather than in place of, the default statistics. This is only meaningful when    stat_funcs  is not  nothing . hdi_prob::Real=0.94 : HDI interval to compute. This is only meaningful when  stat_funcs    is  nothing . skipna::Bool=false : If  true , ignores  NaN  values when computing the summary   statistics. It does not affect the behaviour of the functions passed to  stat_funcs . Returns DataFrames.DataFrame : Summary statistics for each variable. Default statistics are: mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat  (only computed for traces with 2 or more chains) Examples using ArviZ\nidata = load_example_data(\"centered_eight\")\nsummarystats(idata; var_names=(:mu, :tau)) Other statistics can be calculated by passing a list of functions or a dictionary with key, function pairs: using Statistics\nfunction median_sd(x)\n    med = median(x)\n    sd = sqrt(mean((x .- med).^2))\n    return sd\nend\n\nfunc_dict = Dict(\n    \"std\" => x -> std(x; corrected = false),\n    \"median_std\" => median_sd,\n    \"5%\" => x -> quantile(x, 0.05),\n    \"median\" => median,\n    \"95%\" => x -> quantile(x, 0.95),\n)\n\nsummarystats(idata; var_names = (:mu, :tau), stat_funcs = func_dict, extend = false) source"},{"id":97,"pagetitle":"Stats","title":"ArviZ.r2_score","ref":"/ArviZ/stable/api/stats/#ArviZ.r2_score","content":" ArviZ.r2_score  —  Function R² for Bayesian regression models. Only valid for linear models. Note This function is forwarded to Python's  arviz.r2_score . The docstring of that function is included below. \n    Parameters\n    ----------\n    y_true: array-like of shape = (n_outputs,)\n        Ground truth (correct) target values.\n    y_pred: array-like of shape = (n_posterior_samples, n_outputs)\n        Estimated target values.\n\n    Returns\n    -------\n    Pandas Series with the following indices:\n    r2: Bayesian R²\n    r2_std: standard deviation of the Bayesian R².\n\n    See Also\n    --------\n    plot_lm : Posterior predictive and mean plots for regression-like data.\n\n    Examples\n    --------\n    Calculate R² for Bayesian regression models :\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data('regression1d')\n           ...: y_true = data.observed_data[\"y\"].values\n           ...: y_pred = data.posterior_predictive.stack(sample=(\"chain\", \"draw\"))[\"y\"].values.T\n           ...: az.r2_score(y_true, y_pred)\n\n     source"},{"id":98,"pagetitle":"Stats","title":"Pareto-smoothed importance sampling","ref":"/ArviZ/stable/api/stats/#Pareto-smoothed-importance-sampling","content":" Pareto-smoothed importance sampling"},{"id":99,"pagetitle":"Stats","title":"PSIS.PSISResult","ref":"/ArviZ/stable/api/stats/#PSIS.PSISResult","content":" PSIS.PSISResult  —  Type PSISResult Result of Pareto-smoothed importance sampling (PSIS) using  psis . Properties log_weights : un-normalized Pareto-smoothed log weights weights : normalized Pareto-smoothed weights (allocates a copy) pareto_shape : Pareto  $k=ξ$  shape parameter nparams : number of parameters in  log_weights ndraws : number of draws in  log_weights nchains : number of chains in  log_weights reff : the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size. ess : estimated effective sample size of estimate of mean using smoothed importance samples (see  ess_is ) log_weights_norm : the logarithm of the normalization constant of  log_weights tail_length : length of the upper tail of  log_weights  that was smoothed tail_dist : the generalized Pareto distribution that was fit to the tail of  log_weights . Note that the tail weights are scaled to have a maximum of 1, so  tail_dist * exp(maximum(log_ratios))  is the corresponding fit directly to the tail of  log_ratios . Diagnostic The  pareto_shape  parameter  $k=ξ$  of the generalized Pareto distribution  tail_dist  can be used to diagnose reliability and convergence of estimates using the importance weights  [VehtariSimpson2021] . if  $k < \\frac{1}{3}$ , importance sampling is stable, and importance sampling (IS) and PSIS both are reliable. if  $k ≤ \\frac{1}{2}$ , then the importance ratio distributon has finite variance, and the central limit theorem holds. As  $k$  approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE. if  $\\frac{1}{2} < k ≤ 0.7$ , then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime. if  $0.7 < k ≤ 1$ , then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended. if  $k > 1$ , then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes. See  paretoshapeplot  for a diagnostic plot."},{"id":100,"pagetitle":"Stats","title":"PSIS.psis","ref":"/ArviZ/stable/api/stats/#PSIS.psis","content":" PSIS.psis  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with one of the following sizes: (ndraws,) : a vector of draws for a single parameter from a single chain (nparams, ndraws) : a matrix of draws for a multiple parameter from a single chain (nparams, ndraws, nchains) : an array of draws for multiple parameters from multiple chains, e.g. as might be generated with Markov chain Monte Carlo. reff::Union{Real,AbstractVector} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(ndraws * nchains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. Keywords improved=false : If  true , use the adaptive empirical prior of  [Zhang2010] . If  false , use the simpler prior of  [ZhangStephens2009] , which is also used in  [VehtariSimpson2021] . warn=true : If  true , warning messages are delivered Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  paretoshapeplot  for a diagnostic plot."},{"id":101,"pagetitle":"Stats","title":"PSIS.psis!","ref":"/ArviZ/stable/api/stats/#PSIS.psis!","content":" PSIS.psis!  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with one of the following sizes: (ndraws,) : a vector of draws for a single parameter from a single chain (nparams, ndraws) : a matrix of draws for a multiple parameter from a single chain (nparams, ndraws, nchains) : an array of draws for multiple parameters from multiple chains, e.g. as might be generated with Markov chain Monte Carlo. reff::Union{Real,AbstractVector} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(ndraws * nchains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. Keywords improved=false : If  true , use the adaptive empirical prior of  [Zhang2010] . If  false , use the simpler prior of  [ZhangStephens2009] , which is also used in  [VehtariSimpson2021] . warn=true : If  true , warning messages are delivered Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  paretoshapeplot  for a diagnostic plot."},{"id":102,"pagetitle":"Stats","title":"Model assessment and selection","ref":"/ArviZ/stable/api/stats/#Model-assessment-and-selection","content":" Model assessment and selection"},{"id":103,"pagetitle":"Stats","title":"ArviZ.compare","ref":"/ArviZ/stable/api/stats/#ArviZ.compare","content":" ArviZ.compare  —  Function Compare models based on  their expected log pointwise predictive density (ELPD). Note This function is forwarded to Python's  arviz.compare . The docstring of that function is included below. \n    The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend loo. Read more theory here - in a paper by some of the\n    leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353\n\n    Parameters\n    ----------\n    compare_dict: dict of {str: InferenceData or ELPDData}\n        A dictionary of model names and :class:`arviz.InferenceData` or ``ELPDData``.\n    ic: str, optional\n        Method to estimate the ELPD, available options are \"loo\" or \"waic\". Defaults to\n        ``rcParams[\"stats.information_criterion\"]``.\n    method: str, optional\n        Method used to estimate the weights for each model. Available options are:\n\n        - 'stacking' : stacking of predictive distributions.\n        - 'BB-pseudo-BMA' : pseudo-Bayesian Model averaging using Akaike-type\n          weighting. The weights are stabilized using the Bayesian bootstrap.\n        - 'pseudo-BMA': pseudo-Bayesian Model averaging using Akaike-type\n          weighting, without Bootstrap stabilization (not recommended).\n\n        For more information read https://arxiv.org/abs/1704.02030\n    b_samples: int, optional default = 1000\n        Number of samples taken by the Bayesian bootstrap estimation.\n        Only useful when method = 'BB-pseudo-BMA'.\n        Defaults to ``rcParams[\"stats.ic_compare_method\"]``.\n    alpha: float, optional\n        The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. Only\n        useful when method = 'BB-pseudo-BMA'. When alpha=1 (default), the distribution is uniform\n        on the simplex. A smaller alpha will keeps the final weights more away from 0 and 1.\n    seed: int or np.random.RandomState instance, optional\n        If int or RandomState, use it for seeding Bayesian bootstrap. Only\n        useful when method = 'BB-pseudo-BMA'. Default None the global\n        :mod:`numpy.random` state is used.\n    scale: str, optional\n        Output scale for IC. Available options are:\n\n        - `log` : (default) log-score (after Vehtari et al. (2017))\n        - `negative_log` : -1 * (log-score)\n        - `deviance` : -2 * (log-score)\n\n        A higher log-score (or a lower deviance) indicates a model with better predictive\n        accuracy.\n    var_name: str, optional\n        If there is more than a single observed variable in the ``InferenceData``, which\n        should be used as the basis for comparison.\n\n    Returns\n    -------\n    A DataFrame, ordered from best to worst model (measured by the ELPD).\n    The index reflects the key with which the models are passed to this function. The columns are:\n    rank: The rank-order of the models. 0 is the best.\n    elpd: ELPD estimated either using (PSIS-LOO-CV `elpd_loo` or WAIC `elpd_waic`).\n        Higher ELPD indicates higher out-of-sample predictive fit (\"better\" model).\n        If `scale` is `deviance` or `negative_log` smaller values indicates\n        higher out-of-sample predictive fit (\"better\" model).\n    pIC: Estimated effective number of parameters.\n    elpd_diff: The difference in ELPD between two models.\n        If more than two models are compared, the difference is computed relative to the\n        top-ranked model, that always has a elpd_diff of 0.\n    weight: Relative weight for each model.\n        This can be loosely interpreted as the probability of each model (among the compared model)\n        given the data. By default the uncertainty in the weights estimation is considered using\n        Bayesian bootstrap.\n    SE: Standard error of the ELPD estimate.\n        If method = BB-pseudo-BMA these values are estimated using Bayesian bootstrap.\n    dSE: Standard error of the difference in ELPD between each model and the top-ranked model.\n        It's always 0 for the top-ranked model.\n    warning: A value of 1 indicates that the computation of the ELPD may not be reliable.\n        This could be indication of WAIC/LOO starting to fail see\n        http://arxiv.org/abs/1507.04544 for details.\n    scale: Scale used for the ELPD.\n\n    Examples\n    --------\n    Compare the centered and non centered models of the eight school problem:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data1 = az.load_arviz_data(\"non_centered_eight\")\n           ...: data2 = az.load_arviz_data(\"centered_eight\")\n           ...: compare_dict = {\"non centered\": data1, \"centered\": data2}\n           ...: az.compare(compare_dict)\n\n    Compare the models using PSIS-LOO-CV, returning the ELPD in log scale and calculating the\n    weights using the stacking method.\n\n    .. ipython::\n\n        In [1]: az.compare(compare_dict, ic=\"loo\", method=\"stacking\", scale=\"log\")\n\n    See Also\n    --------\n    loo :\n        Compute the ELPD using the Pareto smoothed importance sampling Leave-one-out\n        cross-validation method.\n    waic : Compute the ELPD using the widely applicable information criterion.\n    plot_compare : Summary plot for model comparison.\n\n    References\n    ----------\n    .. [1] Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using\n        leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017)\n        see https://doi.org/10.1007/s11222-016-9696-4\n\n     source"},{"id":104,"pagetitle":"Stats","title":"ArviZ.loo","ref":"/ArviZ/stable/api/stats/#ArviZ.loo","content":" ArviZ.loo  —  Function Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV). Note This function is forwarded to Python's  arviz.loo . The docstring of that function is included below. \n    Estimates the expected log pointwise predictive density (elpd) using Pareto-smoothed\n    importance sampling leave-one-out cross-validation (PSIS-LOO-CV). Also calculates LOO's\n    standard error and the effective number of parameters. Read more theory here\n    https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1507.02646\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of\n        :func:`arviz.convert_to_dataset` for details.\n    pointwise: bool, optional\n        If True the pointwise predictive accuracy will be returned. Defaults to\n        ``stats.ic_pointwise`` rcParam.\n    var_name : str, optional\n        The name of the variable in log_likelihood groups storing the pointwise log\n        likelihood data to use for loo computation.\n    reff: float, optional\n        Relative MCMC efficiency, ``ess / n`` i.e. number of effective samples divided by the number\n        of actual samples. Computed from trace by default.\n    scale: str\n        Output scale for loo. Available options are:\n\n        - ``log`` : (default) log-score\n        - ``negative_log`` : -1 * log-score\n        - ``deviance`` : -2 * log-score\n\n        A higher log-score (or a lower deviance or negative log_score) indicates a model with\n        better predictive accuracy.\n\n    Returns\n    -------\n    ELPDData object (inherits from :class:`pandas.Series`) with the following row/attributes:\n    elpd: approximated expected log pointwise predictive density (elpd)\n    se: standard error of the elpd\n    p_loo: effective number of parameters\n    shape_warn: bool\n        True if the estimated shape parameter of\n        Pareto distribution is greater than 0.7 for one or more samples\n    loo_i: array of pointwise predictive accuracy, only if pointwise True\n    pareto_k: array of Pareto shape values, only if pointwise True\n    scale: scale of the elpd\n\n        The returned object has a custom print method that overrides pd.Series method.\n\n    See Also\n    --------\n    compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation.\n    waic : Compute the widely applicable information criterion.\n    plot_compare : Summary plot for model comparison.\n    plot_elpd : Plot pointwise elpd differences between two or more models.\n    plot_khat : Plot Pareto tail indices for diagnosing convergence.\n\n    Examples\n    --------\n    Calculate LOO of a model:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data(\"centered_eight\")\n           ...: az.loo(data)\n\n    Calculate LOO of a model and return the pointwise values:\n\n    .. ipython::\n\n        In [2]: data_loo = az.loo(data, pointwise=True)\n           ...: data_loo.loo_i\n     source"},{"id":105,"pagetitle":"Stats","title":"ArviZ.loo_pit","ref":"/ArviZ/stable/api/stats/#ArviZ.loo_pit","content":" ArviZ.loo_pit  —  Function Compute leave one out (PSIS-LOO) probability integral transform (PIT) values. Note This function is forwarded to Python's  arviz.loo_pit . The docstring of that function is included below. \n    Parameters\n    ----------\n    idata: InferenceData\n        :class:`arviz.InferenceData` object.\n    y: array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat: array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as y plus an\n        extra dimension at the end of size n_samples (chains and draws stacked). If str or\n        None, ``idata`` must contain the posterior predictive group. If None, y_hat is taken\n        equal to y, thus, y must be str too.\n    log_weights: array or DataArray\n        Smoothed log_weights. It must have the same shape as ``y_hat``\n    dask_kwargs : dict, optional\n        Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.\n\n    Returns\n    -------\n    loo_pit: array or DataArray\n        Value of the LOO-PIT at each observed data point.\n\n    See Also\n    --------\n    plot_loo_pit : Plot Leave-One-Out probability integral transformation (PIT) predictive checks.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out\n          cross-validation (PSIS-LOO-CV).\n    plot_elpd : Plot pointwise elpd differences between two or more models.\n    plot_khat : Plot Pareto tail indices for diagnosing convergence.\n\n    Examples\n    --------\n    Calculate LOO-PIT values using as test quantity the observed values themselves.\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data(\"centered_eight\")\n           ...: az.loo_pit(idata=data, y=\"obs\")\n\n    Calculate LOO-PIT values using as test quantity the square of the difference between\n    each observation and `mu`. Both ``y`` and ``y_hat`` inputs will be array-like,\n    but ``idata`` will still be passed in order to calculate the ``log_weights`` from\n    there.\n\n    .. ipython::\n\n        In [1]: T = data.observed_data.obs - data.posterior.mu.median(dim=(\"chain\", \"draw\"))\n           ...: T_hat = data.posterior_predictive.obs - data.posterior.mu\n           ...: T_hat = T_hat.stack(__sample__=(\"chain\", \"draw\"))\n           ...: az.loo_pit(idata=data, y=T**2, y_hat=T_hat**2)\n\n     source"},{"id":106,"pagetitle":"Stats","title":"ArviZ.waic","ref":"/ArviZ/stable/api/stats/#ArviZ.waic","content":" ArviZ.waic  —  Function Compute the widely applicable information criterion. Note This function is forwarded to Python's  arviz.waic . The docstring of that function is included below. \n    Estimates the expected log pointwise predictive density (elpd) using WAIC. Also calculates the\n    WAIC's standard error and the effective number of parameters.\n    Read more theory here https://arxiv.org/abs/1507.04544 and here https://arxiv.org/abs/1004.2316\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_inference_data` for details.\n    pointwise: bool\n        If True the pointwise predictive accuracy will be returned. Defaults to\n        ``stats.ic_pointwise`` rcParam.\n    var_name : str, optional\n        The name of the variable in log_likelihood groups storing the pointwise log\n        likelihood data to use for waic computation.\n    scale: str\n        Output scale for WAIC. Available options are:\n\n        - `log` : (default) log-score\n        - `negative_log` : -1 * log-score\n        - `deviance` : -2 * log-score\n\n        A higher log-score (or a lower deviance or negative log_score) indicates a model with\n        better predictive accuracy.\n    dask_kwargs : dict, optional\n        Dask related kwargs passed to :func:`~arviz.wrap_xarray_ufunc`.\n\n    Returns\n    -------\n    ELPDData object (inherits from :class:`pandas.Series`) with the following row/attributes:\n    elpd_waic: approximated expected log pointwise predictive density (elpd)\n    se: standard error of the elpd\n    p_waic: effective number parameters\n    var_warn: bool\n        True if posterior variance of the log predictive densities exceeds 0.4\n    waic_i: :class:`~xarray.DataArray` with the pointwise predictive accuracy,\n            only if pointwise=True\n    scale: scale of the elpd\n\n        The returned object has a custom print method that overrides pd.Series method.\n\n    See Also\n    --------\n    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n    compare : Compare models based on PSIS-LOO-CV or WAIC.\n    plot_compare : Summary plot for model comparison.\n\n    Examples\n    --------\n    Calculate WAIC of a model:\n\n    .. ipython::\n\n        In [1]: import arviz as az\n           ...: data = az.load_arviz_data(\"centered_eight\")\n           ...: az.waic(data)\n\n    Calculate WAIC of a model and return the pointwise values:\n\n    .. ipython::\n\n        In [2]: data_waic = az.waic(data, pointwise=True)\n           ...: data_waic.waic_i\n     source VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] ZhangStephens2009 Jin Zhang & Michael A. Stephens (2009) A New and Efficient Estimation Method for the Generalized Pareto Distribution, Technometrics, 51:3, 316-325, DOI:  10.1198/tech.2009.08017 Zhang2010 Jin Zhang (2010) Improving on Estimation for the Generalized Pareto Distribution, Technometrics, 52:3, 335-339, DOI:  10.1198/TECH.2010.09206 VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] ZhangStephens2009 Jin Zhang & Michael A. Stephens (2009) A New and Efficient Estimation Method for the Generalized Pareto Distribution, Technometrics, 51:3, 316-325, DOI:  10.1198/tech.2009.08017 Zhang2010 Jin Zhang (2010) Improving on Estimation for the Generalized Pareto Distribution, Technometrics, 52:3, 335-339, DOI:  10.1198/TECH.2010.09206"},{"id":109,"pagetitle":"Creating custom plots","title":"Environment","ref":"/ArviZ/stable/creating_custom_plots/#Environment","content":" Environment using Pkg, InteractiveUtils using PlutoUI with_terminal(Pkg.status; color=false) Status `~/work/ArviZ.jl/ArviZ.jl/docs/Project.toml`\n  [cbdf2221] AlgebraOfGraphics v0.6.12\n  [131c737c] ArviZ v0.6.7 `~/work/ArviZ.jl/ArviZ.jl`\n⌅ [13f3f980] CairoMakie v0.8.13\n  [593b3428] CmdStan v6.6.0\n  [a93c6f00] DataFrames v1.4.1\n⌅ [0703355e] DimensionalData v0.21.3\n  [31c24e10] Distributions v0.25.76\n  [e30172f5] Documenter v0.27.23\n  [c7f686f2] MCMCChains v5.5.0\n⌅ [359b1769] PlutoStaticHTML v5.0.13\n  [7f904dfe] PlutoUI v0.7.48\n  [438e738f] PyCall v1.94.1\n  [d330b81b] PyPlot v2.11.0\n  [754583d1] SampleChains v0.5.1\n  [fce5fe82] Turing v0.21.12\n  [f43a241f] Downloads v1.6.0\n  [37e2e46d] LinearAlgebra\nInfo Packages marked with ⌅ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`\n with_terminal(versioninfo) Julia Version 1.8.2\nCommit 36034abf260 (2022-09-29 15:21 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 2 × Intel(R) Xeon(R) Platinum 8370C CPU @ 2.80GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-13.0.1 (ORCJIT, icelake-server)\n  Threads: 2 on 2 virtual cores\nEnvironment:\n  JULIA_CMDSTAN_HOME = /home/runner/work/ArviZ.jl/ArviZ.jl/.cmdstan//cmdstan-2.25.0/\n  JULIA_NUM_THREADS = 2\n  JULIA_REVISE_WORKER_ONLY = 1\n"},{"id":112,"pagetitle":"Matplotlib","title":"Matplotlib Example Gallery","ref":"/ArviZ/stable/mpl_examples/#Matplotlib-Example-Gallery","content":" Matplotlib Example Gallery"},{"id":113,"pagetitle":"Matplotlib","title":"Autocorrelation Plot","ref":"/ArviZ/stable/mpl_examples/#Autocorrelation-Plot","content":" Autocorrelation Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_autocorr(data; var_names=[:tau, :mu])\n\ngcf() See  plot_autocorr"},{"id":114,"pagetitle":"Matplotlib","title":"Bayesian P-Value Posterior Plot","ref":"/ArviZ/stable/mpl_examples/#Bayesian-P-Value-Posterior-Plot","content":" Bayesian P-Value Posterior Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nplot_bpv(data)\n\ngcf() See  plot_bpv"},{"id":115,"pagetitle":"Matplotlib","title":"Bayesian P-Value with Median T Statistic Posterior Plot","ref":"/ArviZ/stable/mpl_examples/#Bayesian-P-Value-with-Median-T-Statistic-Posterior-Plot","content":" Bayesian P-Value with Median T Statistic Posterior Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nplot_bpv(data; kind=:t_stat, t_stat=\"0.5\")\n\ngcf() See  plot_bpv"},{"id":116,"pagetitle":"Matplotlib","title":"Compare Plot","ref":"/ArviZ/stable/mpl_examples/#Compare-Plot","content":" Compare Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nmodel_compare = compare(\n    Dict(\n        \"Centered 8 schools\" => load_example_data(\"centered_eight\"),\n        \"Non-centered 8 schools\" => load_example_data(\"non_centered_eight\"),\n    ),\n)\nplot_compare(model_compare; figsize=(12, 4))\n\ngcf() See  compare ,  plot_compare"},{"id":117,"pagetitle":"Matplotlib","title":"Density Plot","ref":"/ArviZ/stable/mpl_examples/#Density-Plot","content":" Density Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered_data = load_example_data(\"centered_eight\")\nnon_centered_data = load_example_data(\"non_centered_eight\")\nplot_density(\n    [centered_data, non_centered_data];\n    data_labels=[\"Centered\", \"Non Centered\"],\n    var_names=[:theta],\n    shade=0.1,\n)\n\ngcf() See  plot_density"},{"id":118,"pagetitle":"Matplotlib","title":"Dist Plot","ref":"/ArviZ/stable/mpl_examples/#Dist-Plot","content":" Dist Plot using Random\nusing Distributions\nusing PyPlot\nusing ArviZ\n\nRandom.seed!(308)\n\nArviZ.use_style(\"arviz-darkgrid\")\n\na = rand(Poisson(4), 1000)\nb = rand(Normal(0, 1), 1000)\n_, ax = plt.subplots(1, 2; figsize=(10, 4))\nplot_dist(a; color=\"C1\", label=\"Poisson\", ax=ax[1])\nplot_dist(b; color=\"C2\", label=\"Gaussian\", ax=ax[2])\n\ngcf() See  plot_dist"},{"id":119,"pagetitle":"Matplotlib","title":"ELPD Plot","ref":"/ArviZ/stable/mpl_examples/#ELPD-Plot","content":" ELPD Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nd1 = load_example_data(\"centered_eight\")\nd2 = load_example_data(\"non_centered_eight\")\nplot_elpd(Dict(\"Centered eight\" => d1, \"Non centered eight\" => d2); xlabels=true)\n\ngcf() See  plot_elpd"},{"id":120,"pagetitle":"Matplotlib","title":"Energy Plot","ref":"/ArviZ/stable/mpl_examples/#Energy-Plot","content":" Energy Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_energy(data; figsize=(12, 8))\n\ngcf() See  plot_energy"},{"id":121,"pagetitle":"Matplotlib","title":"ESS Evolution Plot","ref":"/ArviZ/stable/mpl_examples/#ESS-Evolution-Plot","content":" ESS Evolution Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nplot_ess(idata; var_names=[:b], kind=:evolution)\n\ngcf() See  plot_ess"},{"id":122,"pagetitle":"Matplotlib","title":"ESS Local Plot","ref":"/ArviZ/stable/mpl_examples/#ESS-Local-Plot","content":" ESS Local Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"non_centered_eight\")\nplot_ess(idata; var_names=[:mu], kind=:local, marker=\"_\", ms=20, mew=2, rug=true)\n\ngcf() See  plot_ess"},{"id":123,"pagetitle":"Matplotlib","title":"ESS Quantile Plot","ref":"/ArviZ/stable/mpl_examples/#ESS-Quantile-Plot","content":" ESS Quantile Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nplot_ess(idata; var_names=[:sigma], kind=:quantile, color=\"C4\")\n\ngcf() See  plot_ess"},{"id":124,"pagetitle":"Matplotlib","title":"Forest Plot","ref":"/ArviZ/stable/mpl_examples/#Forest-Plot","content":" Forest Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered_data = load_example_data(\"centered_eight\")\nnon_centered_data = load_example_data(\"non_centered_eight\")\nplot_forest(\n    [centered_data, non_centered_data];\n    model_names=[\"Centered\", \"Non Centered\"],\n    var_names=[:mu],\n)\ntitle(\"Estimated theta for eight schools model\")\n\ngcf() See  plot_forest"},{"id":125,"pagetitle":"Matplotlib","title":"Ridge Plot","ref":"/ArviZ/stable/mpl_examples/#Ridge-Plot","content":" Ridge Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nrugby_data = load_example_data(\"rugby\")\nplot_forest(\n    rugby_data;\n    kind=:ridgeplot,\n    var_names=[:defs],\n    linewidth=4,\n    combined=true,\n    ridgeplot_overlap=1.5,\n    colors=:blue,\n    figsize=(9, 4),\n)\ntitle(\"Relative defensive strength\\nof Six Nation rugby teams\")\n\ngcf() See  plot_forest"},{"id":126,"pagetitle":"Matplotlib","title":"Plot HDI","ref":"/ArviZ/stable/mpl_examples/#Plot-HDI","content":" Plot HDI using Random\nusing PyPlot\nusing ArviZ\n\nRandom.seed!(308)\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nx_data = randn(100)\ny_data = 2 .+ x_data .* 0.5\ny_data_rep = 0.5 .* randn(200, 100) .+ transpose(y_data)\nplot(x_data, y_data; color=\"C6\")\nplot_hdi(x_data, y_data_rep; color=:k, plot_kwargs=Dict(:ls => \"--\"))\n\ngcf() See  plot_hdi"},{"id":127,"pagetitle":"Matplotlib","title":"Joint Plot","ref":"/ArviZ/stable/mpl_examples/#Joint-Plot","content":" Joint Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_pair(\n    data;\n    var_names=[:theta],\n    coords=Dict(:school => [\"Choate\", \"Phillips Andover\"]),\n    kind=:hexbin,\n    marginals=true,\n    figsize=(10, 10),\n)\n\ngcf() See  plot_pair"},{"id":128,"pagetitle":"Matplotlib","title":"KDE Plot","ref":"/ArviZ/stable/mpl_examples/#KDE-Plot","content":" KDE Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\n\n## Combine different posterior draws from different chains\nobs = data.posterior_predictive.obs\nsize_obs = size(obs)\ny_hat = reshape(obs, prod(size_obs[1:2]), size_obs[3:end]...)\n\nplot_kde(\n    y_hat;\n    label=\"Estimated Effect\\n of SAT Prep\",\n    rug=true,\n    plot_kwargs=Dict(:linewidth => 2, :color => :black),\n    rug_kwargs=Dict(:color => :black),\n)\n\ngcf() See  plot_kde"},{"id":129,"pagetitle":"Matplotlib","title":"2d KDE","ref":"/ArviZ/stable/mpl_examples/#d-KDE","content":" 2d KDE using Random\nusing PyPlot\nusing ArviZ\n\nRandom.seed!(308)\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nplot_kde(rand(100), rand(100))\n\ngcf() See  plot_kde"},{"id":130,"pagetitle":"Matplotlib","title":"KDE Quantiles Plot","ref":"/ArviZ/stable/mpl_examples/#KDE-Quantiles-Plot","content":" KDE Quantiles Plot using Random\nusing Distributions\nusing PyPlot\nusing ArviZ\n\nRandom.seed!(308)\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndist = rand(Beta(rand(Uniform(0.5, 10)), 5), 1000)\nplot_kde(dist; quantiles=[0.25, 0.5, 0.75])\n\ngcf() See  plot_kde"},{"id":131,"pagetitle":"Matplotlib","title":"Pareto Shape Plot","ref":"/ArviZ/stable/mpl_examples/#Pareto-Shape-Plot","content":" Pareto Shape Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nloo_data = loo(idata)\nplot_khat(loo_data; show_bins=true)\n\ngcf() See  loo ,  plot_khat"},{"id":132,"pagetitle":"Matplotlib","title":"LOO-PIT ECDF Plot","ref":"/ArviZ/stable/mpl_examples/#LOO-PIT-ECDF-Plot","content":" LOO-PIT ECDF Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\n\nplot_loo_pit(idata; y=:y, ecdf=true, color=:maroon)\n\ngcf() See  psis ,  plot_loo_pit"},{"id":133,"pagetitle":"Matplotlib","title":"LOO-PIT Overlay Plot","ref":"/ArviZ/stable/mpl_examples/#LOO-PIT-Overlay-Plot","content":" LOO-PIT Overlay Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"non_centered_eight\")\nplot_loo_pit(; idata, y=:obs, color=:indigo)\n\ngcf() See  plot_loo_pit"},{"id":134,"pagetitle":"Matplotlib","title":"Quantile Monte Carlo Standard Error Plot","ref":"/ArviZ/stable/mpl_examples/#Quantile-Monte-Carlo-Standard-Error-Plot","content":" Quantile Monte Carlo Standard Error Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_mcse(data; var_names=[:tau, :mu], rug=true, extra_methods=true)\n\ngcf() See  plot_mcse"},{"id":135,"pagetitle":"Matplotlib","title":"Quantile MCSE Errobar Plot","ref":"/ArviZ/stable/mpl_examples/#Quantile-MCSE-Errobar-Plot","content":" Quantile MCSE Errobar Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"radon\")\nplot_mcse(data; var_names=[:sigma_a], color=\"C4\", errorbar=true)\n\ngcf() See  plot_mcse"},{"id":136,"pagetitle":"Matplotlib","title":"Pair Plot","ref":"/ArviZ/stable/mpl_examples/#Pair-Plot","content":" Pair Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered; var_names=[:theta, :mu, :tau], coords, divergences=true, textsize=22\n)\n\ngcf() See  plot_pair"},{"id":137,"pagetitle":"Matplotlib","title":"Hexbin Pair Plot","ref":"/ArviZ/stable/mpl_examples/#Hexbin-Pair-Plot","content":" Hexbin Pair Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[:theta, :mu, :tau],\n    kind=:hexbin,\n    coords,\n    colorbar=true,\n    divergences=true,\n)\n\ngcf() See  plot_pair"},{"id":138,"pagetitle":"Matplotlib","title":"KDE Pair Plot","ref":"/ArviZ/stable/mpl_examples/#KDE-Pair-Plot","content":" KDE Pair Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[:theta, :mu, :tau],\n    kind=:kde,\n    coords,\n    divergences=true,\n    textsize=22,\n)\n\ngcf() See  plot_pair"},{"id":139,"pagetitle":"Matplotlib","title":"Point Estimate Pair Plot","ref":"/ArviZ/stable/mpl_examples/#Point-Estimate-Pair-Plot","content":" Point Estimate Pair Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[:mu, :theta],\n    kind=[:scatter, :kde],\n    kde_kwargs=Dict(:fill_last => false),\n    marginals=true,\n    coords,\n    point_estimate=:median,\n    figsize=(10, 8),\n)\n\ngcf() See  plot_pair"},{"id":140,"pagetitle":"Matplotlib","title":"Parallel Plot","ref":"/ArviZ/stable/mpl_examples/#Parallel-Plot","content":" Parallel Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nax = plot_parallel(data; var_names=[:theta, :tau, :mu])\nax.set_xticklabels(ax.get_xticklabels(); rotation=70)\ndraw()\n\ngcf() See  plot_parallel"},{"id":141,"pagetitle":"Matplotlib","title":"Posterior Plot","ref":"/ArviZ/stable/mpl_examples/#Posterior-Plot","content":" Posterior Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\ncoords = Dict(:school => [\"Choate\"])\nplot_posterior(data; var_names=[:mu, :theta], coords, rope=(-1, 1))\n\ngcf() See  plot_posterior"},{"id":142,"pagetitle":"Matplotlib","title":"Posterior Predictive Check Plot","ref":"/ArviZ/stable/mpl_examples/#Posterior-Predictive-Check-Plot","content":" Posterior Predictive Check Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_ppc(data; data_pairs=Dict(:obs => :obs), alpha=0.03, figsize=(12, 6), textsize=14)\n\ngcf() See  plot_ppc"},{"id":143,"pagetitle":"Matplotlib","title":"Posterior Predictive Check Cumulative Plot","ref":"/ArviZ/stable/mpl_examples/#Posterior-Predictive-Check-Cumulative-Plot","content":" Posterior Predictive Check Cumulative Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_ppc(data; alpha=0.3, kind=:cumulative, figsize=(12, 6), textsize=14)\n\ngcf() See  plot_ppc"},{"id":144,"pagetitle":"Matplotlib","title":"Rank Plot","ref":"/ArviZ/stable/mpl_examples/#Rank-Plot","content":" Rank Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_rank(data; var_names=[:tau, :mu])\n\ngcf() See  plot_rank"},{"id":145,"pagetitle":"Matplotlib","title":"Separation Plot","ref":"/ArviZ/stable/mpl_examples/#Separation-Plot","content":" Separation Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"classification10d\")\nplot_separation(data; y=:outcome, y_hat=:outcome, figsize=(8, 1))\n\ngcf() See  plot_separation"},{"id":146,"pagetitle":"Matplotlib","title":"Trace Plot","ref":"/ArviZ/stable/mpl_examples/#Trace-Plot","content":" Trace Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_trace(data; var_names=[:tau, :mu])\n\ngcf() See  plot_trace"},{"id":147,"pagetitle":"Matplotlib","title":"Violin Plot","ref":"/ArviZ/stable/mpl_examples/#Violin-Plot","content":" Violin Plot using PyPlot\nusing ArviZ\n\nArviZ.use_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_violin(data; var_names=[:mu, :tau])\n\ngcf() See  plot_violin"},{"id":148,"pagetitle":"Matplotlib","title":"Styles","ref":"/ArviZ/stable/mpl_examples/#Styles","content":" Styles using PyPlot\nusing PyCall\nusing Distributions\nusing ArviZ\n\nx = range(0, 1; length=100)\ndist = pdf.(Beta(2, 5), x)\n\nstyle_list = [\n    \"default\",\n    [\"default\", \"arviz-colors\"],\n    \"arviz-darkgrid\",\n    \"arviz-whitegrid\",\n    \"arviz-white\",\n]\n\nfig = figure(; figsize=(12, 12))\nfor (idx, style) in enumerate(style_list)\n    @pywith plt.style.context(style) begin\n        ax = fig.add_subplot(3, 2, idx; label=idx)\n        for i in 0:9\n            ax.plot(x, dist .- i, \"C$i\"; label=\"C$i\")\n        end\n        ax.set_title(style)\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"f(x)\"; rotation=0, labelpad=15)\n        ax.legend(; bbox_to_anchor=(1, 1))\n        draw()\n    end\nend\ntight_layout()\n\ngcf()"},{"id":154,"pagetitle":"Working with InferenceData","title":"Working with  InferenceData","ref":"/ArviZ/stable/working_with_inference_data/#working-with-inference-data","content":" Working with  InferenceData using ArviZ, DimensionalData, Statistics Here we present a collection of common manipulations you can use while working with  InferenceData . Let's load one of ArviZ's example datasets.  posterior ,  posterior_predictive , etc are the groups stored in  idata , and they are stored as  Dataset s. In this HTML view, you can click a group name to expand a summary of the group. idata = load_example_data(\"centered_eight\") InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" posterior_predictive Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.489022\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" sample_stats Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 12 layers:\n  :tune             Bool dims: Dim{:chain}, Dim{:draw} (4×500)\n  :depth            Int64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :tree_size        Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :lp               Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :energy_error     Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :step_size_bar    Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :max_energy_error Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :energy           Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :mean_tree_accept Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :step_size        Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :diverging        Bool dims: Dim{:chain}, Dim{:draw} (4×500)\n  :log_likelihood   Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.485802\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" prior Dataset with dimensions: \n  Dim{:chain} Sampled StepRangeLen(0.0, 0.0, 1) ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 5 layers:\n  :tau       Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :tau_log__ Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :mu        Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :theta     Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n  :obs       Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.490387\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.491909\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" Info Dataset s are  DimensionalData.AbstractDimStack s and can be used identically.   The variables a  Dataset  contains are called \"layers\", and dimensions of the same name that appear in more than one layer within a  Dataset  must have the same indices. InferenceData  behaves like a  NamedTuple  and can be used similarly. Note that unlike a  NamedTuple , the groups always appear in a specific order. length(idata) # number of groups 5 keys(idata) # group names (:posterior, :posterior_predictive, :sample_stats, :prior, :observed_data)"},{"id":155,"pagetitle":"Working with InferenceData","title":"Get the dataset corresponding to a single group","ref":"/ArviZ/stable/working_with_inference_data/#Get-the-dataset-corresponding-to-a-single-group","content":" Get the dataset corresponding to a single group Group datasets can be accessed both as properties or as indexed items. post = idata.posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at                => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library         => \"pymc3\" post  is the dataset itself, so this is a non-allocating operation. idata[:posterior] === post true InferenceData  supports a more advanced indexing syntax, which we'll see later."},{"id":156,"pagetitle":"Working with InferenceData","title":"Getting a new  InferenceData  with a subset of groups","ref":"/ArviZ/stable/working_with_inference_data/#Getting-a-new-InferenceData-with-a-subset-of-groups","content":" Getting a new  InferenceData  with a subset of groups We can index by a collection of group names to get a new  InferenceData  with just those groups. This is also non-allocating. idata_sub = idata[(:posterior, :posterior_predictive)] InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" posterior_predictive Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.489022\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\""},{"id":157,"pagetitle":"Working with InferenceData","title":"Adding groups to an  InferenceData","ref":"/ArviZ/stable/working_with_inference_data/#Adding-groups-to-an-InferenceData","content":" Adding groups to an  InferenceData InferenceData  is immutable, so to add or replace groups we use  merge  to create a new object. merge(idata_sub, idata[(:observed_data, :prior)]) InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" posterior_predictive Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.489022\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" prior Dataset with dimensions: \n  Dim{:chain} Sampled StepRangeLen(0.0, 0.0, 1) ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 5 layers:\n  :tau       Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :tau_log__ Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :mu        Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :theta     Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n  :obs       Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.490387\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.491909\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" We can also use  Base.setindex  to out-of-place add or replace a single group. Base.setindex(idata_sub, idata.prior, :prior) InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" posterior_predictive Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.489022\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" prior Dataset with dimensions: \n  Dim{:chain} Sampled StepRangeLen(0.0, 0.0, 1) ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 5 layers:\n  :tau       Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :tau_log__ Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :mu        Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :theta     Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n  :obs       Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.490387\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\""},{"id":158,"pagetitle":"Working with InferenceData","title":"Add a new variable","ref":"/ArviZ/stable/working_with_inference_data/#Add-a-new-variable","content":" Add a new variable Dataset  is also immutable. So while the values within the underlying data arrays can be mutated, layers cannot be added or removed from  Dataset s, and groups cannot be added/removed from  InferenceData . Instead, we do this out-of-place also using  merge . merge(post, (log_tau=log.(post[:tau]),)) Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 4 layers:\n  :mu      Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :theta   Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n  :tau     Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :log_tau Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at                => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library         => \"pymc3\""},{"id":159,"pagetitle":"Working with InferenceData","title":"Obtain an array for a given parameter","ref":"/ArviZ/stable/working_with_inference_data/#Obtain-an-array-for-a-given-parameter","content":" Obtain an array for a given parameter Let’s say we want to get the values for  mu  as an array. Parameters can be accessed with either property or index syntax. post.tau 4×500 DimArray{Float64,2} tau with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points\n    0        1        2         3        …  497        498        499\n 0  3.7301   2.07538  3.70299   4.14612      10.1079     8.07999    7.72886\n 1  1.19333  1.19333  1.19333   3.0369       13.922      8.86992    4.76318\n 2  5.13725  4.26438  2.14143   1.44099       2.81184   12.1797     4.45297\n 3  0.50007  0.50007  0.902267  1.17612       8.34563    7.71079    5.4068 post[:tau] === post.tau true To remove the dimensions, just use  parent  to retrieve the underlying array. parent(post.tau) 4×500 Matrix{Float64}:\n 3.7301   2.07538  3.70299   4.14612  …  10.1079    8.07999  7.72886\n 1.19333  1.19333  1.19333   3.0369      13.922     8.86992  4.76318\n 5.13725  4.26438  2.14143   1.44099      2.81184  12.1797   4.45297\n 0.50007  0.50007  0.902267  1.17612      8.34563   7.71079  5.4068"},{"id":160,"pagetitle":"Working with InferenceData","title":"Get the dimension lengths","ref":"/ArviZ/stable/working_with_inference_data/#Get-the-dimension-lengths","content":" Get the dimension lengths Let’s check how many groups are in our hierarchical model. size(idata.observed_data, :school) 8"},{"id":161,"pagetitle":"Working with InferenceData","title":"Get coordinate/index values","ref":"/ArviZ/stable/working_with_inference_data/#Get-coordinate/index-values","content":" Get coordinate/index values What are the names of the groups in our hierarchical model? You can access them from the coordinate name  school  in this case. DimensionalData.index(idata.observed_data, :school) 8-element Vector{String}:\n \"Choate\"\n \"Deerfield\"\n \"Phillips Andover\"\n \"Phillips Exeter\"\n \"Hotchkiss\"\n \"Lawrenceville\"\n \"St. Paul's\"\n \"Mt. Hermon\""},{"id":162,"pagetitle":"Working with InferenceData","title":"Get a subset of chains","ref":"/ArviZ/stable/working_with_inference_data/#Get-a-subset-of-chains","content":" Get a subset of chains Let’s keep only chain 0 here. For the subset to take effect on all relevant  InferenceData  groups –  posterior ,  sample_stats ,  log_likelihood , and  posterior_predictive  – we will index  InferenceData  instead of  Dataset . Here we use DimensionalData's  At  selector. Its  other selectors  are also supported. idata[chain=At(0)] InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled Int64[0] ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" posterior_predictive Dataset with dimensions: \n  Dim{:chain} Sampled Int64[0] ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.489022\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" sample_stats Dataset with dimensions: \n  Dim{:chain} Sampled Int64[0] ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 12 layers:\n  :tune             Bool dims: Dim{:chain}, Dim{:draw} (1×500)\n  :depth            Int64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :tree_size        Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :lp               Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :energy_error     Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :step_size_bar    Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :max_energy_error Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :energy           Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :mean_tree_accept Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :step_size        Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :diverging        Bool dims: Dim{:chain}, Dim{:draw} (1×500)\n  :log_likelihood   Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.485802\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" prior Dataset with dimensions: \n  Dim{:chain} Sampled Float64[0.0] ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 5 layers:\n  :tau       Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :tau_log__ Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :mu        Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :theta     Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n  :obs       Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.490387\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.491909\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" Note that in this case,  prior  only has a chain of 0. If it also had the other chains, we could have passed  chain=At([0, 2])  to subset by chains 0 and 2. Warning If we used  idata[chain=[0, 2]]  without the  At  selector, this is equivalent to  idata[chain=DimensionalData.index(idata.posterior, :chain)[0, 2]] , that is,  [0, 2]  indexes an array of dimension indices, which here would error.   But if we had requested  idata[chain=[1, 2]]  we would not hit an error, but we would index the wrong chains.   So it's important to always use a selector to index by values of dimension indices."},{"id":163,"pagetitle":"Working with InferenceData","title":"Remove the first  $n$  draws (burn-in)","ref":"/ArviZ/stable/working_with_inference_data/#Remove-the-first-n-draws-(burn-in)","content":" Remove the first  $n$  draws (burn-in) Let’s say we want to remove the first 100 draws from all the chains and all  InferenceData  groups with draws. To do this we use the  ..  syntax from IntervalSets.jl, which is exported by DimensionalData. idata[draw=100 .. Inf] InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 100:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×400×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" posterior_predictive Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 100:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×400×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.489022\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" sample_stats Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 100:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 12 layers:\n  :tune             Bool dims: Dim{:chain}, Dim{:draw} (4×400)\n  :depth            Int64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :tree_size        Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :lp               Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :energy_error     Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :step_size_bar    Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :max_energy_error Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :energy           Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :mean_tree_accept Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :step_size        Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :diverging        Bool dims: Dim{:chain}, Dim{:draw} (4×400)\n  :log_likelihood   Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×400×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.485802\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" prior Dataset with dimensions: \n  Dim{:chain} Sampled StepRangeLen(0.0, 0.0, 1) ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 100:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 5 layers:\n  :tau       Float64 dims: Dim{:chain}, Dim{:draw} (1×400)\n  :tau_log__ Float64 dims: Dim{:chain}, Dim{:draw} (1×400)\n  :mu        Float64 dims: Dim{:chain}, Dim{:draw} (1×400)\n  :theta     Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×400×8)\n  :obs       Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×400×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.490387\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.491909\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" If you check the object you will see that the groups  posterior ,  posterior_predictive ,  prior , and  sample_stats  have 400 draws compared to  idata , which has 500. The group  observed_data  has not been affected because it does not have the  draw  dimension. Alternatively, you can change a subset of groups by combining indexing styles with  merge . Here we use this to build a new  InferenceData  where we have discarded the first 100 draws only from  posterior . merge(idata, idata[(:posterior,), draw=100 .. Inf]) InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 100:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×400×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (4×400)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" posterior_predictive Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.489022\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" sample_stats Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 12 layers:\n  :tune             Bool dims: Dim{:chain}, Dim{:draw} (4×500)\n  :depth            Int64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :tree_size        Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :lp               Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :energy_error     Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :step_size_bar    Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :max_energy_error Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :energy           Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :mean_tree_accept Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :step_size        Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :diverging        Bool dims: Dim{:chain}, Dim{:draw} (4×500)\n  :log_likelihood   Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.485802\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" prior Dataset with dimensions: \n  Dim{:chain} Sampled StepRangeLen(0.0, 0.0, 1) ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 5 layers:\n  :tau       Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :tau_log__ Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :mu        Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :theta     Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n  :obs       Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.490387\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.491909\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\""},{"id":164,"pagetitle":"Working with InferenceData","title":"Compute posterior mean values along draw and chain dimensions","ref":"/ArviZ/stable/working_with_inference_data/#Compute-posterior-mean-values-along-draw-and-chain-dimensions","content":" Compute posterior mean values along draw and chain dimensions To compute the mean value of the posterior samples, do the following: mean(post) (mu = 4.092610850912027, theta = 4.56047268323059, tau = 4.088982928754772) This computes the mean along all dimensions, discarding all dimensions and returning the result as a  NamedTuple . This may be what you wanted for  mu  and  tau , which have only two dimensions ( chain  and  draw ), but maybe not what you expected for  theta , which has one more dimension  school . You can specify along which dimension you want to compute the mean (or other functions), which instead returns a  Dataset . mean(post; dims=(:chain, :draw)) Dataset with dimensions: \n  Dim{:chain} Sampled 1.5:4.0:1.5 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 249.5:500.0:249.5 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (1×1)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×1×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (1×1)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at                => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library         => \"pymc3\" The singleton dimensions of  chain  and  draw  now contain meaningless indices, so you may want to discard them, which you can do with  dropdims . dropdims(mean(post; dims=(:chain, :draw)); dims=(:chain, :draw)) Dataset with dimensions: \n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: \n  :theta Float64 dims: Dim{:school} (8)\n  :tau   Float64 dims: \n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at                => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library         => \"pymc3\""},{"id":165,"pagetitle":"Working with InferenceData","title":"Renaming a dimension","ref":"/ArviZ/stable/working_with_inference_data/#Renaming-a-dimension","content":" Renaming a dimension We can rename a dimension in a  Dataset  using DimensionalData's  set  method: theta_bis = set(post.theta; school=:school_bis) 4×500×8 DimArray{Float64,3} theta with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school_bis} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n[:, :, 1]\n     0         1        2        …  497       498        499\n 0   1.66865  -6.23936  2.1951       21.6306    9.29298   11.7154\n 1   8.09621   8.09621  8.09621      15.2759   14.7355    -4.83704\n 2  14.5709   12.6867   9.66618       2.6685    5.36165   13.4391\n 3   4.32639   4.32639  2.99078      14.1863   -1.42095   -0.0501594\n[and 7 more slices...] We can use this, for example, to broadcast functions across multiple arrays, automatically matching up shared dimensions, using  DimensionalData.broadcast_dims . theta_school_diff = broadcast_dims(-, post.theta, theta_bis) 4×500×8×8 DimArray{Float64,4} theta with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:school_bis} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n[:, :, 1, 1]\n    0    1    2    3    4    5    6    …  495    496    497    498    499\n 0  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0    0.0    0.0    0.0    0.0\n 1  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0    0.0    0.0    0.0    0.0\n 2  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0    0.0    0.0    0.0    0.0\n 3  0.0  0.0  0.0  0.0  0.0  0.0  0.0       0.0    0.0    0.0    0.0    0.0\n[and 63 more slices...]"},{"id":166,"pagetitle":"Working with InferenceData","title":"Compute and store posterior pushforward quantities","ref":"/ArviZ/stable/working_with_inference_data/#Compute-and-store-posterior-pushforward-quantities","content":" Compute and store posterior pushforward quantities We use “posterior pushfoward quantities” to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables. You can compute these pushforward operations and store them as a new variable in a copy of the posterior group. Here we'll create a new  InferenceData  with  theta_school_diff  in the posterior: idata_new = Base.setindex(idata, merge(post, (; theta_school_diff)), :posterior) InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:school_bis} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 4 layers:\n  :mu                Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :theta             Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n  :tau               Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :theta_school_diff Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school}, Dim{:school_bis} (4×500×8×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" posterior_predictive Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.489022\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" sample_stats Dataset with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 12 layers:\n  :tune             Bool dims: Dim{:chain}, Dim{:draw} (4×500)\n  :depth            Int64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :tree_size        Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :lp               Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :energy_error     Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :step_size_bar    Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :max_energy_error Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :energy           Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :mean_tree_accept Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :step_size        Float64 dims: Dim{:chain}, Dim{:draw} (4×500)\n  :diverging        Bool dims: Dim{:chain}, Dim{:draw} (4×500)\n  :log_likelihood   Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (4×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.485802\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" prior Dataset with dimensions: \n  Dim{:chain} Sampled StepRangeLen(0.0, 0.0, 1) ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 5 layers:\n  :tau       Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :tau_log__ Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :mu        Float64 dims: Dim{:chain}, Dim{:draw} (1×500)\n  :theta     Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n  :obs       Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (1×500×8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.490387\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" observed_data Dataset with dimensions: \n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 1 layer:\n  :obs Float64 dims: Dim{:school} (8)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.491909\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" Once you have these pushforward quantities in an  InferenceData , you’ll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them, or save and share the  InferenceData  object with the pushforward quantities included. Here we compute the  mcse  of  theta_school_diff : mcse(idata_new.posterior).theta_school_diff 8×8 DimArray{Float64,2} theta_school_diff with dimensions: \n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  Dim{:school_bis} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n                       \"Choate\"  …   \"St. Paul's\"   \"Mt. Hermon\"\n  \"Choate\"            0.0           0.111869       0.175078\n  \"Deerfield\"         0.149711      0.15666        0.11984\n  \"Phillips Andover\"  0.276861      0.23738        0.163744\n  \"Phillips Exeter\"   0.163212      0.167966       0.135588\n  \"Hotchkiss\"         0.299984   …  0.260274       0.165326\n  \"Lawrenceville\"     0.223715      0.209422       0.146585\n  \"St. Paul's\"        0.111869      0.0            0.155986\n  \"Mt. Hermon\"        0.175078      0.155986       0.0"},{"id":167,"pagetitle":"Working with InferenceData","title":"Advanced subsetting","ref":"/ArviZ/stable/working_with_inference_data/#Advanced-subsetting","content":" Advanced subsetting To select the value corresponding to the difference between the Choate and Deerfield schools do: school_idx = [\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"]\nschool_bis_idx = [\"Deerfield\", \"Choate\", \"Lawrenceville\"]\ntheta_school_diff[school=At(school_idx), school_bis=At(school_bis_idx)] 4×500×3×3 DimArray{Float64,4} theta with dimensions: \n  Dim{:chain} Sampled 0:3 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Hotchkiss, Mt. Hermon] Unordered,\n  Dim{:school_bis} Categorical String[Deerfield, Choate, Lawrenceville] Unordered\n[:, :, 1, 1]\n     0          1         2         …  497         498        499\n 0  10.2061    -7.31077   5.11594       11.8025     -4.39806    7.22325\n 1   0.339696   0.339696  0.339696      10.027       7.18936  -13.3384\n 2  -0.458749   5.00749   0.540794      -0.935327    2.57992    3.82478\n 3  -0.872076  -0.872076  0.168416       2.42358     2.61346   -0.113698\n[and 8 more slices...]"},{"id":168,"pagetitle":"Working with InferenceData","title":"Add new chains using  concat","ref":"/ArviZ/stable/working_with_inference_data/#Add-new-chains-using-concat","content":" Add new chains using  concat Suppose after checking the  mcse  and realizing you need more samples, you rerun the model with two chains and obtain an  idata_rerun  object. idata_rerun = InferenceData(; posterior=set(post[chain=At([0, 1])]; chain=[4, 5])) InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled Int64[4, 5] ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (2×500)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (2×500×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (2×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\" You can combine the two using  concat . concat(idata[[:posterior]], idata_rerun; dim=:chain) InferenceData posterior Dataset with dimensions: \n  Dim{:chain} Sampled 0:5 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 0:499 ForwardOrdered Regular Points,\n  Dim{:school} Categorical String[Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\nand 3 layers:\n  :mu    Float64 dims: Dim{:chain}, Dim{:draw} (6×500)\n  :theta Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:school} (6×500×8)\n  :tau   Float64 dims: Dim{:chain}, Dim{:draw} (6×500)\n\nwith metadata OrderedCollections.OrderedDict{Symbol, Any} with 3 entries:\n  :created_at => \"2019-06-21T17:36:34.398087\"\n  :inference_library_version => \"3.7\"\n  :inference_library => \"pymc3\""},{"id":171,"pagetitle":"Home","title":"InferenceObjects","ref":"/InferenceObjects/stable/#InferenceObjects","content":" InferenceObjects InferenceObjects.jl is a Julia implementation of the  InferenceData schema  for storing results of Bayesian inference. Its purpose is to serve the following three goals: Usefulness in the analysis of Bayesian inference results. Reproducibility of Bayesian inference analysis. Interoperability between different inference backends and programming languages. The implementation consists primarily of the  InferenceData  and  Dataset  structures. InferenceObjects also provides the function  convert_to_inference_data , which may be overloaded by inference packages to define how various inference outputs can be converted to an  InferenceData . For examples of how  InferenceData  can be used, see the  ArviZ.jl documentation ."},{"id":174,"pagetitle":"Dataset","title":"Dataset","ref":"/InferenceObjects/stable/dataset/#Dataset","content":" Dataset InferenceObjects.Dataset InferenceObjects.convert_to_dataset InferenceObjects.namedtuple_to_dataset"},{"id":175,"pagetitle":"Dataset","title":"Type definition","ref":"/InferenceObjects/stable/dataset/#Type-definition","content":" Type definition"},{"id":176,"pagetitle":"Dataset","title":"InferenceObjects.Dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.Dataset","content":" InferenceObjects.Dataset  —  Type Dataset{L} <: DimensionalData.AbstractDimStack{L} Container of dimensional arrays sharing some dimensions. This type is an  DimensionalData.AbstractDimStack  that implements the same interface as  DimensionalData.DimStack  and has identical usage. When a  Dataset  is passed to Python, it is converted to an  xarray.Dataset  without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an  xarray.Dataset  is passed to Julia, its data must be copied. Constructors Dataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n) In most cases, use  convert_to_dataset  to create a  Dataset  instead of directly using a constructor. source"},{"id":177,"pagetitle":"Dataset","title":"General conversion","ref":"/InferenceObjects/stable/dataset/#General-conversion","content":" General conversion"},{"id":178,"pagetitle":"Dataset","title":"InferenceObjects.convert_to_dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.convert_to_dataset","content":" InferenceObjects.convert_to_dataset  —  Function convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset Convert a supported object to a  Dataset . In most cases, this function calls  convert_to_inference_data  and returns the corresponding  group . source"},{"id":179,"pagetitle":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.namedtuple_to_dataset","content":" InferenceObjects.namedtuple_to_dataset  —  Function namedtuple_to_dataset(data; kwargs...) -> Dataset Convert  NamedTuple  mapping variable names to arrays to a  Dataset . Keywords attrs::AbstractDict{<:AbstractString} : a collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable. library::Union{String,Module} : library used for performing inference. Will be attached to the  attrs  metadata. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. source"},{"id":180,"pagetitle":"Dataset","title":"DimensionalData","ref":"/InferenceObjects/stable/dataset/#DimensionalData","content":" DimensionalData As a  DimensionalData.AbstractDimStack ,  Dataset  also implements the  AbstractDimStack  API and can be used like a  DimStack . See  DimensionalData's documentation  for example usage."},{"id":181,"pagetitle":"Dataset","title":"Tables inteface","ref":"/InferenceObjects/stable/dataset/#Tables-inteface","content":" Tables inteface Dataset  implements the  Tables  interface. This allows  Dataset s to be used as sources for any function that can accept a table. For example, it's straightforward to: write to CSV with CSV.jl flatten to a DataFrame with DataFrames.jl plot with StatsPlots.jl plot with AlgebraOfGraphics.jl"},{"id":184,"pagetitle":"InferenceData","title":"InferenceData","ref":"/InferenceObjects/stable/inference_data/#InferenceData","content":" InferenceData InferenceObjects.InferenceData Base.getindex Base.getproperty Base.merge Base.propertynames Base.setindex InferenceObjects.convert_to_inference_data InferenceObjects.from_namedtuple"},{"id":185,"pagetitle":"InferenceData","title":"Type definition","ref":"/InferenceObjects/stable/inference_data/#Type-definition","content":" Type definition"},{"id":186,"pagetitle":"InferenceData","title":"InferenceObjects.InferenceData","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.InferenceData","content":" InferenceObjects.InferenceData  —  Type InferenceData{group_names,group_types} Container for inference data storage using DimensionalData. This object implements the  InferenceData schema . Internally, groups are stored in a  NamedTuple , which can be accessed using  parent(::InferenceData) . Constructors InferenceData(groups::NamedTuple)\nInferenceData(; groups...) Construct an inference data from either a  NamedTuple  or keyword arguments of groups. Groups must be  Dataset  objects. Instead of directly creating an  InferenceData , use the exported  from_xyz  functions or  convert_to_inference_data . source"},{"id":187,"pagetitle":"InferenceData","title":"Property interface","ref":"/InferenceObjects/stable/inference_data/#Property-interface","content":" Property interface"},{"id":188,"pagetitle":"InferenceData","title":"Base.getproperty","ref":"/InferenceObjects/stable/inference_data/#Base.getproperty","content":" Base.getproperty  —  Function getproperty(data::InferenceData, name::Symbol) -> Dataset Get group with the specified  name . source"},{"id":189,"pagetitle":"InferenceData","title":"Base.propertynames","ref":"/InferenceObjects/stable/inference_data/#Base.propertynames","content":" Base.propertynames  —  Function propertynames(data::InferenceData) -> Tuple{Symbol} Get names of groups source"},{"id":190,"pagetitle":"InferenceData","title":"Indexing interface","ref":"/InferenceObjects/stable/inference_data/#Indexing-interface","content":" Indexing interface"},{"id":191,"pagetitle":"InferenceData","title":"Base.getindex","ref":"/InferenceObjects/stable/inference_data/#Base.getindex","content":" Base.getindex  —  Function Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData Return a new  InferenceData  containing the specified groups sliced to the specified coords. coords  specifies a dimension name mapping to an index, a  DimensionalData.Selector , or an  IntervalSets.AbstractInterval . If one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised. Examples Select data from all groups for just the specified id values. julia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\" Select data from just the posterior, returning a  Dataset  if the indices index more than one element from any of the variables: julia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:19:25.982\" Note that if a single index is provided, the behavior is still to slice so that the dimension is preserved. source"},{"id":192,"pagetitle":"InferenceData","title":"Base.setindex","ref":"/InferenceObjects/stable/inference_data/#Base.setindex","content":" Base.setindex  —  Function Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData Create a new  InferenceData  containing the  group  with the specified  name . If a group with  name  is already in  data , it is replaced. source"},{"id":193,"pagetitle":"InferenceData","title":"Iteration interface","ref":"/InferenceObjects/stable/inference_data/#Iteration-interface","content":" Iteration interface InferenceData  also implements the same iteration interface as its underlying  NamedTuple . That is, iterating over an  InferenceData  iterates over its groups."},{"id":194,"pagetitle":"InferenceData","title":"General conversion","ref":"/InferenceObjects/stable/inference_data/#General-conversion","content":" General conversion"},{"id":195,"pagetitle":"InferenceData","title":"InferenceObjects.convert_to_inference_data","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.convert_to_inference_data","content":" InferenceObjects.convert_to_inference_data  —  Function convert_to_inference_data(obj; group, kwargs...) -> InferenceData Convert a supported object to an  InferenceData  object. If  obj  converts to a single dataset,  group  specifies which dataset in the resulting  InferenceData  that is. See  convert_to_dataset Arguments obj  can be many objects. Basic supported types are: InferenceData : return unchanged Dataset / DimensionalData.AbstractDimStack : add to  InferenceData  as the only group NamedTuple / AbstractDict : create a  Dataset  as the only group AbstractArray{<:Real} : create a  Dataset  as the only group, given an arbitrary name, if the name is not set More specific types may be documented separately. Keywords group::Symbol = :posterior : If  obj  converts to a single dataset, assign the resulting dataset to this group. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. kwargs : remaining keywords forwarded to converter functions source"},{"id":196,"pagetitle":"InferenceData","title":"InferenceObjects.from_namedtuple","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_namedtuple","content":" InferenceObjects.from_namedtuple  —  Function from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{<:NamedTuple}; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Matrix{<:NamedTuple}; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData Convert a  NamedTuple  or container of  NamedTuple s to an  InferenceData . If containers are passed, they are flattened into a single  NamedTuple  with array elements whose first dimensions correspond to the dimensions of the containers. Arguments posterior : The data to be converted. It may be of the following types: ::NamedTuple : The keys are the variable names and the values are arrays with dimensions  ([sizes...], ndraws, [nchains]) . ::Matrix{<:NamedTuple} : Each element is a single draw from a single chain, with array/scalar values with dimensions  sizes . The dimensions of the matrix container are  (ndraws, nchains) ::Vector{Vector{<:NamedTuple}} : The same as the above case. Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior::Any=nothing : Draws from the prior prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata1 = (\n    x=rand(ndraws, nchains), y=randn(2, ndraws, nchains), z=randn(3, 2, ndraws, nchains)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [(x=rand(ndraws), y=randn(2, ndraws), z=randn(3, 2, ndraws)) for _ in 1:nchains];\nidata2 = from_namedtuple(data2)\n\ndata3 = [(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws, _ in 1:nchains];\nidata3 = from_namedtuple(data3)\n\ndata4 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata4 = from_namedtuple(data4) source"},{"id":197,"pagetitle":"InferenceData","title":"General functions","ref":"/InferenceObjects/stable/inference_data/#General-functions","content":" General functions"},{"id":198,"pagetitle":"InferenceData","title":"Base.merge","ref":"/InferenceObjects/stable/inference_data/#Base.merge","content":" Base.merge  —  Function merge(data::InferenceData, others::InferenceData...) -> InferenceData Merge  InferenceData  objects. The result contains all groups in  data  and  others . If a group appears more than once, the one that occurs first is kept. source"},{"id":202,"pagetitle":"InferenceObjectsNetCDF.jl","title":"InferenceObjectsNetCDF.jl","ref":"/InferenceObjects/stable/subpackages/inferenceobjectsnetcdf/#InferenceObjectsNetCDF.jl","content":" InferenceObjectsNetCDF.jl InferenceObjectsNetCDF extends InferenceObjects with the ability to read  InferenceData  from a  NetCDF file  or write it to a file."},{"id":203,"pagetitle":"InferenceObjectsNetCDF.jl","title":"InferenceObjectsNetCDF.from_netcdf","ref":"/InferenceObjects/stable/subpackages/inferenceobjectsnetcdf/#InferenceObjectsNetCDF.from_netcdf","content":" InferenceObjectsNetCDF.from_netcdf  —  Function from_netcdf(path::AbstractString; kwargs...) -> InferenceData Load an  InferenceData  from an unopened NetCDF file. Remaining  kwargs  are passed to  NCDatasets.NCDataset . This method loads data eagerly. To instead load data lazily, pass an opened  NCDataset  to  from_netcdf . Examples julia> idata = from_netcdf(\"centered_eight.nc\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data from_netcdf(ds::NCDatasets.NCDataset; load_mode) -> InferenceData Load an  InferenceData  from an opened NetCDF file. load_mode  defaults to  :lazy , which avoids reading variables into memory. Operations on these arrays will be slow.  load_mode  can also be  :eager , which copies all variables into memory. It is then safe to close  ds . If  load_mode  is  :lazy  and  ds  is closed after constructing  InferenceData , using the variable arrays will have undefined behavior. Examples Here is how we might load an  InferenceData  from an  InferenceData  lazily from a web-hosted NetCDF file. julia> using HTTP, NCDatasets\n\njulia> resp = HTTP.get(\"https://github.com/arviz-devs/arviz_example_data/blob/main/data/centered_eight.nc?raw=true\");\n\njulia> ds = NCDataset(\"centered_eight\", \"r\"; memory = resp.body);\n\njulia> idata = from_netcdf(ds)\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\njulia> idata_copy = copy(idata); # disconnect from the loaded dataset\n\njulia> close(ds); source"},{"id":204,"pagetitle":"InferenceObjectsNetCDF.jl","title":"InferenceObjectsNetCDF.to_netcdf","ref":"/InferenceObjects/stable/subpackages/inferenceobjectsnetcdf/#InferenceObjectsNetCDF.to_netcdf","content":" InferenceObjectsNetCDF.to_netcdf  —  Function to_netcdf(data, dest::AbstractString; group::Symbol=:posterior, kwargs...)\nto_netcdf(data, dest::NCDatasets.NCDataset; group::Symbol=:posterior) Write  data  to a NetCDF file. data  is any type that can be converted to an  InferenceData  using  convert_to_inference_data . If not an  InferenceData , then  group  specifies which group the data represents. dest  specifies either the path to the NetCDF file or an opened NetCDF file. If  dest  is a path, remaining  kwargs  are passed to  NCDatasets.NCDataset . Examples julia> using NCDatasets\n\njulia> idata = from_namedtuple((; x = randn(4, 100, 3), z = randn(4, 100)))\nInferenceData with groups:\n  > posterior\n\njulia> to_netcdf(idata, \"data.nc\")\n\"data.nc\" source"},{"id":207,"pagetitle":"Home","title":"ArviZExampleData","ref":"/ArviZExampleData/stable/#ArviZExampleData","content":" ArviZExampleData Utilities for loading datasets defined in the  arviz_example_data  repository. The resulting objects are  InferenceObjects.jl 's  InferenceData .  These utilities are used in  ArviZ.jl ."},{"id":210,"pagetitle":"API","title":"API","ref":"/ArviZExampleData/stable/api/#API","content":" API"},{"id":211,"pagetitle":"API","title":"ArviZExampleData.describe_example_data","ref":"/ArviZExampleData/stable/api/#ArviZExampleData.describe_example_data","content":" ArviZExampleData.describe_example_data  —  Function describe_example_data() -> String Return a string containing descriptions of all available datasets. Examples julia> describe_example_data(\"radon\") |> println\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472 source"},{"id":212,"pagetitle":"API","title":"ArviZExampleData.load_example_data","ref":"/ArviZExampleData/stable/api/#ArviZExampleData.load_example_data","content":" ArviZExampleData.load_example_data  —  Function load_example_data(name; kwargs...) -> InferenceObjects.InferenceData\nload_example_data() -> Dict{String,AbstractFileMetadata} Load a local or remote pre-made dataset. kwargs  are forwarded to  InferenceObjects.from_netcdf . Pass no parameters to get a  Dict  listing all available datasets. Data files are handled by DataDeps.jl. A file is downloaded only when it is requested and then cached for future use. Examples julia> keys(load_example_data())\nKeySet for a OrderedCollections.OrderedDict{String, ArviZExampleData.AbstractFileMetadata} with 9 entries. Keys:\n  \"centered_eight\"\n  \"non_centered_eight\"\n  \"radon\"\n  \"rugby\"\n  \"regression1d\"\n  \"regression10d\"\n  \"classification1d\"\n  \"classification10d\"\n  \"glycan_torsion_angles\"\n\njulia> load_example_data(\"centered_eight\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > log_likelihood\n  > sample_stats\n  > prior\n  > prior_predictive\n  > observed_data\n  > constant_data source"},{"id":215,"pagetitle":"Datasets","title":"Datasets","ref":"/ArviZExampleData/stable/datasets/#Datasets","content":" Datasets The following shows the currently available example datasets: using ArviZExampleData\n\nprintln(describe_example_data()) centered_eight\n==============\n\nA centered parameterization of the eight schools model. Provided as an example of a model that NUTS has trouble fitting. Compare to `non_centered_eight`.\n\nThe eight schools model is a hierarchical model used for an analysis of the effectiveness of classes that were designed to improve students' performance on the Scholastic Aptitude Test.\n\nSee Bayesian Data Analysis (Gelman et. al.) for more details.\n\nlocal: /home/runner/.julia/artifacts/1a48fb48ab2b35cdeeb84e0dcdcda134e24a1c20/arviz_example_data-0.1.1/data/centered_eight.nc\n\nnon_centered_eight\n==================\n\nA non-centered parameterization of the eight schools model. This is a hierarchical model where sampling problems may be fixed by a non-centered parametrization. Compare to `centered_eight`.\n\nThe eight schools model is a hierarchical model used for an analysis of the effectiveness of classes that were designed to improve students' performance on the Scholastic Aptitude Test.\n\nSee Bayesian Data Analysis (Gelman et. al.) for more details.\n\nlocal: /home/runner/.julia/artifacts/1a48fb48ab2b35cdeeb84e0dcdcda134e24a1c20/arviz_example_data-0.1.1/data/non_centered_eight.nc\n\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472\n\nrugby\n=====\n\nThe Six Nations Championship is a yearly rugby competition between Italy, Ireland, Scotland, England, France and Wales. Fifteen games are played each year, representing all combinations of the six teams.\n\nThis example uses and includes results from 2014 - 2017, comprising 60 total games. It models latent parameters for each team's attack and defense, as well as a parameter for home team advantage.\n\nSee https://docs.pymc.io/notebooks/rugby_analytics.html by Peader Coyle for more details and references.\n\nremote: http://ndownloader.figshare.com/files/16254359\n\nregression1d\n============\n\nA synthetic one dimensional linear regression dataset with latent slope, intercept, and noise (\"eps\"). One hundred data points, fit with PyMC3.\n\nTrue slope and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16254899\n\nregression10d\n=============\n\nA synthetic multi-dimensional (10 dimensions) linear regression dataset with latent weights (\"w\"), intercept, and noise (\"eps\"). Five hundred data points, fit with PyMC3.\n\nTrue weights and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16255736\n\nclassification1d\n================\n\nA synthetic one dimensional logistic regression dataset with latent slope and intercept, passed into a Bernoulli random variable. One hundred data points, fit with PyMC3.\n\nTrue slope and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16256678\n\nclassification10d\n=================\n\nA synthetic multi dimensional (10 dimensions) logistic regression dataset with latent weights (\"w\") and intercept, passed into a Bernoulli random variable. Five hundred data points, fit with PyMC3.\n\nTrue weights and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16256681\n\nglycan_torsion_angles\n=====================\n\nTorsion angles phi and psi are critical for determining the three dimensional structure of bio-molecules. Combinations of phi and psi torsion angles that produce clashes between atoms in the bio-molecule result in high energy, unlikely structures.\n\nThis model uses a Von Mises distribution to propose torsion angles for the structure of a glycan molecule (pdb id: 2LIQ), and a Potential to estimate the proposed structure's energy. Said Potential is bound by Boltzman's law.\n\nremote: http://ndownloader.figshare.com/files/22882652"},{"id":218,"pagetitle":"For developers","title":"For developers","ref":"/ArviZExampleData/stable/for_developers/#For-developers","content":" For developers This package has  arviz_example_data  as a data dependency, which is included as an  artifact . When  arviz_example_data  is updated, and a new release is made,  Artifacts.toml  should be updated to point to the new tarball corresponding to the release: julia> using ArtifactUtils\n\njulia> version = v\"0.1.0\";\n\njulia> tarball_url = \"https://github.com/arviz-devs/arviz_example_data/archive/refs/tags/v$version.tar.gz\";\n\njulia> add_artifact!(\"Artifacts.toml\", \"arviz_example_data\", tarball_url; force=true);"},{"id":222,"pagetitle":"Home","title":"PSIS","ref":"/PSIS/stable/#PSIS","content":" PSIS PSIS.jl implements the Pareto smoothed importance sampling (PSIS) algorithm from  [VehtariSimpson2021] . Given a set of importance weights used in some estimator, PSIS both improves the reliability of the estimates by smoothing the importance weights and acts as a diagnostic of the reliability of the estimates. See  psis  for details."},{"id":223,"pagetitle":"Home","title":"Example","ref":"/PSIS/stable/#Example","content":" Example In this example, we use PSIS to smooth log importance ratios for importance sampling 30 isotropic Student  $t$ -distributed parameters using standard normal distributions as proposals. using PSIS, Distributions\nproposal = Normal()\ntarget = TDist(7)\nx = rand(proposal, 30, 1_000)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) ┌ Warning: 11 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/work/PSIS.jl/PSIS.jl/src/core.jl:299\n┌ Warning: 1 parameters had Pareto shape values k > 1. Corresponding importance sampling estimates are likely to be unstable and are unlikely to converge with additional samples.\n└ @ PSIS ~/work/PSIS.jl/PSIS.jl/src/core.jl:302 PSISResult with 30 parameters, 1000 draws, and 1 chains\nPareto shape (k) diagnostic values:\n                        Count       Min. ESS\n (-Inf, 0.5]  good       7 (23.3%)  947\n  (0.5, 0.7]  okay      11 (36.7%)  881\n    (0.7, 1]  bad       11 (36.7%)  ——\n    (1, Inf)  very bad   1 (3.3%)   —— As indicated by the warnings, this is a poor choice of a proposal distribution, and estimates are unlikely to converge (see  PSISResult  for an explanation of the shape thresholds). When running PSIS with many parameters, it is useful to plot the Pareto shape values to diagnose convergence. See  Plotting PSIS results  for examples. VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO]"},{"id":226,"pagetitle":"API","title":"API","ref":"/PSIS/stable/api/#API","content":" API"},{"id":227,"pagetitle":"API","title":"PSIS.PSISResult","ref":"/PSIS/stable/api/#PSIS.PSISResult","content":" PSIS.PSISResult  —  Type PSISResult Result of Pareto-smoothed importance sampling (PSIS) using  psis . Properties log_weights : un-normalized Pareto-smoothed log weights weights : normalized Pareto-smoothed weights (allocates a copy) pareto_shape : Pareto  $k=ξ$  shape parameter nparams : number of parameters in  log_weights ndraws : number of draws in  log_weights nchains : number of chains in  log_weights reff : the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size. ess : estimated effective sample size of estimate of mean using smoothed importance samples (see  ess_is ) log_weights_norm : the logarithm of the normalization constant of  log_weights tail_length : length of the upper tail of  log_weights  that was smoothed tail_dist : the generalized Pareto distribution that was fit to the tail of  log_weights . Note that the tail weights are scaled to have a maximum of 1, so  tail_dist * exp(maximum(log_ratios))  is the corresponding fit directly to the tail of  log_ratios . Diagnostic The  pareto_shape  parameter  $k=ξ$  of the generalized Pareto distribution  tail_dist  can be used to diagnose reliability and convergence of estimates using the importance weights  [VehtariSimpson2021] . if  $k < \\frac{1}{3}$ , importance sampling is stable, and importance sampling (IS) and PSIS both are reliable. if  $k ≤ \\frac{1}{2}$ , then the importance ratio distributon has finite variance, and the central limit theorem holds. As  $k$  approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE. if  $\\frac{1}{2} < k ≤ 0.7$ , then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime. if  $0.7 < k ≤ 1$ , then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended. if  $k > 1$ , then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes. See  paretoshapeplot  for a diagnostic plot. source"},{"id":228,"pagetitle":"API","title":"PSIS.psis","ref":"/PSIS/stable/api/#PSIS.psis","content":" PSIS.psis  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with one of the following sizes: (ndraws,) : a vector of draws for a single parameter from a single chain (nparams, ndraws) : a matrix of draws for a multiple parameter from a single chain (nparams, ndraws, nchains) : an array of draws for multiple parameters from multiple chains, e.g. as might be generated with Markov chain Monte Carlo. reff::Union{Real,AbstractVector} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(ndraws * nchains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. Keywords improved=false : If  true , use the adaptive empirical prior of  [Zhang2010] . If  false , use the simpler prior of  [ZhangStephens2009] , which is also used in  [VehtariSimpson2021] . warn=true : If  true , warning messages are delivered Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  paretoshapeplot  for a diagnostic plot. source"},{"id":229,"pagetitle":"API","title":"PSIS.paretoshapeplot","ref":"/PSIS/stable/api/#PSIS.paretoshapeplot","content":" PSIS.paretoshapeplot  —  Function paretoshapeplot(values; kwargs...)\nparetoshapeplot!(values; kwargs...) Plot shape parameters of fitted Pareto tail distributions for diagnosing convergence. Arguments values : may be either a vector of Pareto shape parameters or a  PSISResult . Keywords showlines=false : if  true , plot horizontal lines indicating relevant Pareto shape thresholds are drawn. See  PSISResult  for explanation of thresholds. backend::Symbol : backend to use for plotting, defaulting to  :Plots , unless  :Makie  is available. All remaining keywords are passed to the plotting backend. See  psis ,  PSISResult . Note Plots.jl or a Makie.jl backend must be loaded to use these functions. Examples using PSIS, Distributions, Plots\nproposal = Normal()\ntarget = TDist(7)\nx = rand(proposal, 100, 1_000)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) Plot with Plots.jl. using Plots\nplot(result; showlines=true) Plot with GLMakie.jl. using GLMakie\nplot(result; showlines=true) source"},{"id":230,"pagetitle":"API","title":"PSIS.ess_is","ref":"/PSIS/stable/api/#PSIS.ess_is","content":" PSIS.ess_is  —  Function ess_is(weights; reff=1) Estimate effective sample size (ESS) for importance sampling over the sample dimensions. Given normalized weights  $w_{1:n}$ , the ESS is estimated using the L2-norm of the weights: \\[\\mathrm{ESS}(w_{1:n}) = \\frac{r_{\\mathrm{eff}}}{\\sum_{i=1}^n w_i^2}\\] where  $r_{\\mathrm{eff}}$  is the relative efficiency of the  log_weights . ess_is(result::PSISResult; bad_shape_missing=true) Estimate ESS for Pareto-smoothed importance sampling. Note ESS estimates for Pareto shape values  $k > 0.7$ , which are unreliable and misleadingly high, are set to  missing . To avoid this, set  bad_shape_missing=false . source VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] ZhangStephens2009 Jin Zhang & Michael A. Stephens (2009) A New and Efficient Estimation Method for the Generalized Pareto Distribution, Technometrics, 51:3, 316-325, DOI:  10.1198/tech.2009.08017 Zhang2010 Jin Zhang (2010) Improving on Estimation for the Generalized Pareto Distribution, Technometrics, 52:3, 335-339, DOI:  10.1198/TECH.2010.09206"},{"id":233,"pagetitle":"Internal","title":"Internal","ref":"/PSIS/stable/internal/#Internal","content":" Internal"},{"id":234,"pagetitle":"Internal","title":"PSIS.GeneralizedPareto","ref":"/PSIS/stable/internal/#PSIS.GeneralizedPareto","content":" PSIS.GeneralizedPareto  —  Type GeneralizedPareto{T<:Real} The generalized Pareto distribution. This is equivalent to  Distributions.GeneralizedPareto  and can be converted to one with  convert(Distributions.GeneralizedPareto, d) . Constructor GeneralizedPareto(μ, σ, k) Construct the generalized Pareto distribution (GPD) with location parameter  $μ$ , scale parameter  $σ$  and shape parameter  $k$ . Note The shape parameter  $k$  is equivalent to the commonly used shape parameter  $ξ$ . This is the same parameterization used by  [VehtariSimpson2021]  and is related to that used by  [ZhangStephens2009]  as  $k \\mapsto -k$ . source"},{"id":235,"pagetitle":"Internal","title":"PSIS.fit_gpd","ref":"/PSIS/stable/internal/#PSIS.fit_gpd-Tuple{AbstractArray}","content":" PSIS.fit_gpd  —  Method fit_gpd(x; μ=0, kwargs...) Fit a  GeneralizedPareto  with location  μ  to the data  x . The fit is performed using the Empirical Bayes method of  [ZhangStephens2009] [Zhang2010] . Keywords sorted::Bool=issorted(x) : If  true ,  x  is assumed to be sorted. If  false , a sorted copy of  x  is made. improved::Bool=true : If  true , use the adaptive empirical prior of  [Zhang2010] . If  false , use the simpler prior of  [ZhangStephens2009] . min_points::Int=30 : The minimum number of quadrature points to use when estimating the posterior mean of  $\\theta = \\frac{\\xi}{\\sigma}$ . source"},{"id":236,"pagetitle":"Internal","title":"PSIS.plotting_backend!","ref":"/PSIS/stable/internal/#PSIS.plotting_backend!-Tuple{Symbol}","content":" PSIS.plotting_backend!  —  Method plotting_backend!(backend::Symbol) Set default plotting backend. Valid values are  :Plots  and  :Makie . source ZhangStephens2009 Jin Zhang & Michael A. Stephens (2009) A New and Efficient Estimation Method for the Generalized Pareto Distribution, Technometrics, 51:3, 316-325, DOI:  10.1198/tech.2009.08017 Zhang2010 Jin Zhang (2010) Improving on Estimation for the Generalized Pareto Distribution, Technometrics, 52:3, 335-339, DOI:  10.1198/TECH.2010.09206"},{"id":239,"pagetitle":"Plotting","title":"Plotting PSIS results","ref":"/PSIS/stable/plotting/#Plotting-PSIS-results","content":" Plotting PSIS results PSIS.jl includes plotting recipes for  PSISResult  using any Plots.jl or Makie.jl backend, as well as the utility plotting function  paretoshapeplot . We demonstrate this with a simple example. using PSIS, Distributions\nproposal = Normal()\ntarget = TDist(7)\nx = rand(proposal, 20, 1_000)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) PSISResult with 20 parameters, 1000 draws, and 1 chains\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  3 (15.0%)  957\n  (0.5, 0.7]  okay  8 (40.0%)  833\n    (0.7, 1]  bad   9 (45.0%)  ——"},{"id":240,"pagetitle":"Plotting","title":"Plots.jl","ref":"/PSIS/stable/plotting/#Plots.jl","content":" Plots.jl PSISResult  objects can be plotted directly: using Plots\nPlots.plot(result; showlines=true, marker=:+, legend=false, linewidth=2) This is equivalent to calling  paretoshapeplot(result; kwargs...) ."},{"id":241,"pagetitle":"Plotting","title":"Makie.jl","ref":"/PSIS/stable/plotting/#Makie.jl","content":" Makie.jl The same syntax is supported with Makie.jl backends. using CairoMakie\nMakie.plot(result; showlines=true, marker=:+)"},{"id":242,"pagetitle":"Plotting","title":"Selecting the backend","ref":"/PSIS/stable/plotting/#Selecting-the-backend","content":" Selecting the backend If a Makie backend is loaded, then by default  paretoshapeplot  and  paretoshapeplot!  will use that backend; otherwise, a Plots.jl backend is used if available. However, both functions accept a  backend  keyword that can be used to specify the backend if for some reason both are loaded. paretoshapeplot(result; backend=:Plots) paretoshapeplot(result; backend=:Makie)"}]