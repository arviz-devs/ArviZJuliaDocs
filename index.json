[{"id":3,"pagetitle":"Home","title":"ArviZ.jl: Exploratory analysis of Bayesian models in Julia","ref":"/ArviZ/stable/#arvizjl","content":" ArviZ.jl: Exploratory analysis of Bayesian models in Julia ArviZ.jl is a Julia meta-package for exploratory analysis of Bayesian models. It is part of the  ArviZ project , which also includes a related  Python package . ArviZ consists of and re-exports the following subpackages, along with extensions integrating them with InferenceObjects: InferenceObjects.jl : a base package implementing the  InferenceData  type with utilities for building, saving, and working with it MCMCDiagnosticTools.jl : diagnostics for Markov Chain Monte Carlo methods PSIS.jl : Pareto-smoothed importance sampling PosteriorStats.jl : common statistical analyses for the Bayesian workflow Additional functionality can be loaded with the following packages: ArviZExampleData.jl : example  InferenceData  objects, useful for demonstration and testing ArviZPythonPlots.jl : Python ArviZ's library of plotting functions for Julia types See the navigation bar for more useful packages."},{"id":4,"pagetitle":"Home","title":"Installation","ref":"/ArviZ/stable/#installation","content":" Installation From the Julia REPL, type  ]  to enter the Pkg REPL mode and run pkg> add ArviZ"},{"id":5,"pagetitle":"Home","title":"Usage","ref":"/ArviZ/stable/#usage","content":" Usage See the  Quickstart  for example usage and the  API Overview  for description of functions."},{"id":6,"pagetitle":"Home","title":"Extending ArviZ.jl","ref":"/ArviZ/stable/#extendingarviz","content":" Extending ArviZ.jl To use a custom data type with ArviZ.jl, simply overload  InferenceObjects.convert_to_inference_data  to convert your input(s) to an  InferenceObjects.InferenceData ."},{"id":9,"pagetitle":"API Overview","title":"API Overview","ref":"/ArviZ/stable/api/#api","content":" API Overview Data Dataset Diagnostics InferenceData Stats"},{"id":12,"pagetitle":"Data","title":"Data","ref":"/ArviZ/stable/api/data/#data-api","content":" Data ArviZ.from_mcmcchains ArviZ.from_samplechains InferenceObjects.from_netcdf InferenceObjects.to_netcdf"},{"id":13,"pagetitle":"Data","title":"Inference library converters","ref":"/ArviZ/stable/api/data/#Inference-library-converters","content":" Inference library converters"},{"id":14,"pagetitle":"Data","title":"ArviZ.from_mcmcchains","ref":"/ArviZ/stable/api/data/#ArviZ.from_mcmcchains","content":" ArviZ.from_mcmcchains  —  Function from_mcmcchains(posterior::MCMCChains.Chains; kwargs...) -> InferenceData\nfrom_mcmcchains(; kwargs...) -> InferenceData\nfrom_mcmcchains(\n    posterior::MCMCChains.Chains,\n    posterior_predictive,\n    predictions,\n    log_likelihood;\n    kwargs...\n) -> InferenceData Convert data in an  MCMCChains.Chains  format into an  InferenceData . Any keyword argument below without an an explicitly annotated type above is allowed, so long as it can be passed to  convert_to_inference_data . Arguments posterior::MCMCChains.Chains : Draws from the posterior Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution or   name(s) of predictive variables in  posterior predictions : Out-of-sample predictions for the posterior. prior : Draws from the prior prior_predictive : Draws from the prior predictive distribution or name(s) of predictive   variables in  prior observed_data : Observed data on which the  posterior  is conditional. It should only   contain data which is modeled as a random variable. Keys are parameter names and values. constant_data : Model constants, data included in the model that are not modeled as   random variables. Keys are parameter names. predictions_constant_data : Constants relevant to the model predictions (i.e. new  x    values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this   argument as a named tuple whose keys are observed variable names and whose values are log   likelihood arrays. Alternatively, provide the name of variable in  posterior  containing   log likelihoods. library=MCMCChains : Name of library that generated the chains coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions eltypes : Map from variable names to eltypes. This is primarily used to assign discrete   eltypes to discrete variables that were stored in  Chains  as floats. Returns InferenceData : The data with groups corresponding to the provided data source"},{"id":15,"pagetitle":"Data","title":"ArviZ.from_samplechains","ref":"/ArviZ/stable/api/data/#ArviZ.from_samplechains","content":" ArviZ.from_samplechains  —  Function from_samplechains(\n    posterior=nothing;\n    prior=nothing,\n    library=SampleChains,\n    kwargs...,\n) -> InferenceData Convert SampleChains samples to an  InferenceData . Either  posterior  or  prior  may be a  SampleChains.AbstractChain  or  SampleChains.MultiChain  object. For descriptions of remaining  kwargs , see  from_namedtuple . source"},{"id":16,"pagetitle":"Data","title":"IO / Conversion","ref":"/ArviZ/stable/api/data/#IO-/-Conversion","content":" IO / Conversion"},{"id":17,"pagetitle":"Data","title":"InferenceObjects.from_netcdf","ref":"/ArviZ/stable/api/data/#InferenceObjects.from_netcdf","content":" InferenceObjects.from_netcdf  —  Function from_netcdf(path::AbstractString; kwargs...) -> InferenceData Load an  InferenceData  from an unopened NetCDF file. Remaining  kwargs  are passed to  NCDatasets.NCDataset . This method loads data eagerly. To instead load data lazily, pass an opened  NCDataset  to  from_netcdf . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_netcdf(\"centered_eight.nc\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data from_netcdf(ds::NCDatasets.NCDataset; load_mode) -> InferenceData Load an  InferenceData  from an opened NetCDF file. load_mode  defaults to  :lazy , which avoids reading variables into memory. Operations on these arrays will be slow.  load_mode  can also be  :eager , which copies all variables into memory. It is then safe to close  ds . If  load_mode  is  :lazy  and  ds  is closed after constructing  InferenceData , using the variable arrays will have undefined behavior. Examples Here is how we might load an  InferenceData  from an  InferenceData  lazily from a web-hosted NetCDF file. julia> using HTTP, InferenceObjects, NCDatasets\n\njulia> resp = HTTP.get(\"https://github.com/arviz-devs/arviz_example_data/blob/main/data/centered_eight.nc?raw=true\");\n\njulia> ds = NCDataset(\"centered_eight\", \"r\"; memory = resp.body);\n\njulia> idata = from_netcdf(ds)\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\njulia> idata_copy = copy(idata); # disconnect from the loaded dataset\n\njulia> close(ds); source"},{"id":18,"pagetitle":"Data","title":"InferenceObjects.to_netcdf","ref":"/ArviZ/stable/api/data/#InferenceObjects.to_netcdf","content":" InferenceObjects.to_netcdf  —  Function to_netcdf(data, dest::AbstractString; group::Symbol=:posterior, kwargs...)\nto_netcdf(data, dest::NCDatasets.NCDataset; group::Symbol=:posterior) Write  data  to a NetCDF file. data  is any type that can be converted to an  InferenceData  using  convert_to_inference_data . If not an  InferenceData , then  group  specifies which group the data represents. dest  specifies either the path to the NetCDF file or an opened NetCDF file. If  dest  is a path, remaining  kwargs  are passed to  NCDatasets.NCDataset . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_namedtuple((; x = randn(4, 100, 3), z = randn(4, 100)))\nInferenceData with groups:\n  > posterior\n\njulia> to_netcdf(idata, \"data.nc\")\n\"data.nc\" source"},{"id":21,"pagetitle":"Dataset","title":"Dataset","ref":"/ArviZ/stable/api/dataset/#dataset-api","content":" Dataset InferenceObjects.Dataset InferenceObjects.convert_to_dataset InferenceObjects.namedtuple_to_dataset"},{"id":22,"pagetitle":"Dataset","title":"Type definition","ref":"/ArviZ/stable/api/dataset/#Type-definition","content":" Type definition"},{"id":23,"pagetitle":"Dataset","title":"InferenceObjects.Dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.Dataset","content":" InferenceObjects.Dataset  —  Type Dataset{L} <: DimensionalData.AbstractDimStack{L} Container of dimensional arrays sharing some dimensions. This type is an  DimensionalData.AbstractDimStack  that implements the same interface as  DimensionalData.DimStack  and has identical usage. When a  Dataset  is passed to Python, it is converted to an  xarray.Dataset  without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an  xarray.Dataset  is passed to Julia, its data must be copied. Constructors Dataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n) In most cases, use  convert_to_dataset  to create a  Dataset  instead of directly using a constructor. source"},{"id":24,"pagetitle":"Dataset","title":"General conversion","ref":"/ArviZ/stable/api/dataset/#General-conversion","content":" General conversion"},{"id":25,"pagetitle":"Dataset","title":"InferenceObjects.convert_to_dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.convert_to_dataset","content":" InferenceObjects.convert_to_dataset  —  Function convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset Convert a supported object to a  Dataset . In most cases, this function calls  convert_to_inference_data  and returns the corresponding  group . source"},{"id":26,"pagetitle":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","ref":"/ArviZ/stable/api/dataset/#InferenceObjects.namedtuple_to_dataset","content":" InferenceObjects.namedtuple_to_dataset  —  Function namedtuple_to_dataset(data; kwargs...) -> Dataset Convert  NamedTuple  mapping variable names to arrays to a  Dataset . Any non-array values will be converted to a 0-dimensional array. Keywords attrs::AbstractDict{<:AbstractString} : a collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable. library::Union{String,Module} : library used for performing inference. Will be attached to the  attrs  metadata. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. source"},{"id":27,"pagetitle":"Dataset","title":"DimensionalData","ref":"/ArviZ/stable/api/dataset/#DimensionalData","content":" DimensionalData As a  DimensionalData.AbstractDimStack ,  Dataset  also implements the  AbstractDimStack  API and can be used like a  DimStack . See  DimensionalData's documentation  for example usage."},{"id":28,"pagetitle":"Dataset","title":"Tables inteface","ref":"/ArviZ/stable/api/dataset/#Tables-inteface","content":" Tables inteface Dataset  implements the  Tables  interface. This allows  Dataset s to be used as sources for any function that can accept a table. For example, it's straightforward to: write to CSV with CSV.jl flatten to a DataFrame with DataFrames.jl plot with StatsPlots.jl plot with AlgebraOfGraphics.jl"},{"id":31,"pagetitle":"Diagnostics","title":"Diagnostics","ref":"/ArviZ/stable/api/diagnostics/#diagnostics-api","content":" Diagnostics MCMCDiagnosticTools.AutocovMethod MCMCDiagnosticTools.BDAAutocovMethod MCMCDiagnosticTools.FFTAutocovMethod MCMCDiagnosticTools.bfmi MCMCDiagnosticTools.ess MCMCDiagnosticTools.ess_rhat MCMCDiagnosticTools.mcse MCMCDiagnosticTools.rhat MCMCDiagnosticTools.rstar"},{"id":32,"pagetitle":"Diagnostics","title":"Bayesian fraction of missing information","ref":"/ArviZ/stable/api/diagnostics/#bfmi","content":" Bayesian fraction of missing information"},{"id":33,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.bfmi","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.bfmi","content":" MCMCDiagnosticTools.bfmi  —  Function bfmi(energy::AbstractVector{<:Real}) -> Real\nbfmi(energy::AbstractMatrix{<:Real}; dims::Int=1) -> AbstractVector{<:Real} Calculate the estimated Bayesian fraction of missing information (BFMI). When sampling with Hamiltonian Monte Carlo (HMC), BFMI quantifies how well momentum resampling matches the marginal energy distribution. The current advice is that values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may change. A BFMI value below the threshold often indicates poor adaptation of sampling parameters or that the target distribution has heavy tails that were not well explored by the Markov chain. For more information, see Section 6.1 of  [Betancourt2018]  or  [Betancourt2016]  for a complete account. energy  is either a vector of Hamiltonian energies of draws or a matrix of energies of draws for multiple chains.  dims  indicates the dimension in  energy  that contains the draws. The default  dims=1  assumes  energy  has the shape  draws  or  (draws, chains) . If a different shape is provided,  dims  must be set accordingly. If  energy  is a vector, a single BFMI value is returned. Otherwise, a vector of BFMI values for each chain is returned. source"},{"id":34,"pagetitle":"Diagnostics","title":"Effective sample size and  $\\widehat{R}$  diagnostic","ref":"/ArviZ/stable/api/diagnostics/#ess_rhat","content":" Effective sample size and  $\\widehat{R}$  diagnostic"},{"id":35,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.ess","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.ess","content":" MCMCDiagnosticTools.ess  —  Function ess(data::InferenceData; kwargs...) -> Dataset\ness(data::Dataset; kwargs...) -> Dataset Calculate the effective sample size (ESS) for each parameter in the data. source ess(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind=:bulk,\n    relative::Bool=false,\n    autocov_method=AutocovMethod(),\n    split_chains::Int=2,\n    maxlag::Int=250,\n    kwargs...\n) Estimate the effective sample size (ESS) of the  samples  of shape  (draws, [chains[, parameters...]])  with the  autocov_method . Optionally, the  kind  of ESS estimate to be computed can be specified (see below). Some  kind s accept additional  kwargs . If  relative  is  true , the relative ESS is returned, i.e.  ess / (draws * chains) . split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. There must be at least 3 draws in each chain after splitting. maxlag  indicates the maximum lag for which autocovariance is computed and must be greater than 0. For a given estimand, it is recommended that the ESS is at least  100 * chains  and that  $\\widehat{R} < 1.01$ . [VehtariGelman2021] See also:  AutocovMethod ,  FFTAutocovMethod ,  BDAAutocovMethod ,  rhat ,  ess_rhat ,  mcse Kinds of ESS estimates If  kind  isa a  Symbol , it may take one of the following values: :bulk : basic ESS computed on rank-normalized draws. This kind diagnoses poor convergence   in the bulk of the distribution due to trends or different locations of the chains. :tail : minimum of the quantile-ESS for the symmetric quantiles where    tail_prob=0.1  is the probability in the tails. This kind diagnoses poor convergence in   the tails of the distribution. If this kind is chosen,  kwargs  may contain a    tail_prob  keyword. :basic : basic ESS, equivalent to specifying  kind=Statistics.mean . Note While Bulk-ESS is conceptually related to basic ESS, it is well-defined even if the chains do not have finite variance. [VehtariGelman2021]  For each parameter, rank-normalization proceeds by first ranking the inputs using \"tied ranking\" and then transforming the ranks to normal quantiles so that the result is standard normally distributed. This transform is monotonic. Otherwise,  kind  specifies one of the following estimators, whose ESS is to be estimated: Statistics.mean Statistics.median Statistics.std StatsBase.mad Base.Fix2(Statistics.quantile, p::Real) source"},{"id":36,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.rhat","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.rhat","content":" MCMCDiagnosticTools.rhat  —  Function rhat(data::InferenceData; kwargs...) -> Dataset\nrhat(data::Dataset; kwargs...) -> Dataset Calculate the  $\\widehat{R}$  diagnostic for each parameter in the data. source rhat(samples::AbstractArray{Union{Real,Missing}}; kind::Symbol=:rank, split_chains=2) Compute the  $\\widehat{R}$  diagnostics for each parameter in  samples  of shape  (draws, [chains[, parameters...]]) . [VehtariGelman2021] kind  indicates the kind of  $\\widehat{R}$  to compute (see below). split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. See also  ess ,  ess_rhat ,  rstar Kinds of  $\\widehat{R}$ The following  kind s are supported: :rank : maximum of  $\\widehat{R}$  with  kind=:bulk  and  kind=:tail . :bulk : basic  $\\widehat{R}$  computed on rank-normalized draws. This kind diagnoses   poor convergence in the bulk of the distribution due to trends or different locations of   the chains. :tail :  $\\widehat{R}$  computed on draws folded around the median and then   rank-normalized. This kind diagnoses poor convergence in the tails of the distribution   due to different scales of the chains. :basic : Classic  $\\widehat{R}$ . source"},{"id":37,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.ess_rhat","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.ess_rhat","content":" MCMCDiagnosticTools.ess_rhat  —  Function ess_rhat(data::InferenceData; kwargs...) -> Dataset\ness_rhat(data::Dataset; kwargs...) -> Dataset Calculate the effective sample size (ESS) and  $\\widehat{R}$  diagnostic for each parameter in the data. source ess_rhat(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind::Symbol=:rank,\n    kwargs...,\n) -> NamedTuple{(:ess, :rhat)} Estimate the effective sample size and  $\\widehat{R}$  of the  samples  of shape  (draws, [chains[, parameters...]]) . When both ESS and  $\\widehat{R}$  are needed, this method is often more efficient than calling  ess  and  rhat  separately. See  rhat  for a description of supported  kind s and  ess  for a description of  kwargs . source The following autocovariance methods are supported:"},{"id":38,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.AutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.AutocovMethod","content":" MCMCDiagnosticTools.AutocovMethod  —  Type AutocovMethod <: AbstractAutocovMethod The  AutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021]  and uses the biased estimator of the autocovariance, as discussed by  [Geyer1992] . source"},{"id":39,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.FFTAutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.FFTAutocovMethod","content":" MCMCDiagnosticTools.FFTAutocovMethod  —  Type FFTAutocovMethod <: AbstractAutocovMethod The  FFTAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. The algorithm is the same as the one of  AutocovMethod  but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation. Info To be able to use this method, you have to load a package that implements the  AbstractFFTs.jl  interface such as  FFTW.jl  or  FastTransforms.jl . source"},{"id":40,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.BDAAutocovMethod","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.BDAAutocovMethod","content":" MCMCDiagnosticTools.BDAAutocovMethod  —  Type BDAAutocovMethod <: AbstractAutocovMethod The  BDAAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021] . and uses the variogram estimator of the autocorrelation function discussed by  [BDA3] . source"},{"id":41,"pagetitle":"Diagnostics","title":"Monte Carlo standard error","ref":"/ArviZ/stable/api/diagnostics/#mcse","content":" Monte Carlo standard error"},{"id":42,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.mcse","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.mcse","content":" MCMCDiagnosticTools.mcse  —  Function mcse(data::InferenceData; kwargs...) -> Dataset\nmcse(data::Dataset; kwargs...) -> Dataset Calculate the Monte Carlo standard error (MCSE) for each parameter in the data. source mcse(samples::AbstractArray{<:Union{Missing,Real}}; kind=Statistics.mean, kwargs...) Estimate the Monte Carlo standard errors (MCSE) of the estimator  kind  applied to  samples  of shape  (draws, [chains[, parameters...]]) . See also:  ess Kinds of MCSE estimates The estimator whose MCSE should be estimated is specified with  kind .  kind  must accept a vector of the same  eltype  as  samples  and return a real estimate. For the following estimators, the effective sample size  ess  and an estimate of the asymptotic variance are used to compute the MCSE, and  kwargs  are forwarded to  ess : Statistics.mean Statistics.median Statistics.std Base.Fix2(Statistics.quantile, p::Real) For other estimators, the subsampling bootstrap method (SBM) [FlegalJones2011] [Flegal2012]  is used as a fallback, and the only accepted  kwargs  are  batch_size , which indicates the size of the overlapping batches used to estimate the MCSE, defaulting to  floor(Int, sqrt(draws * chains)) . Note that SBM tends to underestimate the MCSE, especially for highly autocorrelated chains. One should verify that autocorrelation is low by checking the bulk- and tail-ESS values. source"},{"id":43,"pagetitle":"Diagnostics","title":"$R^*$  diagnostic","ref":"/ArviZ/stable/api/diagnostics/#rstar","content":" $R^*$  diagnostic"},{"id":44,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.rstar","ref":"/ArviZ/stable/api/diagnostics/#MCMCDiagnosticTools.rstar","content":" MCMCDiagnosticTools.rstar  —  Function rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    data::Union{InferenceData,Dataset};\n    kwargs...,\n) Calculate the  $R^*$  diagnostic for the data. source rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples,\n    chain_indices::AbstractVector{Int};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the table  samples  with the  classifier . samples  must be either an  AbstractMatrix , an  AbstractVector , or a table (i.e. implements the Tables.jl interface) whose rows are draws and whose columns are parameters. chain_indices  indicates the chain ids of each row of  samples . This method supports ragged chains, i.e. chains of nonequal lengths. source rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples::AbstractArray{<:Real};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the  samples  with the  classifier . samples  is an array of draws with the shape  (draws, [chains[, parameters...]]) .` This implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari. The  classifier  has to be a supervised classifier of the MLJ framework (see the  MLJ documentation  for a list of supported models). It is trained with a  subset  of the samples from each chain. Each chain is split into  split_chains  separate chains to additionally check for within-chain convergence. The training of the classifier can be inspected by adjusting the  verbosity  level. If the classifier is deterministic, i.e., if it predicts a class, the value of the  $R^*$  statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the  $R^*$  statistic is returned (algorithm 2). Note The correctness of the statistic depends on the convergence of the  classifier  used internally in the statistic. Examples julia> using MLJBase, MLJIteration, EvoTrees, Statistics, StatisticalMeasures\n\njulia> samples = fill(4.0, 100, 3, 2); One can compute the distribution of the  $R^*$  statistic (algorithm 2) with a probabilistic classifier. For instance, we can use a gradient-boosted trees model with  nrounds = 100  sequentially stacked trees and learning rate  eta = 0.05 : julia> model = EvoTreeClassifier(; nrounds=100, eta=0.05);\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 Note, however, that it is recommended to determine  nrounds  based on early-stopping. With the MLJ framework, this can be achieved in the following way (see the  MLJ documentation  for additional explanations): julia> model = IteratedModel(;\n           model=EvoTreeClassifier(; eta=0.05),\n           iteration_parameter=:nrounds,\n           resampling=Holdout(),\n           measures=log_loss,\n           controls=[Step(5), Patience(2), NumberLimit(100)],\n           retrain=true,\n       );\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 For deterministic classifiers, a single  $R^*$  statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models. julia> evotree_deterministic = Pipeline(model; operation=predict_mode);\n\njulia> value = rstar(evotree_deterministic, samples);\n\njulia> round(value; digits=2)\n1.0 References Lambert, B., & Vehtari, A. (2020).  $R^*$ : A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers. source Betancourt2018 Betancourt M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo.  arXiv:1701.02434v2  [stat.ME] Betancourt2016 Betancourt M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.  arXiv:1604.00695v1  [stat.ME] VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 Geyer1992 Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483. VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 BDA3 Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press. FlegalJones2011 Flegal JM, Jones GL. (2011) Implementing MCMC: estimating with confidence.                 Handbook of Markov Chain Monte Carlo. pp. 175-97.                  pdf Flegal2012 Flegal JM. (2012) Applicability of subsampling bootstrap methods in Markov chain Monte Carlo.            Monte Carlo and Quasi-Monte Carlo Methods 2010. pp. 363-72.            doi:  10.1007/978-3-642-27440-4_18"},{"id":47,"pagetitle":"InferenceData","title":"InferenceData","ref":"/ArviZ/stable/api/inference_data/#inferencedata-api","content":" InferenceData InferenceObjects.InferenceData Base.cat Base.getindex Base.getproperty Base.merge Base.propertynames Base.setindex InferenceObjects.convert_to_inference_data InferenceObjects.from_dict InferenceObjects.from_namedtuple"},{"id":48,"pagetitle":"InferenceData","title":"Type definition","ref":"/ArviZ/stable/api/inference_data/#Type-definition","content":" Type definition"},{"id":49,"pagetitle":"InferenceData","title":"InferenceObjects.InferenceData","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.InferenceData","content":" InferenceObjects.InferenceData  —  Type InferenceData{group_names,group_types} Container for inference data storage using DimensionalData. This object implements the  InferenceData schema . Internally, groups are stored in a  NamedTuple , which can be accessed using  parent(::InferenceData) . Constructors InferenceData(groups::NamedTuple)\nInferenceData(; groups...) Construct an inference data from either a  NamedTuple  or keyword arguments of groups. Groups must be  Dataset  objects. Instead of directly creating an  InferenceData , use the exported  from_xyz  functions or  convert_to_inference_data . source"},{"id":50,"pagetitle":"InferenceData","title":"Property interface","ref":"/ArviZ/stable/api/inference_data/#Property-interface","content":" Property interface"},{"id":51,"pagetitle":"InferenceData","title":"Base.getproperty","ref":"/ArviZ/stable/api/inference_data/#Base.getproperty","content":" Base.getproperty  —  Function getproperty(data::InferenceData, name::Symbol) -> Dataset Get group with the specified  name . source"},{"id":52,"pagetitle":"InferenceData","title":"Base.propertynames","ref":"/ArviZ/stable/api/inference_data/#Base.propertynames","content":" Base.propertynames  —  Function propertynames(data::InferenceData) -> Tuple{Symbol} Get names of groups source"},{"id":53,"pagetitle":"InferenceData","title":"Indexing interface","ref":"/ArviZ/stable/api/inference_data/#Indexing-interface","content":" Indexing interface"},{"id":54,"pagetitle":"InferenceData","title":"Base.getindex","ref":"/ArviZ/stable/api/inference_data/#Base.getindex","content":" Base.getindex  —  Function Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData Return a new  InferenceData  containing the specified groups sliced to the specified coords. coords  specifies a dimension name mapping to an index, a  DimensionalData.Selector , or an  IntervalSets.AbstractInterval . If one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised. Examples Select data from all groups for just the specified id values. julia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\" Select data from just the posterior, returning a  Dataset  if the indices index more than one element from any of the variables: julia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:19:25.982\" Note that if a single index is provided, the behavior is still to slice so that the dimension is preserved. source"},{"id":55,"pagetitle":"InferenceData","title":"Base.setindex","ref":"/ArviZ/stable/api/inference_data/#Base.setindex","content":" Base.setindex  —  Function Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData Create a new  InferenceData  containing the  group  with the specified  name . If a group with  name  is already in  data , it is replaced. source"},{"id":56,"pagetitle":"InferenceData","title":"Iteration interface","ref":"/ArviZ/stable/api/inference_data/#Iteration-interface","content":" Iteration interface InferenceData  also implements the same iteration interface as its underlying  NamedTuple . That is, iterating over an  InferenceData  iterates over its groups."},{"id":57,"pagetitle":"InferenceData","title":"General conversion","ref":"/ArviZ/stable/api/inference_data/#General-conversion","content":" General conversion"},{"id":58,"pagetitle":"InferenceData","title":"InferenceObjects.convert_to_inference_data","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.convert_to_inference_data","content":" InferenceObjects.convert_to_inference_data  —  Function convert_to_inference_data(obj; group, kwargs...) -> InferenceData Convert a supported object to an  InferenceData  object. If  obj  converts to a single dataset,  group  specifies which dataset in the resulting  InferenceData  that is. See  convert_to_dataset Arguments obj  can be many objects. Basic supported types are: InferenceData : return unchanged Dataset / DimensionalData.AbstractDimStack : add to  InferenceData  as the only group NamedTuple / AbstractDict : create a  Dataset  as the only group AbstractArray{<:Real} : create a  Dataset  as the only group, given an arbitrary name, if the name is not set More specific types may be documented separately. Keywords group::Symbol = :posterior : If  obj  converts to a single dataset, assign the resulting dataset to this group. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. kwargs : remaining keywords forwarded to converter functions source"},{"id":59,"pagetitle":"InferenceData","title":"InferenceObjects.from_dict","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.from_dict","content":" InferenceObjects.from_dict  —  Function from_dict(posterior::AbstractDict; kwargs...) -> InferenceData Convert a  Dict  to an  InferenceData . Arguments posterior : The data to be converted. Its strings must be  Symbol  or  AbstractString , and its values must be arrays. Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior::Dict=nothing : Draws from the prior prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata = Dict(\n    :x => rand(ndraws, nchains),\n    :y => randn(2, ndraws, nchains),\n    :z => randn(3, 2, ndraws, nchains),\n)\nidata = from_dict(data) source"},{"id":60,"pagetitle":"InferenceData","title":"InferenceObjects.from_namedtuple","ref":"/ArviZ/stable/api/inference_data/#InferenceObjects.from_namedtuple","content":" InferenceObjects.from_namedtuple  —  Function from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData Convert a  NamedTuple  or container of  NamedTuple s to an  InferenceData . If containers are passed, they are flattened into a single  NamedTuple  with array elements whose first dimensions correspond to the dimensions of the containers. Arguments posterior : The data to be converted. It may be of the following types: ::NamedTuple : The keys are the variable names and the values are arrays with dimensions  (ndraws, nchains[, sizes...]) . ::Vector{Vector{<:NamedTuple}} : A vector of length  nchains  whose elements have length  ndraws . Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior=nothing : Draws from the prior. Accepts the same types as  posterior . prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Note If a  NamedTuple  is provided for  observed_data ,  constant_data , or predictions constant data`, any non-array values (e.g. integers) are converted to 0-dimensional arrays. Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata1 = (\n    x=rand(ndraws, nchains), y=randn(ndraws, nchains, 2), z=randn(ndraws, nchains, 3, 2)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata2 = from_namedtuple(data2) source"},{"id":61,"pagetitle":"InferenceData","title":"General functions","ref":"/ArviZ/stable/api/inference_data/#General-functions","content":" General functions"},{"id":62,"pagetitle":"InferenceData","title":"Base.cat","ref":"/ArviZ/stable/api/inference_data/#Base.cat","content":" Base.cat  —  Function cat(data::InferenceData...; [groups=keys(data[1]),] dims) -> InferenceData Concatenate  InferenceData  objects along the specified dimension  dims . Only the groups in  groups  are concatenated. Remaining groups are  merge d into the new  InferenceData  object. Examples Here is how we can concatenate all groups of two  InferenceData  objects along the existing  chain  dimension: julia> coords = (; a_dim=[\"x\", \"y\", \"z\"]);\n\njulia> dims = dims=(; a=[:a_dim]);\n\njulia> data = Dict(:a => randn(100, 4, 3), :b => randn(100, 4));\n\njulia> idata = from_dict(data; coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1 = cat(idata, idata; dims=:chain)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1.posterior\n╭─────────────────╮\n│ 100×8×3 Dataset │\n├─────────────────┴──────────────────────────────────── dims ┐\n  ↓ draw ,\n  → chain,\n  ↗ a_dim Categorical{String} [\"x\", \"y\", \"z\"] ForwardOrdered\n├──────────────────────────────────────────────────── layers ┤\n  :a eltype: Float64 dims: draw, chain, a_dim size: 100×8×3\n  :b eltype: Float64 dims: draw, chain size: 100×8\n├────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2024-03-11T14:10:48.434\" Alternatively, we can concatenate along a new  run  dimension, which will be created. julia> idata_cat2 = cat(idata, idata; dims=:run)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat2.posterior\n╭───────────────────╮\n│ 100×4×3×2 Dataset │\n├───────────────────┴─────────────────────────────────── dims ┐\n  ↓ draw ,\n  → chain,\n  ↗ a_dim Categorical{String} [\"x\", \"y\", \"z\"] ForwardOrdered,\n  ⬔ run\n├─────────────────────────────────────────────────────────────┴ layers ┐\n  :a eltype: Float64 dims: draw, chain, a_dim, run size: 100×4×3×2\n  :b eltype: Float64 dims: draw, chain, run size: 100×4×2\n├──────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2024-03-11T14:10:48.434\" We can also concatenate only a subset of groups and merge the rest, which is useful when some groups are present only in some of the  InferenceData  objects or will be identical in all of them: julia> observed_data = Dict(:y => randn(10));\n\njulia> idata2 = from_dict(data; observed_data=observed_data, coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3 = cat(idata, idata2; groups=(:posterior,), dims=:run)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3.posterior\n╭───────────────────╮\n│ 100×4×3×2 Dataset │\n├───────────────────┴─────────────────────────────────── dims ┐\n  ↓ draw ,\n  → chain,\n  ↗ a_dim Categorical{String} [\"x\", \"y\", \"z\"] ForwardOrdered,\n  ⬔ run\n├─────────────────────────────────────────────────────────────┴ layers ┐\n  :a eltype: Float64 dims: draw, chain, a_dim, run size: 100×4×3×2\n  :b eltype: Float64 dims: draw, chain, run size: 100×4×2\n├──────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2024-03-11T14:10:48.434\"\n\njulia> idata_cat3.observed_data\n╭────────────────────╮\n│ 10-element Dataset │\n├────────────── dims ┤\n  ↓ y_dim_1\n├────────────────────┴─────────────── layers ┐\n  :y eltype: Float64 dims: y_dim_1 size: 10\n├────────────────────────────────────────────┴ metadata ┐\n  Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2024-03-11T14:10:53.539\" source"},{"id":63,"pagetitle":"InferenceData","title":"Base.merge","ref":"/ArviZ/stable/api/inference_data/#Base.merge","content":" Base.merge  —  Function merge(data::InferenceData...) -> InferenceData Merge  InferenceData  objects. The result contains all groups in  data  and  others . If a group appears more than once, the one that occurs last is kept. See also:  cat Examples Here we merge an  InferenceData  containing only a posterior group with one containing only a prior group to create a new one containing both groups. julia> idata1 = from_dict(Dict(:a => randn(100, 4, 3), :b => randn(100, 4)))\nInferenceData with groups:\n  > posterior\n\njulia> idata2 = from_dict(; prior=Dict(:a => randn(100, 1, 3), :c => randn(100, 1)))\nInferenceData with groups:\n  > prior\n\njulia> idata_merged = merge(idata1, idata2)\nInferenceData with groups:\n  > posterior\n  > prior source"},{"id":66,"pagetitle":"Stats","title":"Stats","ref":"/ArviZ/stable/api/stats/#stats-api","content":" Stats PSIS.PSISResult PosteriorStats.AbstractELPDResult PosteriorStats.AbstractModelWeightsMethod PosteriorStats.BootstrappedPseudoBMA PosteriorStats.ModelComparisonResult PosteriorStats.PSISLOOResult PosteriorStats.PseudoBMA PosteriorStats.Stacking PosteriorStats.SummaryStats PosteriorStats.WAICResult PSIS.PSISPlots.paretoshapeplot PSIS.ess_is PSIS.psis PSIS.psis! PosteriorStats.compare PosteriorStats.default_diagnostics PosteriorStats.default_stats PosteriorStats.default_summary_stats PosteriorStats.elpd_estimates PosteriorStats.hdi PosteriorStats.hdi! PosteriorStats.information_criterion PosteriorStats.loo PosteriorStats.loo_pit PosteriorStats.model_weights PosteriorStats.r2_score PosteriorStats.smooth_data PosteriorStats.summarize PosteriorStats.waic StatsBase.summarystats"},{"id":67,"pagetitle":"Stats","title":"Summary statistics","ref":"/ArviZ/stable/api/stats/#Summary-statistics","content":" Summary statistics"},{"id":68,"pagetitle":"Stats","title":"PosteriorStats.SummaryStats","ref":"/ArviZ/stable/api/stats/#PosteriorStats.SummaryStats","content":" PosteriorStats.SummaryStats  —  Type struct SummaryStats{D, V<:(AbstractVector)} A container for a column table of values computed by  summarize . This object implements the Tables and TableTraits column table interfaces. It has a custom  show  method. SummaryStats  behaves like an  OrderedDict  of columns, where the columns can be accessed using either  Symbol s or a 1-based integer index. name::String : The name of the collection of summary statistics, used as the table title in display. data::Any : The summary statistics for each parameter. It must implement the Tables interface. parameter_names::AbstractVector : Names of the parameters SummaryStats([name::String,] data[, parameter_names])\nSummaryStats(data[, parameter_names]; name::String=\"SummaryStats\") Construct a  SummaryStats  from tabular  data  with optional stats  name  and  param_names . data  must not contain a column  :parameter , as this is reserved for the parameter names, which are always in the first column. source"},{"id":69,"pagetitle":"Stats","title":"PosteriorStats.default_summary_stats","ref":"/ArviZ/stable/api/stats/#PosteriorStats.default_summary_stats","content":" PosteriorStats.default_summary_stats  —  Function default_summary_stats(focus=Statistics.mean; kwargs...) Combinatiton of  default_stats  and  default_diagnostics  to be used with  summarize . source"},{"id":70,"pagetitle":"Stats","title":"PosteriorStats.default_stats","ref":"/ArviZ/stable/api/stats/#PosteriorStats.default_stats","content":" PosteriorStats.default_stats  —  Function default_stats(focus=Statistics.mean; prob_interval=0.94, kwargs...) Default statistics to be computed with  summarize . The value of  focus  determines the statistics to be returned: Statistics.mean :  mean ,  std ,  hdi_3% ,  hdi_97% Statistics.median :  median ,  mad ,  eti_3% ,  eti_97% If  prob_interval  is set to a different value than the default, then different HDI and ETI statistics are computed accordingly.  hdi  refers to the highest-density interval, while  eti  refers to the equal-tailed interval (i.e. the credible interval computed from symmetric quantiles). See also:  hdi source"},{"id":71,"pagetitle":"Stats","title":"PosteriorStats.default_diagnostics","ref":"/ArviZ/stable/api/stats/#PosteriorStats.default_diagnostics","content":" PosteriorStats.default_diagnostics  —  Function default_diagnostics(focus=Statistics.mean; kwargs...) Default diagnostics to be computed with  summarize . The value of  focus  determines the diagnostics to be returned: Statistics.mean :  mcse_mean ,  mcse_std ,  ess_tail ,  ess_bulk ,  rhat Statistics.median :  mcse_median ,  ess_tail ,  ess_bulk ,  rhat source"},{"id":72,"pagetitle":"Stats","title":"PosteriorStats.summarize","ref":"/ArviZ/stable/api/stats/#PosteriorStats.summarize","content":" PosteriorStats.summarize  —  Function summarize(data, stats_funs...; name=\"SummaryStats\", [var_names]) -> SummaryStats Compute the summary statistics in  stats_funs  on each param in  data . stats_funs  is a collection of functions that reduces a matrix with shape  (draws, chains)  to a scalar or a collection of scalars. Alternatively, an item in  stats_funs  may be a  Pair  of the form  name => fun  specifying the name to be used for the statistic or of the form  (name1, ...) => fun  when the function returns a collection. When the function returns a collection, the names in this latter format must be provided. If no stats functions are provided, then those specified in  default_summary_stats  are computed. var_names  specifies the names of the parameters in  data . If not provided, the names are inferred from  data . To support computing summary statistics from a custom object, overload this method specifying the type of  data . See also  SummaryStats ,  default_summary_stats ,  default_stats ,  default_diagnostics . Examples Compute  mean ,  std  and the Monte Carlo standard error (MCSE) of the mean estimate: julia> using Statistics, StatsBase\n\njulia> x = randn(1000, 4, 3) .+ reshape(0:10:20, 1, 1, :);\n\njulia> summarize(x, mean, std, :mcse_mean => sem; name=\"Mean/Std\")\nMean/Std\n       mean    std  mcse_mean\n 1   0.0003  0.990      0.016\n 2  10.02    0.988      0.016\n 3  19.98    0.988      0.016 Avoid recomputing the mean by using  mean_and_std , and provide parameter names: julia> summarize(x, (:mean, :std) => mean_and_std, mad; var_names=[:a, :b, :c])\nSummaryStats\n         mean    std    mad\n a   0.000305  0.990  0.978\n b  10.0       0.988  0.995\n c  20.0       0.988  0.979 Note that when an estimator and its MCSE are both computed, the MCSE is used to determine the number of significant digits that will be displayed. julia> summarize(x; var_names=[:a, :b, :c])\nSummaryStats\n       mean   std  hdi_3%  hdi_97%  mcse_mean  mcse_std  ess_tail  ess_bulk  r ⋯\n a   0.0003  0.99   -1.92     1.78      0.016     0.012      3567      3663  1 ⋯\n b  10.02    0.99    8.17    11.9       0.016     0.011      3841      3906  1 ⋯\n c  19.98    0.99   18.1     21.9       0.016     0.012      3892      3749  1 ⋯\n                                                                1 column omitted Compute just the statistics with an 89% HDI on all parameters, and provide the parameter names: julia> summarize(x, default_stats(; prob_interval=0.89)...; var_names=[:a, :b, :c])\nSummaryStats\n         mean    std  hdi_5.5%  hdi_94.5%\n a   0.000305  0.990     -1.63       1.52\n b  10.0       0.988      8.53      11.6\n c  20.0       0.988     18.5       21.6 Compute the summary stats focusing on  Statistics.median : julia> summarize(x, default_summary_stats(median)...; var_names=[:a, :b, :c])\nSummaryStats\n    median    mad  eti_3%  eti_97%  mcse_median  ess_tail  ess_median  rhat\n a   0.004  0.978   -1.83     1.89        0.020      3567        3336  1.00\n b  10.02   0.995    8.17    11.9         0.023      3841        3787  1.00\n c  19.99   0.979   18.1     21.9         0.020      3892        3829  1.00 source"},{"id":73,"pagetitle":"Stats","title":"StatsBase.summarystats","ref":"/ArviZ/stable/api/stats/#StatsBase.summarystats","content":" StatsBase.summarystats  —  Function summarystats(data::InferenceData; group=:posterior, kwargs...) -> SummaryStats\nsummarystats(data::Dataset; kwargs...) -> SummaryStats Compute default summary statistics for the data using  summarize . source"},{"id":74,"pagetitle":"Stats","title":"General statistics","ref":"/ArviZ/stable/api/stats/#General-statistics","content":" General statistics"},{"id":75,"pagetitle":"Stats","title":"PosteriorStats.hdi","ref":"/ArviZ/stable/api/stats/#PosteriorStats.hdi","content":" PosteriorStats.hdi  —  Function hdi(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) Estimate the unimodal highest density interval (HDI) of  samples  for the probability  prob . The HDI is the minimum width Bayesian credible interval (BCI). That is, it is the smallest possible interval containing  (100*prob) % of the probability mass. [Hyndman1996] samples  is an array of shape  (draws[, chains[, params...]]) . If multiple parameters are present, then  lower  and  upper  are arrays with the shape  (params...,) , computed separately for each marginal. This implementation uses the algorithm of  [ChenShao1999] . Note Any default value of  prob  is arbitrary. The default value of  prob=0.94  instead of a more common default like  prob=0.95  is chosen to reminder the user of this arbitrariness. Examples Here we calculate the 83% HDI for a normal random variable: julia> x = randn(2_000);\n\njulia> hdi(x; prob=0.83) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => -1.38266\n  :upper => 1.25982 We can also calculate the HDI for a 3-dimensional array of samples: julia> x = randn(1_000, 1, 1) .+ reshape(0:5:10, 1, 1, :);\n\njulia> hdi(x) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => [-1.9674, 3.0326, 8.0326]\n  :upper => [1.90028, 6.90028, 11.9003] source hdi(data::InferenceData; kwargs...) -> Dataset\nhdi(data::Dataset; kwargs...) -> Dataset Calculate the highest density interval (HDI) for each parameter in the data. source"},{"id":76,"pagetitle":"Stats","title":"PosteriorStats.hdi!","ref":"/ArviZ/stable/api/stats/#PosteriorStats.hdi!","content":" PosteriorStats.hdi!  —  Function hdi!(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) A version of  hdi  that sorts  samples  in-place while computing the HDI. source"},{"id":77,"pagetitle":"Stats","title":"PosteriorStats.r2_score","ref":"/ArviZ/stable/api/stats/#PosteriorStats.r2_score","content":" PosteriorStats.r2_score  —  Function r2_score(y_true::AbstractVector, y_pred::AbstractArray) -> (; r2, r2_std) $R²$  for linear Bayesian regression models. [GelmanGoodrich2019] Arguments y_true : Observed data of length  noutputs y_pred : Predicted data with size  (ndraws[, nchains], noutputs) Examples julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"regression1d\");\n\njulia> y_true = idata.observed_data.y;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.y, (:draw, :chain, :y_dim_0));\n\njulia> r2_score(y_true, y_pred) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.683197\n  :r2_std => 0.0368838 source r2_score(idata::InferenceData; y_name, y_pred_name) -> (; r2, r2_std) Compute  $R²$  from  idata , automatically formatting the predictions to the correct shape. Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. Examples julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"regression10d\");\n\njulia> r2_score(idata) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.998385\n  :r2_std => 0.000100621 source"},{"id":78,"pagetitle":"Stats","title":"Pareto-smoothed importance sampling","ref":"/ArviZ/stable/api/stats/#Pareto-smoothed-importance-sampling","content":" Pareto-smoothed importance sampling"},{"id":79,"pagetitle":"Stats","title":"PSIS.PSISResult","ref":"/ArviZ/stable/api/stats/#PSIS.PSISResult","content":" PSIS.PSISResult  —  Type PSISResult Result of Pareto-smoothed importance sampling (PSIS) using  psis . Properties log_weights : un-normalized Pareto-smoothed log weights weights : normalized Pareto-smoothed weights (allocates a copy) pareto_shape : Pareto  $k=ξ$  shape parameter nparams : number of parameters in  log_weights ndraws : number of draws in  log_weights nchains : number of chains in  log_weights reff : the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size. ess : estimated effective sample size of estimate of mean using smoothed importance samples (see  ess_is ) tail_length : length of the upper tail of  log_weights  that was smoothed tail_dist : the generalized Pareto distribution that was fit to the tail of  log_weights . Note that the tail weights are scaled to have a maximum of 1, so  tail_dist * exp(maximum(log_ratios))  is the corresponding fit directly to the tail of  log_ratios . normalized::Bool :indicates whether  log_weights  are log-normalized along the sample dimensions. Diagnostic The  pareto_shape  parameter  $k=ξ$  of the generalized Pareto distribution  tail_dist  can be used to diagnose reliability and convergence of estimates using the importance weights  [VehtariSimpson2021] . if  $k < \\frac{1}{3}$ , importance sampling is stable, and importance sampling (IS) and PSIS both are reliable. if  $k ≤ \\frac{1}{2}$ , then the importance ratio distributon has finite variance, and the central limit theorem holds. As  $k$  approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE. if  $\\frac{1}{2} < k ≤ 0.7$ , then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime. if  $0.7 < k ≤ 1$ , then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended. if  $k > 1$ , then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes. See  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":80,"pagetitle":"Stats","title":"PSIS.ess_is","ref":"/ArviZ/stable/api/stats/#PSIS.ess_is","content":" PSIS.ess_is  —  Function ess_is(weights; reff=1) Estimate effective sample size (ESS) for importance sampling over the sample dimensions. Given normalized weights  $w_{1:n}$ , the ESS is estimated using the L2-norm of the weights: \\[\\mathrm{ESS}(w_{1:n}) = \\frac{r_{\\mathrm{eff}}}{\\sum_{i=1}^n w_i^2}\\] where  $r_{\\mathrm{eff}}$  is the relative efficiency of the  log_weights . ess_is(result::PSISResult; bad_shape_nan=true) Estimate ESS for Pareto-smoothed importance sampling. Note ESS estimates for Pareto shape values  $k > 0.7$ , which are unreliable and misleadingly high, are set to  NaN . To avoid this, set  bad_shape_nan=false . source"},{"id":81,"pagetitle":"Stats","title":"PSIS.PSISPlots.paretoshapeplot","ref":"/ArviZ/stable/api/stats/#PSIS.PSISPlots.paretoshapeplot","content":" PSIS.PSISPlots.paretoshapeplot  —  Function paretoshapeplot(values; showlines=false, ...)\nparetoshapeplot!(values; showlines=false, kwargs...) Plot shape parameters of fitted Pareto tail distributions for diagnosing convergence. values  may be either a vector of Pareto shape parameters or a  PSIS.PSISResult . If  showlines==true , horizontal lines indicating relevant Pareto shape thresholds are drawn. See  PSIS.PSISResult  for an explanation of the thresholds. All remaining  kwargs  are forwarded to the plotting function. See  psis ,  PSISResult . Examples using PSIS, Distributions, Plots\nproposal = Normal()\ntarget = TDist(7)\nx = rand(proposal, 1_000, 100)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios)\nparetoshapeplot(result) We can also plot the Pareto shape parameters directly: paretoshapeplot(result.pareto_shape) We can also use  plot  directly: plot(result.pareto_shape; showlines=true) source"},{"id":82,"pagetitle":"Stats","title":"PSIS.psis","ref":"/ArviZ/stable/api/stats/#PSIS.psis","content":" PSIS.psis  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":83,"pagetitle":"Stats","title":"PSIS.psis!","ref":"/ArviZ/stable/api/stats/#PSIS.psis!","content":" PSIS.psis!  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":84,"pagetitle":"Stats","title":"LOO and WAIC","ref":"/ArviZ/stable/api/stats/#LOO-and-WAIC","content":" LOO and WAIC"},{"id":85,"pagetitle":"Stats","title":"PosteriorStats.AbstractELPDResult","ref":"/ArviZ/stable/api/stats/#PosteriorStats.AbstractELPDResult","content":" PosteriorStats.AbstractELPDResult  —  Type abstract type AbstractELPDResult An abstract type representing the result of an ELPD computation. Every subtype stores estimates of both the expected log predictive density ( elpd ) and the effective number of parameters  p , as well as standard errors and pointwise estimates of each, from which other relevant estimates can be computed. Subtypes implement the following functions: elpd_estimates information_criterion source"},{"id":86,"pagetitle":"Stats","title":"PosteriorStats.PSISLOOResult","ref":"/ArviZ/stable/api/stats/#PosteriorStats.PSISLOOResult","content":" PosteriorStats.PSISLOOResult  —  Type Results of Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO). See also:  loo ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates psis_result : Pareto-smoothed importance sampling (PSIS) results source"},{"id":87,"pagetitle":"Stats","title":"PosteriorStats.WAICResult","ref":"/ArviZ/stable/api/stats/#PosteriorStats.WAICResult","content":" PosteriorStats.WAICResult  —  Type Results of computing the widely applicable information criterion (WAIC). See also:  waic ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates source"},{"id":88,"pagetitle":"Stats","title":"PosteriorStats.elpd_estimates","ref":"/ArviZ/stable/api/stats/#PosteriorStats.elpd_estimates","content":" PosteriorStats.elpd_estimates  —  Function elpd_estimates(result::AbstractELPDResult; pointwise=false) -> (; elpd, elpd_mcse, lpd) Return the (E)LPD estimates from the  result . source"},{"id":89,"pagetitle":"Stats","title":"PosteriorStats.information_criterion","ref":"/ArviZ/stable/api/stats/#PosteriorStats.information_criterion","content":" PosteriorStats.information_criterion  —  Function information_criterion(elpd, scale::Symbol) Compute the information criterion for the given  scale  from the  elpd  estimate. scale  must be one of  (:deviance, :log, :negative_log) . See also:  loo ,  waic source information_criterion(result::AbstractELPDResult, scale::Symbol; pointwise=false) Compute information criterion for the given  scale  from the existing ELPD  result . scale  must be one of  (:deviance, :log, :negative_log) . If  pointwise=true , then pointwise estimates are returned. source"},{"id":90,"pagetitle":"Stats","title":"PosteriorStats.loo","ref":"/ArviZ/stable/api/stats/#PosteriorStats.loo","content":" PosteriorStats.loo  —  Function loo(log_likelihood; reff=nothing, kwargs...) -> PSISLOOResult{<:NamedTuple,<:NamedTuple} Compute the Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO).  [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . Keywords reff::Union{Real,AbstractArray{<:Real}} : The relative effective sample size(s) of the  likelihood  values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using  MCMCDiagnosticTools.ess . kwargs : Remaining keywords are forwarded to [ PSIS.psis ]. See also:  PSISLOOResult ,  waic Examples Manually compute  $R_\\mathrm{eff}$  and calculate PSIS-LOO of a model: julia> using ArviZExampleData, MCMCDiagnosticTools\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> reff = ess(log_like; kind=:basic, split_chains=1, relative=true);\n\njulia> loo(log_like; reff)\nPSISLOOResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.34\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  7 (87.5%)  151\n  (0.5, 0.7]  okay  1 (12.5%)  446 source loo(data::Dataset; [var_name::Symbol,] kwargs...) -> PSISLOOResult{<:NamedTuple,<:Dataset}\nloo(data::InferenceData; [var_name::Symbol,] kwargs...) -> PSISLOOResult{<:NamedTuple,<:Dataset} Compute PSIS-LOO from log-likelihood values in  data . If more than one log-likelihood variable is present, then  var_name  must be provided. Examples Calculate PSIS-LOO of a model: julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo(idata)\nPSISLOOResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.34\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  6 (75.0%)  135\n  (0.5, 0.7]  okay  2 (25.0%)  421 source"},{"id":91,"pagetitle":"Stats","title":"PosteriorStats.waic","ref":"/ArviZ/stable/api/stats/#PosteriorStats.waic","content":" PosteriorStats.waic  —  Function waic(log_likelihood::AbstractArray) -> WAICResult{<:NamedTuple,<:NamedTuple} Compute the widely applicable information criterion (WAIC). [Watanabe2010] [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . See also:  WAICResult ,  loo Examples Calculate WAIC of a model: julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> waic(log_like)\nWAICResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.33 source waic(data::Dataset; [var_name::Symbol]) -> WAICResult{<:NamedTuple,<:Dataset}\nwaic(data::InferenceData; [var_name::Symbol]) -> WAICResult{<:NamedTuple,<:Dataset} Compute WAIC from log-likelihood values in  data . If more than one log-likelihood variable is present, then  var_name  must be provided. Examples Calculate WAIC of a model: julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> waic(idata)\nWAICResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.33 source"},{"id":92,"pagetitle":"Stats","title":"Model comparison","ref":"/ArviZ/stable/api/stats/#Model-comparison","content":" Model comparison"},{"id":93,"pagetitle":"Stats","title":"PosteriorStats.ModelComparisonResult","ref":"/ArviZ/stable/api/stats/#PosteriorStats.ModelComparisonResult","content":" PosteriorStats.ModelComparisonResult  —  Type ModelComparisonResult Result of model comparison using ELPD. This struct implements the Tables and TableTraits interfaces. Each field returns a collection of the corresponding entry for each model: name : Names of the models, if provided. rank : Ranks of the models (ordered by decreasing ELPD) elpd_diff : ELPD of a model subtracted from the largest ELPD of any model elpd_diff_mcse : Monte Carlo standard error of the ELPD difference weight : Model weights computed with  weights_method elpd_result :  AbstactELPDResult s for each model, which can be used to access useful stats like ELPD estimates, pointwise estimates, and Pareto shape values for PSIS-LOO weights_method : Method used to compute model weights with  model_weights source"},{"id":94,"pagetitle":"Stats","title":"PosteriorStats.compare","ref":"/ArviZ/stable/api/stats/#PosteriorStats.compare","content":" PosteriorStats.compare  —  Function compare(models; kwargs...) -> ModelComparisonResult Compare models based on their expected log pointwise predictive density (ELPD). The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353 Arguments models : a  Tuple ,  NamedTuple , or  AbstractVector  whose values are either  AbstractELPDResult  entries or any argument to  elpd_method . Keywords weights_method::AbstractModelWeightsMethod=Stacking() : the method to be used to weight the models. See  model_weights  for details elpd_method=loo : a method that computes an  AbstractELPDResult  from an argument in  models . sort::Bool=true : Whether to sort models by decreasing ELPD. Returns ModelComparisonResult : A container for the model comparison results. The fields contain a similar collection to  models . Examples Compare the centered and non centered models of the eight school problem using the defaults:  loo  and  Stacking  weights. A custom  myloo  method formates the inputs as expected by  loo . julia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> function myloo(idata)\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n\njulia> mc = compare(models; elpd_method=myloo)\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\nModelComparisonResult with Stacking weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0       1.0  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067     0.0  0.9   ⋯\n                                                                1 column omitted\njulia> mc.weight |> pairs\npairs(::NamedTuple) with 2 entries:\n  :non_centered => 1.0\n  :centered     => 5.34175e-19 Compare the same models from pre-computed PSIS-LOO results and computing  BootstrappedPseudoBMA  weights: julia> elpd_results = mc.elpd_result;\n\njulia> compare(elpd_results; weights_method=BootstrappedPseudoBMA())\nModelComparisonResult with BootstrappedPseudoBMA weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0      0.52  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067    0.48  0.9   ⋯\n                                                                1 column omitted source"},{"id":95,"pagetitle":"Stats","title":"PosteriorStats.model_weights","ref":"/ArviZ/stable/api/stats/#PosteriorStats.model_weights","content":" PosteriorStats.model_weights  —  Function model_weights(elpd_results; method=Stacking())\nmodel_weights(method::AbstractModelWeightsMethod, elpd_results) Compute weights for each model in  elpd_results  using  method . elpd_results  is a  Tuple ,  NamedTuple , or  AbstractVector  with  AbstractELPDResult  entries. The weights are returned in the same type of collection. Stacking  is the recommended approach, as it performs well even when the true data generating process is not included among the candidate models. See  [YaoVehtari2018]  for details. See also:  AbstractModelWeightsMethod ,  compare Examples Compute  Stacking  weights for two models: julia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> elpd_results = map(models) do idata\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\n\njulia> model_weights(elpd_results; method=Stacking()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 5.34175e-19\n  :non_centered => 1.0 Now we compute  BootstrappedPseudoBMA  weights for the same models: julia> model_weights(elpd_results; method=BootstrappedPseudoBMA()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 0.483723\n  :non_centered => 0.516277 source The following model weighting methods are available"},{"id":96,"pagetitle":"Stats","title":"PosteriorStats.AbstractModelWeightsMethod","ref":"/ArviZ/stable/api/stats/#PosteriorStats.AbstractModelWeightsMethod","content":" PosteriorStats.AbstractModelWeightsMethod  —  Type abstract type AbstractModelWeightsMethod An abstract type representing methods for computing model weights. Subtypes implement  model_weights (method, elpd_results) . source"},{"id":97,"pagetitle":"Stats","title":"PosteriorStats.BootstrappedPseudoBMA","ref":"/ArviZ/stable/api/stats/#PosteriorStats.BootstrappedPseudoBMA","content":" PosteriorStats.BootstrappedPseudoBMA  —  Type struct BootstrappedPseudoBMA{R<:Random.AbstractRNG, T<:Real} <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging using Akaike-type weighting with the Bayesian bootstrap (pseudo-BMA+) [YaoVehtari2018] . The Bayesian bootstrap stabilizes the model weights. BootstrappedPseudoBMA(; rng=Random.default_rng(), samples=1_000, alpha=1)\nBootstrappedPseudoBMA(rng, samples, alpha) Construct the method. rng::Random.AbstractRNG : The random number generator to use for the Bayesian bootstrap samples::Int64 : The number of samples to draw for bootstrapping alpha::Real : The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. The default (1) corresponds to a uniform distribution on the simplex. See also:  Stacking source"},{"id":98,"pagetitle":"Stats","title":"PosteriorStats.PseudoBMA","ref":"/ArviZ/stable/api/stats/#PosteriorStats.PseudoBMA","content":" PosteriorStats.PseudoBMA  —  Type struct PseudoBMA <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging (pseudo-BMA) and Akaike-type weighting. PseudoBMA(; regularize=false)\nPseudoBMA(regularize) Construct the method with optional regularization of the weights using the standard error of the ELPD estimate. Note This approach is not recommended, as it produces unstable weight estimates. It is recommended to instead use  BootstrappedPseudoBMA  to stabilize the weights or  Stacking . For details, see  [YaoVehtari2018] . See also:  Stacking source"},{"id":99,"pagetitle":"Stats","title":"PosteriorStats.Stacking","ref":"/ArviZ/stable/api/stats/#PosteriorStats.Stacking","content":" PosteriorStats.Stacking  —  Type struct Stacking{O<:Optim.AbstractOptimizer} <: AbstractModelWeightsMethod Model weighting using stacking of predictive distributions [YaoVehtari2018] . Stacking(; optimizer=Optim.LBFGS(), options=Optim.Options()\nStacking(optimizer[, options]) Construct the method, optionally customizing the optimization. optimizer::Optim.AbstractOptimizer : The optimizer to use for the optimization of the weights. The optimizer must support projected gradient optimization via a  manifold  field. options::Optim.Options : The Optim options to use for the optimization of the weights. See also:  BootstrappedPseudoBMA source"},{"id":100,"pagetitle":"Stats","title":"Predictive checks","ref":"/ArviZ/stable/api/stats/#Predictive-checks","content":" Predictive checks"},{"id":101,"pagetitle":"Stats","title":"PosteriorStats.loo_pit","ref":"/ArviZ/stable/api/stats/#PosteriorStats.loo_pit","content":" PosteriorStats.loo_pit  —  Function loo_pit(y, y_pred, log_weights; kwargs...) -> Union{Real,AbstractArray} Compute leave-one-out probability integral transform (LOO-PIT) checks. Arguments y : array of observations with shape  (params...,) y_pred : array of posterior predictive samples with shape  (draws, chains, params...) . log_weights : array of normalized log LOO importance weights with shape  (draws, chains, params...) . Keywords is_discrete : If not provided, then it is set to  true  iff elements of  y  and  y_pred  are all integer-valued. If  true , then data are smoothed using  smooth_data  to make them non-discrete before estimating LOO-PIT values. kwargs : Remaining keywords are forwarded to  smooth_data  if data is discrete. Returns pitvals : LOO-PIT values with same size as  y . If  y  is a scalar, then  pitvals  is a scalar. LOO-PIT is a marginal posterior predictive check. If  $y_{-i}$  is the array  $y$  of observations with the  $i$ th observation left out, and  $y_i^*$  is a posterior prediction of the  $i$ th observation, then the LOO-PIT value for the  $i$ th observation is defined as \\[P(y_i^* \\le y_i \\mid y_{-i}) = \\int_{-\\infty}^{y_i} p(y_i^* \\mid y_{-i}) \\mathrm{d} y_i^*\\] The LOO posterior predictions and the corresponding observations should have similar distributions, so if conditional predictive distributions are well-calibrated, then all LOO-PIT values should be approximately uniformly distributed on  $[0, 1]$ . [Gabry2019] Examples Calculate LOO-PIT values using as test quantity the observed values themselves. julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> y = idata.observed_data.obs;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.obs, (:draw, :chain, :school));\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> log_weights = loo(log_like).psis_result.log_weights;\n\njulia> loo_pit(y, y_pred, log_weights)\n╭───────────────────────────────╮\n│ 8-element DimArray{Float64,1} │\n├───────────────────────────────┴──────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n└──────────────────────────────────────────────────────────────────────────────┘\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 Calculate LOO-PIT values using as test quantity the square of the difference between each observation and  mu . julia> using Statistics\n\njulia> mu = idata.posterior.mu;\n\njulia> T = y .- median(mu);\n\njulia> T_pred = y_pred .- mu;\n\njulia> loo_pit(T .^ 2, T_pred .^ 2, log_weights)\n╭───────────────────────────────╮\n│ 8-element DimArray{Float64,1} │\n├───────────────────────────────┴──────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n└──────────────────────────────────────────────────────────────────────────────┘\n \"Choate\"            0.873577\n \"Deerfield\"         0.243686\n \"Phillips Andover\"  0.357563\n \"Phillips Exeter\"   0.149908\n \"Hotchkiss\"         0.435094\n \"Lawrenceville\"     0.220627\n \"St. Paul's\"        0.775086\n \"Mt. Hermon\"        0.296706 source loo_pit(idata::InferenceData, log_weights; kwargs...) -> DimArray Compute LOO-PIT values using existing normalized log LOO importance weights. Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. kwargs : Remaining keywords are forwarded to the base method of  loo_pit . Examples Calculate LOO-PIT values using already computed log weights. julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo_result = loo(idata; var_name=:obs);\n\njulia> loo_pit(idata, loo_result.psis_result.log_weights; y_name=:obs)\n╭───────────────────────────────────────────╮\n│ 8-element DimArray{Float64,1} loo_pit_obs │\n├───────────────────────────────────────────┴──────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n└──────────────────────────────────────────────────────────────────────────────┘\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 source loo_pit(idata::InferenceData; kwargs...) -> DimArray Compute LOO-PIT from groups in  idata  using PSIS-LOO. Keywords y_name : Name of observed data variable in  idata.observed_data . If not provided, then the only observed data variable is used. y_pred_name : Name of posterior predictive variable in  idata.posterior_predictive . If not provided, then  y_name  is used. log_likelihood_name : Name of log-likelihood variable in  idata.log_likelihood . If not provided, then  y_name  is used if  idata  has a  log_likelihood  group, otherwise the only variable is used. reff::Union{Real,AbstractArray{<:Real}} : The relative effective sample size(s) of the  likelihood  values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using  ess . kwargs : Remaining keywords are forwarded to the base method of  loo_pit . Examples Calculate LOO-PIT values using as test quantity the observed values themselves. julia> using ArviZExampleData, PosteriorStats\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> loo_pit(idata; y_name=:obs)\n╭───────────────────────────────────────────╮\n│ 8-element DimArray{Float64,1} loo_pit_obs │\n├───────────────────────────────────────────┴──────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n└──────────────────────────────────────────────────────────────────────────────┘\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 source"},{"id":102,"pagetitle":"Stats","title":"Utilities","ref":"/ArviZ/stable/api/stats/#Utilities","content":" Utilities"},{"id":103,"pagetitle":"Stats","title":"PosteriorStats.smooth_data","ref":"/ArviZ/stable/api/stats/#PosteriorStats.smooth_data","content":" PosteriorStats.smooth_data  —  Function smooth_data(y; dims=:, interp_method=CubicSpline, offset_frac=0.01) Smooth  y  along  dims  using  interp_method . interp_method  is a 2-argument callabale that takes the arguments  y  and  x  and returns a DataInterpolations.jl interpolation method, defaulting to a cubic spline interpolator. offset_frac  is the fraction of the length of  y  to use as an offset when interpolating. source Hyndman1996 Rob J. Hyndman (1996) Computing and Graphing Highest Density Regions,             Amer. Stat., 50(2): 120-6.             DOI:  10.1080/00031305.1996.10474359 jstor . ChenShao1999 Ming-Hui Chen & Qi-Man Shao (1999)              Monte Carlo Estimation of Bayesian Credible and HPD Intervals,              J Comput. Graph. Stat., 8:1, 69-92.              DOI:  10.1080/10618600.1999.10474802 jstor . GelmanGoodrich2019 Andrew Gelman, Ben Goodrich, Jonah Gabry & Aki Vehtari (2019) R-squared for Bayesian Regression Models, The American Statistician, 73:3, 307-9, DOI:  10.1080/00031305.2018.1549100 . VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html Watanabe2010 Watanabe, S. Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. 11(116):3571−3594, 2010. https://jmlr.csail.mit.edu/papers/v11/watanabe10a.html Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 Gabry2019 Gabry, J., Simpson, D., Vehtari, A., Betancourt, M. & Gelman, A. Visualization in Bayesian Workflow. J. R. Stat. Soc. Ser. A Stat. Soc. 182, 389–402 (2019). doi:  10.1111/rssa.12378  arXiv:  1709.01449"},{"id":106,"pagetitle":"Creating custom plots","title":"Environment","ref":"/ArviZ/stable/creating_custom_plots/#Environment","content":" Environment using Pkg, InteractiveUtils using PlutoUI with_terminal(Pkg.status; color=false) Status `~/work/ArviZ.jl/ArviZ.jl/docs/Project.toml`\n  [cbdf2221] AlgebraOfGraphics v0.6.18\n  [131c737c] ArviZ v0.10.5 `~/work/ArviZ.jl/ArviZ.jl`\n  [2f96bb34] ArviZExampleData v0.1.9\n  [4a6e88f0] ArviZPythonPlots v0.1.4\n  [13f3f980] CairoMakie v0.11.9\n  [a93c6f00] DataFrames v1.6.1\n  [0703355e] DimensionalData v0.26.3\n  [31c24e10] Distributions v0.25.107\n  [e30172f5] Documenter v1.3.0\n  [f6006082] EvoTrees v0.16.6\n  [b5cf5a8d] InferenceObjects v0.3.16\n  [be115224] MCMCDiagnosticTools v0.3.10\n  [a7f614a8] MLJBase v1.1.2\n  [614be32b] MLJIteration v0.6.1\n  [ce719bf2] PSIS v0.9.5\n  [359b1769] PlutoStaticHTML v6.0.20\n  [7f904dfe] PlutoUI v0.7.58\n  [7f36be82] PosteriorStats v0.2.2\n  [c1514b29] StanSample v7.8.0\n  [a19d573c] StatisticalMeasures v0.1.6\n  [2913bbd2] StatsBase v0.34.2\n  [fce5fe82] Turing v0.30.5\n  [f43a241f] Downloads v1.6.0\n  [37e2e46d] LinearAlgebra\n  [10745b16] Statistics v1.10.0\n with_terminal(versioninfo) Julia Version 1.10.2\nCommit bd47eca2c8a (2024-03-01 10:14 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 4 × AMD EPYC 7763 64-Core Processor\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, znver3)\nThreads: 2 default, 0 interactive, 1 GC (on 4 virtual cores)\nEnvironment:\n  JULIA_PKG_SERVER_REGISTRY_PREFERENCE = eager\n  JULIA_NUM_THREADS = 2\n  JULIA_CMDSTAN_HOME = /home/runner/work/ArviZ.jl/ArviZ.jl/.cmdstan//cmdstan-2.33.1/\n  JULIA_REVISE_WORKER_ONLY = 1\n"},{"id":111,"pagetitle":"Working with InferenceData","title":"Working with  InferenceData","ref":"/ArviZ/stable/working_with_inference_data/#working-with-inference-data","content":" Working with  InferenceData using ArviZ, ArviZExampleData, DimensionalData, Statistics Here we present a collection of common manipulations you can use while working with  InferenceData . Let's load one of ArviZ's example datasets.  posterior ,  posterior_predictive , etc are the groups stored in  idata , and they are stored as  Dataset s. In this HTML view, you can click a group name to expand a summary of the group. idata = load_example_data(\"centered_eight\") InferenceData posterior ╭─────────────────╮\n│ 500×4×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 500×4\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×4\n  :tau   eltype: Float64 dims: draw, chain size: 500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n posterior_predictive ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n log_likelihood ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n sample_stats ╭───────────────╮\n│ 500×4 Dataset │\n├───────────────┴─────────────────────────────────────────────────────── dims ┐\n  ↓ draw  Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├─────────────────────────────────────────────────────────────────────────────┴ layers ┐\n  :max_energy_error    eltype: Float64 dims: draw, chain size: 500×4\n  :energy_error        eltype: Float64 dims: draw, chain size: 500×4\n  :lp                  eltype: Float64 dims: draw, chain size: 500×4\n  :index_in_trajectory eltype: Int64 dims: draw, chain size: 500×4\n  :acceptance_rate     eltype: Float64 dims: draw, chain size: 500×4\n  :diverging           eltype: Bool dims: draw, chain size: 500×4\n  :process_time_diff   eltype: Float64 dims: draw, chain size: 500×4\n  :n_steps             eltype: Float64 dims: draw, chain size: 500×4\n  :perf_counter_start  eltype: Float64 dims: draw, chain size: 500×4\n  :largest_eigval      eltype: Union{Missing, Float64} dims: draw, chain size: 500×4\n  :smallest_eigval     eltype: Union{Missing, Float64} dims: draw, chain size: 500×4\n  :step_size_bar       eltype: Float64 dims: draw, chain size: 500×4\n  :step_size           eltype: Float64 dims: draw, chain size: 500×4\n  :energy              eltype: Float64 dims: draw, chain size: 500×4\n  :tree_depth          eltype: Int64 dims: draw, chain size: 500×4\n  :perf_counter_diff   eltype: Float64 dims: draw, chain size: 500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior ╭─────────────────╮\n│ 500×1×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :tau   eltype: Float64 dims: draw, chain size: 500×1\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×1\n  :mu    eltype: Float64 dims: draw, chain size: 500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior_predictive ╭─────────────────╮\n│ 8×500×1 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n observed_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n constant_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :scores eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n Info Dataset s are  DimensionalData.AbstractDimStack s and can be used identically.   The variables a  Dataset  contains are called \"layers\", and dimensions of the same name that appear in more than one layer within a  Dataset  must have the same indices. InferenceData  behaves like a  NamedTuple  and can be used similarly. Note that unlike a  NamedTuple , the groups always appear in a specific order. length(idata) # number of groups 8 keys(idata) # group names (:posterior, :posterior_predictive, :log_likelihood, :sample_stats, :prior, :prior_predictive, :observed_data, :constant_data)"},{"id":112,"pagetitle":"Working with InferenceData","title":"Get the dataset corresponding to a single group","ref":"/ArviZ/stable/working_with_inference_data/#Get-the-dataset-corresponding-to-a-single-group","content":" Get the dataset corresponding to a single group Group datasets can be accessed both as properties or as indexed items. post = idata.posterior ╭─────────────────╮ \n │  500 × 4 × 8  Dataset  │ \n ├─────────────────┴────────────────────────────────────────────────────── dims ┐ \n   ↓ draw    Sampled{Int64}  [0, 1, …, 498, 499]   ForwardOrdered   Irregular   Points ,\n   → chain   Sampled{Int64}  [0, 1, 2, 3]   ForwardOrdered   Irregular   Points ,\n   ↗ school  Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered \n ├────────────────────────────────────────────────────────────────────── layers ┤ \n   :mu     eltype:  Float64  dims:  draw ,  chain  size:  500 × 4 \n   :theta  eltype:  Float64  dims:  school ,  draw ,  chain  size:  8 × 500 × 4 \n   :tau    eltype:  Float64  dims:  draw ,  chain  size:  500 × 4 \n ├──────────────────────────────────────────────────────────────────── metadata ┤ \n  Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\"\n post  is the dataset itself, so this is a non-allocating operation. idata[:posterior] === post true InferenceData  supports a more advanced indexing syntax, which we'll see later."},{"id":113,"pagetitle":"Working with InferenceData","title":"Getting a new  InferenceData  with a subset of groups","ref":"/ArviZ/stable/working_with_inference_data/#Getting-a-new-InferenceData-with-a-subset-of-groups","content":" Getting a new  InferenceData  with a subset of groups We can index by a collection of group names to get a new  InferenceData  with just those groups. This is also non-allocating. idata_sub = idata[(:posterior, :posterior_predictive)] InferenceData posterior ╭─────────────────╮\n│ 500×4×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 500×4\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×4\n  :tau   eltype: Float64 dims: draw, chain size: 500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n posterior_predictive ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n"},{"id":114,"pagetitle":"Working with InferenceData","title":"Adding groups to an  InferenceData","ref":"/ArviZ/stable/working_with_inference_data/#Adding-groups-to-an-InferenceData","content":" Adding groups to an  InferenceData InferenceData  is immutable, so to add or replace groups we use  merge  to create a new object. merge(idata_sub, idata[(:observed_data, :prior)]) InferenceData posterior ╭─────────────────╮\n│ 500×4×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 500×4\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×4\n  :tau   eltype: Float64 dims: draw, chain size: 500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n posterior_predictive ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior ╭─────────────────╮\n│ 500×1×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :tau   eltype: Float64 dims: draw, chain size: 500×1\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×1\n  :mu    eltype: Float64 dims: draw, chain size: 500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n observed_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n We can also use  Base.setindex  to out-of-place add or replace a single group. Base.setindex(idata_sub, idata.prior, :prior) InferenceData posterior ╭─────────────────╮\n│ 500×4×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 500×4\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×4\n  :tau   eltype: Float64 dims: draw, chain size: 500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n posterior_predictive ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior ╭─────────────────╮\n│ 500×1×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :tau   eltype: Float64 dims: draw, chain size: 500×1\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×1\n  :mu    eltype: Float64 dims: draw, chain size: 500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n"},{"id":115,"pagetitle":"Working with InferenceData","title":"Add a new variable","ref":"/ArviZ/stable/working_with_inference_data/#Add-a-new-variable","content":" Add a new variable Dataset  is also immutable. So while the values within the underlying data arrays can be mutated, layers cannot be added or removed from  Dataset s, and groups cannot be added/removed from  InferenceData . Instead, we do this out-of-place also using  merge . merge(post, (log_tau=log.(post[:tau]),)) ╭─────────────────╮ \n │  500 × 4 × 8  Dataset  │ \n ├─────────────────┴────────────────────────────────────────────────────── dims ┐ \n   ↓ draw    Sampled{Int64}  [0, 1, …, 498, 499]   ForwardOrdered   Irregular   Points ,\n   → chain   Sampled{Int64}  [0, 1, 2, 3]   ForwardOrdered   Irregular   Points ,\n   ↗ school  Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered \n ├────────────────────────────────────────────────────────────────────── layers ┤ \n   :mu       eltype:  Float64  dims:  draw ,  chain  size:  500 × 4 \n   :theta    eltype:  Float64  dims:  school ,  draw ,  chain  size:  8 × 500 × 4 \n   :tau      eltype:  Float64  dims:  draw ,  chain  size:  500 × 4 \n   :log_tau  eltype:  Float64  dims:  draw ,  chain  size:  500 × 4 \n ├──────────────────────────────────────────────────────────────────── metadata ┤ \n  Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\"\n"},{"id":116,"pagetitle":"Working with InferenceData","title":"Obtain an array for a given parameter","ref":"/ArviZ/stable/working_with_inference_data/#Obtain-an-array-for-a-given-parameter","content":" Obtain an array for a given parameter Let’s say we want to get the values for  mu  as an array. Parameters can be accessed with either property or index syntax. post.tau ╭───────────────────────────────╮ \n │  500 × 4  DimArray{Float64,2}  tau  │ \n ├───────────────────────────────┴─────────────────────────────────────── dims ┐ \n   ↓ draw   Sampled{Int64}  [0, 1, …, 498, 499]   ForwardOrdered   Irregular   Points ,\n   → chain  Sampled{Int64}  [0, 1, 2, 3]   ForwardOrdered   Irregular   Points \n └─────────────────────────────────────────────────────────────────────────────┘ \n    ↓   →    0          1           2          3 \n    0     4.72574  1.97083   3.50128  6.07326\n    1     3.90899  2.04903   2.89324  3.77187\n    2     4.84403  2.12376   4.27329  3.17054\n    3     1.8567   3.39183  11.8965   6.00193\n   ⋮                                \n  495     7.56498  1.61268   3.56495  2.78607\n  496     2.24702  1.84816   2.55959  4.28196\n  497     1.89384  2.17459   4.08978  2.74061\n  498     5.92006  1.32755   2.72017  2.93238\n  499     4.3259   1.21199   1.91701  4.46125 post[:tau] === post.tau true To remove the dimensions, just use  parent  to retrieve the underlying array. parent(post.tau) 500×4 Matrix{Float64}:\n 4.72574   1.97083   3.50128  6.07326\n 3.90899   2.04903   2.89324  3.77187\n 4.84403   2.12376   4.27329  3.17054\n 1.8567    3.39183  11.8965   6.00193\n 4.74841   4.84368   7.11325  3.28632\n 3.51387  10.8872    7.18892  2.16314\n 4.20898   4.01889   9.0977   7.68505\n 2.6834    4.28584   7.84286  4.08612\n 1.16889   3.70403  17.1548   5.1157\n 1.21052   3.15829  16.7573   4.86939\n ⋮                            \n 2.05742   1.09087  10.8168   5.08507\n 2.72536   1.09087   2.16788  6.1552\n 5.97049   1.67101   5.19169  8.23756\n 8.15827   1.61268   4.96249  3.13966\n 7.56498   1.61268   3.56495  2.78607\n 2.24702   1.84816   2.55959  4.28196\n 1.89384   2.17459   4.08978  2.74061\n 5.92006   1.32755   2.72017  2.93238\n 4.3259    1.21199   1.91701  4.46125"},{"id":117,"pagetitle":"Working with InferenceData","title":"Get the dimension lengths","ref":"/ArviZ/stable/working_with_inference_data/#Get-the-dimension-lengths","content":" Get the dimension lengths Let’s check how many groups are in our hierarchical model. size(idata.observed_data, :school) 8"},{"id":118,"pagetitle":"Working with InferenceData","title":"Get coordinate/index values","ref":"/ArviZ/stable/working_with_inference_data/#Get-coordinate/index-values","content":" Get coordinate/index values What are the names of the groups in our hierarchical model? You can access them from the coordinate name  school  in this case. DimensionalData.index(idata.observed_data, :school) 8-element Vector{String}:\n \"Choate\"\n \"Deerfield\"\n \"Phillips Andover\"\n \"Phillips Exeter\"\n \"Hotchkiss\"\n \"Lawrenceville\"\n \"St. Paul's\"\n \"Mt. Hermon\""},{"id":119,"pagetitle":"Working with InferenceData","title":"Get a subset of chains","ref":"/ArviZ/stable/working_with_inference_data/#Get-a-subset-of-chains","content":" Get a subset of chains Let’s keep only chain 0 here. For the subset to take effect on all relevant  InferenceData  groups –  posterior ,  sample_stats ,  log_likelihood , and  posterior_predictive  – we will index  InferenceData  instead of  Dataset . Here we use DimensionalData's  At  selector. Its  other selectors  are also supported. idata[chain=At(0)] InferenceData posterior ╭─────────────────╮\n│ 500×1×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 500×1\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×1\n  :tau   eltype: Float64 dims: draw, chain size: 500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n posterior_predictive ╭─────────────────╮\n│ 8×500×1 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n log_likelihood ╭─────────────────╮\n│ 8×500×1 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n sample_stats ╭───────────────╮\n│ 500×1 Dataset │\n├───────────────┴─────────────────────────────────────────────────────── dims ┐\n  ↓ draw  Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain Sampled{Int64} [0] ForwardOrdered Irregular Points\n├─────────────────────────────────────────────────────────────────────────────┴ layers ┐\n  :max_energy_error    eltype: Float64 dims: draw, chain size: 500×1\n  :energy_error        eltype: Float64 dims: draw, chain size: 500×1\n  :lp                  eltype: Float64 dims: draw, chain size: 500×1\n  :index_in_trajectory eltype: Int64 dims: draw, chain size: 500×1\n  :acceptance_rate     eltype: Float64 dims: draw, chain size: 500×1\n  :diverging           eltype: Bool dims: draw, chain size: 500×1\n  :process_time_diff   eltype: Float64 dims: draw, chain size: 500×1\n  :n_steps             eltype: Float64 dims: draw, chain size: 500×1\n  :perf_counter_start  eltype: Float64 dims: draw, chain size: 500×1\n  :largest_eigval      eltype: Union{Missing, Float64} dims: draw, chain size: 500×1\n  :smallest_eigval     eltype: Union{Missing, Float64} dims: draw, chain size: 500×1\n  :step_size_bar       eltype: Float64 dims: draw, chain size: 500×1\n  :step_size           eltype: Float64 dims: draw, chain size: 500×1\n  :energy              eltype: Float64 dims: draw, chain size: 500×1\n  :tree_depth          eltype: Int64 dims: draw, chain size: 500×1\n  :perf_counter_diff   eltype: Float64 dims: draw, chain size: 500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior ╭─────────────────╮\n│ 500×1×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :tau   eltype: Float64 dims: draw, chain size: 500×1\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×1\n  :mu    eltype: Float64 dims: draw, chain size: 500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior_predictive ╭─────────────────╮\n│ 8×500×1 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n observed_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n constant_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :scores eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n Note that in this case,  prior  only has a chain of 0. If it also had the other chains, we could have passed  chain=At([0, 2])  to subset by chains 0 and 2. Warning If we used  idata[chain=[0, 2]]  without the  At  selector, this is equivalent to  idata[chain=DimensionalData.index(idata.posterior, :chain)[0, 2]] , that is,  [0, 2]  indexes an array of dimension indices, which here would error.   But if we had requested  idata[chain=[1, 2]]  we would not hit an error, but we would index the wrong chains.   So it's important to always use a selector to index by values of dimension indices."},{"id":120,"pagetitle":"Working with InferenceData","title":"Remove the first  $n$  draws (burn-in)","ref":"/ArviZ/stable/working_with_inference_data/#Remove-the-first-n-draws-(burn-in)","content":" Remove the first  $n$  draws (burn-in) Let’s say we want to remove the first 100 draws from all the chains and all  InferenceData  groups with draws. To do this we use the  ..  syntax from IntervalSets.jl, which is exported by DimensionalData. idata[draw=100 .. Inf] InferenceData posterior ╭─────────────────╮\n│ 400×4×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 400×4\n  :theta eltype: Float64 dims: school, draw, chain size: 8×400×4\n  :tau   eltype: Float64 dims: draw, chain size: 400×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n posterior_predictive ╭─────────────────╮\n│ 8×400×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×400×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n log_likelihood ╭─────────────────╮\n│ 8×400×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×400×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n sample_stats ╭───────────────╮\n│ 400×4 Dataset │\n├───────────────┴──────────────────────────────────────────────────────── dims ┐\n  ↓ draw  Sampled{Int64} [100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :max_energy_error    eltype: Float64 dims: draw, chain size: 400×4\n  :energy_error        eltype: Float64 dims: draw, chain size: 400×4\n  :lp                  eltype: Float64 dims: draw, chain size: 400×4\n  :index_in_trajectory eltype: Int64 dims: draw, chain size: 400×4\n  :acceptance_rate     eltype: Float64 dims: draw, chain size: 400×4\n  :diverging           eltype: Bool dims: draw, chain size: 400×4\n  :process_time_diff   eltype: Float64 dims: draw, chain size: 400×4\n  :n_steps             eltype: Float64 dims: draw, chain size: 400×4\n  :perf_counter_start  eltype: Float64 dims: draw, chain size: 400×4\n  :largest_eigval      eltype: Union{Missing, Float64} dims: draw, chain size: 400×4\n  :smallest_eigval     eltype: Union{Missing, Float64} dims: draw, chain size: 400×4\n  :step_size_bar       eltype: Float64 dims: draw, chain size: 400×4\n  :step_size           eltype: Float64 dims: draw, chain size: 400×4\n  :energy              eltype: Float64 dims: draw, chain size: 400×4\n  :tree_depth          eltype: Int64 dims: draw, chain size: 400×4\n  :perf_counter_diff   eltype: Float64 dims: draw, chain size: 400×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior ╭─────────────────╮\n│ 400×1×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :tau   eltype: Float64 dims: draw, chain size: 400×1\n  :theta eltype: Float64 dims: school, draw, chain size: 8×400×1\n  :mu    eltype: Float64 dims: draw, chain size: 400×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior_predictive ╭─────────────────╮\n│ 8×400×1 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×400×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n observed_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n constant_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :scores eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n If you check the object you will see that the groups  posterior ,  posterior_predictive ,  prior , and  sample_stats  have 400 draws compared to  idata , which has 500. The group  observed_data  has not been affected because it does not have the  draw  dimension. Alternatively, you can change a subset of groups by combining indexing styles with  merge . Here we use this to build a new  InferenceData  where we have discarded the first 100 draws only from  posterior . merge(idata, idata[(:posterior,), draw=100 .. Inf]) InferenceData posterior ╭─────────────────╮\n│ 400×4×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [100, 101, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 400×4\n  :theta eltype: Float64 dims: school, draw, chain size: 8×400×4\n  :tau   eltype: Float64 dims: draw, chain size: 400×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n posterior_predictive ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n log_likelihood ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n sample_stats ╭───────────────╮\n│ 500×4 Dataset │\n├───────────────┴─────────────────────────────────────────────────────── dims ┐\n  ↓ draw  Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├─────────────────────────────────────────────────────────────────────────────┴ layers ┐\n  :max_energy_error    eltype: Float64 dims: draw, chain size: 500×4\n  :energy_error        eltype: Float64 dims: draw, chain size: 500×4\n  :lp                  eltype: Float64 dims: draw, chain size: 500×4\n  :index_in_trajectory eltype: Int64 dims: draw, chain size: 500×4\n  :acceptance_rate     eltype: Float64 dims: draw, chain size: 500×4\n  :diverging           eltype: Bool dims: draw, chain size: 500×4\n  :process_time_diff   eltype: Float64 dims: draw, chain size: 500×4\n  :n_steps             eltype: Float64 dims: draw, chain size: 500×4\n  :perf_counter_start  eltype: Float64 dims: draw, chain size: 500×4\n  :largest_eigval      eltype: Union{Missing, Float64} dims: draw, chain size: 500×4\n  :smallest_eigval     eltype: Union{Missing, Float64} dims: draw, chain size: 500×4\n  :step_size_bar       eltype: Float64 dims: draw, chain size: 500×4\n  :step_size           eltype: Float64 dims: draw, chain size: 500×4\n  :energy              eltype: Float64 dims: draw, chain size: 500×4\n  :tree_depth          eltype: Int64 dims: draw, chain size: 500×4\n  :perf_counter_diff   eltype: Float64 dims: draw, chain size: 500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior ╭─────────────────╮\n│ 500×1×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :tau   eltype: Float64 dims: draw, chain size: 500×1\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×1\n  :mu    eltype: Float64 dims: draw, chain size: 500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior_predictive ╭─────────────────╮\n│ 8×500×1 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n observed_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n constant_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :scores eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n"},{"id":121,"pagetitle":"Working with InferenceData","title":"Compute posterior mean values along draw and chain dimensions","ref":"/ArviZ/stable/working_with_inference_data/#Compute-posterior-mean-values-along-draw-and-chain-dimensions","content":" Compute posterior mean values along draw and chain dimensions To compute the mean value of the posterior samples, do the following: mean(post) (mu = 4.485933103402338,\n theta = 4.911515591394205,\n tau = 4.124222787491913,) This computes the mean along all dimensions, discarding all dimensions and returning the result as a  NamedTuple . This may be what you wanted for  mu  and  tau , which have only two dimensions ( chain  and  draw ), but maybe not what you expected for  theta , which has one more dimension  school . You can specify along which dimension you want to compute the mean (or other functions), which instead returns a  Dataset . mean(post; dims=(:chain, :draw)) ╭───────────────╮ \n │  1 × 1 × 8  Dataset  │ \n ├───────────────┴──────────────────────────────────────────────────────── dims ┐ \n   ↓ draw    Sampled{Float64}  [249.5]   ForwardOrdered   Irregular   Points ,\n   → chain   Sampled{Float64}  [1.5]   ForwardOrdered   Irregular   Points ,\n   ↗ school  Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered \n ├────────────────────────────────────────────────────────────────────── layers ┤ \n   :mu     eltype:  Float64  dims:  draw ,  chain  size:  1 × 1 \n   :theta  eltype:  Float64  dims:  school ,  draw ,  chain  size:  8 × 1 × 1 \n   :tau    eltype:  Float64  dims:  draw ,  chain  size:  1 × 1 \n ├──────────────────────────────────────────────────────────────────── metadata ┤ \n  Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\"\n The singleton dimensions of  chain  and  draw  now contain meaningless indices, so you may want to discard them, which you can do with  dropdims . dropdims(mean(post; dims=(:chain, :draw)); dims=(:chain, :draw)) ╭───────────────────╮ \n │  8-element  Dataset  │ \n ├───────────────────┴──────────────────────────────────────────────────── dims ┐ \n   ↓ school  Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered \n ├────────────────────────────────────────────────────────────────────── layers ┤ \n   :mu     eltype:  Float64  dims:  \n   :theta  eltype:  Float64  dims:  school  size:  8 \n   :tau    eltype:  Float64  dims:  \n ├──────────────────────────────────────────────────────────────────── metadata ┤ \n  Dict{String, Any} with 6 entries:\n  \"created_at\"                => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\"             => 7.48011\n  \"tuning_steps\"              => 1000\n  \"arviz_version\"             => \"0.13.0.dev0\"\n  \"inference_library\"         => \"pymc\"\n"},{"id":122,"pagetitle":"Working with InferenceData","title":"Renaming a dimension","ref":"/ArviZ/stable/working_with_inference_data/#Renaming-a-dimension","content":" Renaming a dimension We can rename a dimension in a  Dataset  using DimensionalData's  set  method: theta_bis = set(post.theta; school=:school_bis) ╭───────────────────────────────────╮ \n │  8 × 500 × 4  DimArray{Float64,3}  theta  │ \n ├───────────────────────────────────┴──────────────────────────────────── dims ┐ \n   ↓ school_bis  Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered ,\n   → draw        Sampled{Int64}  [0, 1, …, 498, 499]   ForwardOrdered   Irregular   Points ,\n   ↗ chain       Sampled{Int64}  [0, 1, 2, 3]   ForwardOrdered   Irregular   Points \n └──────────────────────────────────────────────────────────────────────────────┘ \n[ : ,  : ,  1 ]\n  ↓   →                     0         …   497           498          499 \n   \"Choate\"             12.3207       -0.213828   10.4025     6.66131\n   \"Deerfield\"           9.90537       1.35515     6.90741    7.41377\n   \"Phillips Andover\"   14.9516        6.98269    -4.96414   -9.3226\n   \"Phillips Exeter\"    11.0115        3.71681     3.13584    2.69192\n   \"Hotchkiss\"           5.5796   …    5.32446    -2.2243    -0.502331\n   \"Lawrenceville\"      16.9018        6.96589    -2.83504   -4.25487\n   \"St. Paul's\"         13.1981        4.9302      5.39106    7.56657\n   \"Mt. Hermon\"         15.0614        3.0586      6.38124    9.98762 We can use this, for example, to broadcast functions across multiple arrays, automatically matching up shared dimensions, using  DimensionalData.broadcast_dims . theta_school_diff = broadcast_dims(-, post.theta, theta_bis) ╭─────────────────────────────────────╮ \n │  8 × 500 × 4 × 8  DimArray{Float64,4}  theta  │ \n ├─────────────────────────────────────┴────────────────────────────────── dims ┐ \n   ↓ school      Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered ,\n   → draw        Sampled{Int64}  [0, 1, …, 498, 499]   ForwardOrdered   Irregular   Points ,\n   ↗ chain       Sampled{Int64}  [0, 1, 2, 3]   ForwardOrdered   Irregular   Points ,\n   ⬔ school_bis  Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered \n └──────────────────────────────────────────────────────────────────────────────┘ \n[ : ,  : ,  1 ,  1 ]\n  ↓   →                     0          …   497          498          499 \n   \"Choate\"              0.0            0.0        0.0        0.0\n   \"Deerfield\"          -2.41532        1.56898   -3.49509    0.752459\n   \"Phillips Andover\"    2.63093        7.19652  -15.3666   -15.9839\n   \"Phillips Exeter\"    -1.3092         3.93064   -7.26666   -3.96939\n   \"Hotchkiss\"          -6.74108   …    5.53829  -12.6268    -7.16364\n   \"Lawrenceville\"       4.58111        7.17972  -13.2375   -10.9162\n   \"St. Paul's\"          0.877374       5.14403   -5.01144    0.905263\n   \"Mt. Hermon\"          2.74068        3.27243   -4.02126    3.32631"},{"id":123,"pagetitle":"Working with InferenceData","title":"Compute and store posterior pushforward quantities","ref":"/ArviZ/stable/working_with_inference_data/#Compute-and-store-posterior-pushforward-quantities","content":" Compute and store posterior pushforward quantities We use “posterior pushfoward quantities” to refer to quantities that are not variables in the posterior but deterministic computations using posterior variables. You can compute these pushforward operations and store them as a new variable in a copy of the posterior group. Here we'll create a new  InferenceData  with  theta_school_diff  in the posterior: idata_new = Base.setindex(idata, merge(post, (; theta_school_diff)), :posterior) InferenceData posterior ╭───────────────────╮\n│ 500×4×8×8 Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ draw       Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain      Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points,\n  ↗ school     Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  ⬔ school_bis Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu                eltype: Float64 dims: draw, chain size: 500×4\n  :theta             eltype: Float64 dims: school, draw, chain size: 8×500×4\n  :tau               eltype: Float64 dims: draw, chain size: 500×4\n  :theta_school_diff eltype: Float64 dims: school, draw, chain, school_bis size: 8×500×4×8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n posterior_predictive ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:41.460544\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n log_likelihood ╭─────────────────╮\n│ 8×500×4 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.487399\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n sample_stats ╭───────────────╮\n│ 500×4 Dataset │\n├───────────────┴─────────────────────────────────────────────────────── dims ┐\n  ↓ draw  Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain Sampled{Int64} [0, 1, 2, 3] ForwardOrdered Irregular Points\n├─────────────────────────────────────────────────────────────────────────────┴ layers ┐\n  :max_energy_error    eltype: Float64 dims: draw, chain size: 500×4\n  :energy_error        eltype: Float64 dims: draw, chain size: 500×4\n  :lp                  eltype: Float64 dims: draw, chain size: 500×4\n  :index_in_trajectory eltype: Int64 dims: draw, chain size: 500×4\n  :acceptance_rate     eltype: Float64 dims: draw, chain size: 500×4\n  :diverging           eltype: Bool dims: draw, chain size: 500×4\n  :process_time_diff   eltype: Float64 dims: draw, chain size: 500×4\n  :n_steps             eltype: Float64 dims: draw, chain size: 500×4\n  :perf_counter_start  eltype: Float64 dims: draw, chain size: 500×4\n  :largest_eigval      eltype: Union{Missing, Float64} dims: draw, chain size: 500×4\n  :smallest_eigval     eltype: Union{Missing, Float64} dims: draw, chain size: 500×4\n  :step_size_bar       eltype: Float64 dims: draw, chain size: 500×4\n  :step_size           eltype: Float64 dims: draw, chain size: 500×4\n  :energy              eltype: Float64 dims: draw, chain size: 500×4\n  :tree_depth          eltype: Int64 dims: draw, chain size: 500×4\n  :perf_counter_diff   eltype: Float64 dims: draw, chain size: 500×4\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.324929\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior ╭─────────────────╮\n│ 500×1×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :tau   eltype: Float64 dims: draw, chain size: 500×1\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×1\n  :mu    eltype: Float64 dims: draw, chain size: 500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.602116\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n prior_predictive ╭─────────────────╮\n│ 8×500×1 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered,\n  → draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  ↗ chain  Sampled{Int64} [0] ForwardOrdered Irregular Points\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school, draw, chain size: 8×500×1\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.604969\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n observed_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :obs eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.606375\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n constant_data ╭───────────────────╮\n│ 8-element Dataset │\n├───────────────────┴──────────────────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :scores eltype: Float64 dims: school size: 8\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 4 entries:\n  \"created_at\" => \"2022-10-13T14:37:26.607471\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n Once you have these pushforward quantities in an  InferenceData , you’ll then be able to plot them with ArviZ functions, calculate stats and diagnostics on them, or save and share the  InferenceData  object with the pushforward quantities included. Here we compute the  mcse  of  theta_school_diff : mcse(idata_new.posterior).theta_school_diff ╭───────────────────────────────────────────╮ \n │  8 × 8  DimArray{Float64,2}  theta_school_diff  │ \n ├───────────────────────────────────────────┴──────────────────────────── dims ┐ \n   ↓ school      Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered ,\n   → school_bis  Categorical{String}  [Choate, Deerfield, …, St. Paul's, Mt. Hermon]   Unordered \n └──────────────────────────────────────────────────────────────────────────────┘ \n  ↓   →                       \"Choate\"   …      \"St. Paul's\"       \"Mt. Hermon\" \n   \"Choate\"             NaN               0.117476         0.219695\n   \"Deerfield\"            0.191463        0.16484          0.189386\n   \"Phillips Andover\"     0.255636        0.258001         0.160477\n   \"Phillips Exeter\"      0.162782        0.156724         0.144923\n   \"Hotchkiss\"            0.282881   …    0.283969         0.189015\n   \"Lawrenceville\"        0.259065        0.251988         0.178094\n   \"St. Paul's\"           0.117476      NaN                0.222054\n   \"Mt. Hermon\"           0.219695        0.222054       NaN"},{"id":124,"pagetitle":"Working with InferenceData","title":"Advanced subsetting","ref":"/ArviZ/stable/working_with_inference_data/#Advanced-subsetting","content":" Advanced subsetting To select the value corresponding to the difference between the Choate and Deerfield schools do: school_idx = [\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"]\nschool_bis_idx = [\"Deerfield\", \"Choate\", \"Lawrenceville\"]\ntheta_school_diff[school=At(school_idx), school_bis=At(school_bis_idx)] ╭─────────────────────────────────────╮ \n │  3 × 500 × 4 × 3  DimArray{Float64,4}  theta  │ \n ├─────────────────────────────────────┴────────────────────────────────── dims ┐ \n   ↓ school      Categorical{String}  [\"Choate\", \"Hotchkiss\", \"Mt. Hermon\"]   Unordered ,\n   → draw        Sampled{Int64}  [0, 1, …, 498, 499]   ForwardOrdered   Irregular   Points ,\n   ↗ chain       Sampled{Int64}  [0, 1, 2, 3]   ForwardOrdered   Irregular   Points ,\n   ⬔ school_bis  Categorical{String}  [\"Deerfield\", \"Choate\", \"Lawrenceville\"]   Unordered \n └──────────────────────────────────────────────────────────────────────────────┘ \n[ : ,  : ,  1 ,  1 ]\n  ↓   →               0           1         …   497          498           499 \n   \"Choate\"        2.41532   2.1563       -1.56898    3.49509    -0.752459\n   \"Hotchkiss\"    -4.32577  -1.31781       3.96931   -9.13171    -7.9161\n   \"Mt. Hermon\"    5.156    -2.9526        1.70345   -0.526168    2.57385"},{"id":125,"pagetitle":"Working with InferenceData","title":"Add new chains using  cat","ref":"/ArviZ/stable/working_with_inference_data/#Add-new-chains-using-cat","content":" Add new chains using  cat Suppose after checking the  mcse  and realizing you need more samples, you rerun the model with two chains and obtain an  idata_rerun  object. idata_rerun = InferenceData(; posterior=set(post[chain=At([0, 1])]; chain=[4, 5])) InferenceData posterior ╭─────────────────╮\n│ 500×2×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [4, 5] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 500×2\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×2\n  :tau   eltype: Float64 dims: draw, chain size: 500×2\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n You can combine the two using  cat . cat(idata[[:posterior]], idata_rerun; dims=:chain) InferenceData posterior ╭─────────────────╮\n│ 500×6×8 Dataset │\n├─────────────────┴────────────────────────────────────────────────────── dims ┐\n  ↓ draw   Sampled{Int64} [0, 1, …, 498, 499] ForwardOrdered Irregular Points,\n  → chain  Sampled{Int64} [0, 1, …, 4, 5] ForwardOrdered Irregular Points,\n  ↗ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n├────────────────────────────────────────────────────────────────────── layers ┤\n  :mu    eltype: Float64 dims: draw, chain size: 500×6\n  :theta eltype: Float64 dims: school, draw, chain size: 8×500×6\n  :tau   eltype: Float64 dims: draw, chain size: 500×6\n├──────────────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 6 entries:\n  \"created_at\" => \"2022-10-13T14:37:37.315398\"\n  \"inference_library_version\" => \"4.2.2\"\n  \"sampling_time\" => 7.48011\n  \"tuning_steps\" => 1000\n  \"arviz_version\" => \"0.13.0.dev0\"\n  \"inference_library\" => \"pymc\"\n"},{"id":128,"pagetitle":"Home","title":"ArviZPythonPlots.jl","ref":"/ArviZPythonPlots/stable/#ArviZPythonPlots.jl","content":" ArviZPythonPlots.jl ArviZPythonPlots.jl provides PyPlot-compatible plotting functions for exploratory analysis of Bayesian models using  ArviZ.jl . It uses  PythonCall.jl  to provide an interface for using the plotting functions in  Python ArviZ  with Julia types. It also re-exports all methods exported by  PythonPlot.jl . For details, see the  Example Gallery  or the  API ."},{"id":129,"pagetitle":"Home","title":"Installation","ref":"/ArviZPythonPlots/stable/#installation","content":" Installation To install ArviZPythonPlots.jl, we first need to install Python ArviZ. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run pkg> add ArviZPythonPlots"},{"id":132,"pagetitle":"API Overview","title":"API Overview","ref":"/ArviZPythonPlots/stable/api/#api","content":" API Overview Plotting styles rcParams Plotting functions"},{"id":135,"pagetitle":"Plotting functions","title":"Plotting functions","ref":"/ArviZPythonPlots/stable/api/plots/#plots-api","content":" Plotting functions ArviZPythonPlots.plot_autocorr ArviZPythonPlots.plot_bf ArviZPythonPlots.plot_bpv ArviZPythonPlots.plot_compare ArviZPythonPlots.plot_density ArviZPythonPlots.plot_dist ArviZPythonPlots.plot_dist_comparison ArviZPythonPlots.plot_dot ArviZPythonPlots.plot_ecdf ArviZPythonPlots.plot_elpd ArviZPythonPlots.plot_energy ArviZPythonPlots.plot_ess ArviZPythonPlots.plot_forest ArviZPythonPlots.plot_hdi ArviZPythonPlots.plot_kde ArviZPythonPlots.plot_khat ArviZPythonPlots.plot_lm ArviZPythonPlots.plot_loo_pit ArviZPythonPlots.plot_mcse ArviZPythonPlots.plot_pair ArviZPythonPlots.plot_parallel ArviZPythonPlots.plot_posterior ArviZPythonPlots.plot_ppc ArviZPythonPlots.plot_rank ArviZPythonPlots.plot_separation ArviZPythonPlots.plot_trace ArviZPythonPlots.plot_violin"},{"id":136,"pagetitle":"Plotting functions","title":"Reference","ref":"/ArviZPythonPlots/stable/api/plots/#Reference","content":" Reference"},{"id":137,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_autocorr","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_autocorr-Tuple","content":" ArviZPythonPlots.plot_autocorr  —  Method Bar plot of the autocorrelation function (ACF) for a sequence of data.\n\n    The ACF plots are helpful as a convergence diagnostic for posteriors from MCMC\n    samples which display autocorrelation.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    max_lag : int, optional\n        Maximum lag to calculate autocorrelation. By Default, the plot displays the\n        first 100 lag or the total number of draws, whichever is smaller.\n    combined : bool, default False\n        Flag for combining multiple chains into a single chain. If False, chains will be\n        plotted separately.\n    grid : tuple, optional\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n        Note this is not used if `ax` is supplied.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_config : dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in ``rcParams``.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    autocov : Compute autocovariance estimates for every lag for the input array.\n    autocorr : Compute autocorrelation using FFT for every lag for the input array.\n\n    Examples\n    --------\n    Plot default autocorrelation\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_autocorr(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'] )\n\n\n    Combine chains by variable and select variables by excluding some with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['~thet'], filter_vars=\"like\", combined=True)\n\n\n    Specify maximum lag (x axis bound)\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'], max_lag=200, combined=True)\n    \n source"},{"id":138,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_bf","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_bf-Tuple","content":" ArviZPythonPlots.plot_bf  —  Method Approximated Bayes Factor for comparing hypothesis of two nested models.\n\n    The Bayes factor is estimated by comparing a model (H1) against a model in which the\n    parameter of interest has been restricted to be a point-null (H0). This computation\n    assumes the models are nested and thus H0 is a special case of H1.\n\n    Notes\n    -----\n    The bayes Factor is approximated pproximated as the Savage-Dickey density ratio\n    algorithm presented in [1]_.\n\n    Parameters\n    -----------\n    idata : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    var_name : str, optional\n        Name of variable we want to test.\n    prior : numpy.array, optional\n        In case we want to use different prior, for example for sensitivity analysis.\n    ref_val : int, default 0\n        Point-null for Bayes factor estimation.\n    xlim :  tuple, optional\n        Set the x limits, which might be used for visualization purposes.\n    colors : tuple, default ('C0', 'C1')\n        Tuple of valid Matplotlib colors. First element for the prior, second for the posterior.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    textsize: float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`.\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :func:`matplotlib.pyplot.plot`.\n    hist_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist`. Only works for discrete variables.\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend :{\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    dict : A dictionary with BF10 (Bayes Factor 10 (H1/H0 ratio), and BF01 (H0/H1 ratio).\n    axes : matplotlib_axes or bokeh_figure\n\n    References\n    ----------\n    .. [1] Heck, D., 2019. A caveat on the avage-Dickey density ratio:\n    The case of computing Bayes factors for regression parameters.\n\n    Examples\n    --------\n    Moderate evidence indicating that the parameter \"a\" is different from zero.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> idata = az.from_dict(posterior={\"a\":np.random.normal(1, 0.5, 5000)},\n        ...     prior={\"a\":np.random.normal(0, 1, 5000)})\n        >>> az.plot_bf(idata, var_name=\"a\", ref_val=0)\n    \n source"},{"id":139,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_bpv","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_bpv-Tuple","content":" ArviZPythonPlots.plot_bpv  —  Method Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n\n    Parameters\n    ----------\n    data : InferenceData\n        :class:`arviz.InferenceData` object containing the observed and\n        posterior/prior predictive data.\n    kind : {\"u_value\", \"p_value\", \"t_stat\"}, default \"u_value\"\n        Specify the kind of plot:\n\n        * The ``kind=\"p_value\"`` computes :math:`p := p(y* \\leq y | y)`.\n          This is the probability of the data y being larger or equal than the predicted data y*.\n          The ideal value is 0.5 (half the predictions below and half above the data).\n        * The ``kind=\"u_value\"`` argument computes :math:`p_i := p(y_i* \\leq y_i | y)`.\n          i.e. like a p_value but per observation :math:`y_i`. This is also known as marginal\n          p_value. The ideal distribution is uniform. This is similar to the LOO-PIT\n          calculation/plot, the difference is than in LOO-pit plot we compute\n          :math:`pi = p(y_i* r \\leq y_i | y_{-i} )`, where :math:`y_{-i}`,\n          is all other data except :math:`y_i`.\n        * The ``kind=\"t_stat\"`` argument computes :math:`:= p(T(y)* \\leq T(y) | y)`\n          where T is any test statistic. See ``t_stat`` argument below for details\n          of available options.\n\n    t_stat : str, float, or callable, default \"median\"\n        Test statistics to compute from the observations and predictive distributions.\n        Allowed strings are “mean”, “median” or “std”. Alternative a quantile can be passed\n        as a float (or str) in the interval (0, 1). Finally a user defined function is also\n        acepted, see examples section for details.\n    bpv : bool, default True\n        If True add the Bayesian p_value to the legend when ``kind = t_stat``.\n    plot_mean : bool, default True\n        Whether or not to plot the mean test statistic.\n    reference : {\"analytical\", \"samples\", None}, default \"analytical\"\n        How to compute the distributions used as reference for ``kind=u_values``\n        or ``kind=p_values``. Use `None` to not plot any reference.\n    mse : bool, default False\n        Show scaled mean square error between uniform distribution and marginal p_value\n        distribution.\n    n_ref : int, default 100\n        Number of reference distributions to sample when ``reference=samples``.\n    hdi_prob : float, optional\n        Probability for the highest density interval for the analytical reference distribution when\n        ``kind=u_values``. Should be in the interval (0, 1]. Defaults to the\n        rcParam ``stats.hdi_prob``. See :ref:`this section <common_hdi_prob>` for usage examples.\n    color : str, optional\n        Matplotlib color\n    grid : tuple, optional\n        Number of rows and columns. By default, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on `figsize`.\n    data_pairs : dict, optional\n        Dictionary containing relations between observed data and posterior/prior predictive data.\n        Dictionary structure:\n\n        - key = data var_name\n        - value = posterior/prior predictive var_name\n\n        For example, ``data_pairs = {'y' : 'y_hat'}``\n        If None, it will assume that the observed data and the posterior/prior\n        predictive data have the same variable name.\n    Labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    var_names : list of str, optional\n        Variables to be plotted. If `None` all variable are plotted. Prefix the variables by ``~``\n        when you want to exclude them from the plot. See the :ref:`this section <common_var_names>`\n        for usage examples. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    coords : dict, optional\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None. See :ref:`this section <common_coords>` for usage examples.\n    flatten : list, optional\n        List of dimensions to flatten in observed_data. Only flattens across the coordinates\n        specified in the coords argument. Defaults to flattening all of the dimensions.\n    flatten_pp : list, optional\n        List of dimensions to flatten in posterior_predictive/prior_predictive. Only flattens\n        across the coordinates specified in the coords argument. Defaults to flattening all\n        of the dimensions. Dimensions should match flatten excluding dimensions for data_pairs\n        parameters. If `flatten` is defined and `flatten_pp` is None, then ``flatten_pp=flatten``.\n    legend : bool, default True\n        Add legend to figure.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : str, optional\n        Select plotting backend {\"matplotlib\", \"bokeh\"}. Default \"matplotlib\".\n    plot_ref_kwargs :  dict, optional\n        Extra keyword arguments to control how reference is represented.\n        Passed to :meth:`matplotlib.axes.Axes.plot` or\n        :meth:`matplotlib.axes.Axes.axhspan` (when ``kind=u_value``\n        and ``reference=analytical``).\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    group : {\"posterior\", \"prior\"}, default \"posterior\"\n        Specifies which InferenceData group should be plotted. If \"posterior\", then the values\n        in `posterior_predictive` group are compared to the ones in `observed_data`, if \"prior\" then\n        the same comparison happens, but with the values in `prior_predictive` group.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_ppc : Plot for posterior/prior predictive checks.\n    plot_loo_pit : Plot Leave-One-Out probability integral transformation (PIT) predictive checks.\n    plot_dist_comparison : Plot to compare fitted and unfitted distributions.\n\n    References\n    ----------\n    * Gelman et al. (2013) see http://www.stat.columbia.edu/~gelman/book/ pages 151-153 for details\n\n    Examples\n    --------\n    Plot Bayesian p_values.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data(\"regression1d\")\n        >>> az.plot_bpv(data, kind=\"p_value\")\n\n    Plot custom test statistic comparison.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data(\"regression1d\")\n        >>> az.plot_bpv(data, kind=\"t_stat\", t_stat=lambda x:np.percentile(x, q=50, axis=-1))\n    \n source"},{"id":140,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_compare","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_compare-Tuple","content":" ArviZPythonPlots.plot_compare  —  Method Summary plot for model comparison.\n\n    Models are compared based on their expected log pointwise predictive density (ELPD).\n    This plot is in the style of the one used in [2]_. Chapter 6 in the first edition\n    or 7 in the second.\n\n    Notes\n    -----\n    The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend LOO in line with the work presented by [1]_.\n\n    Parameters\n    ----------\n    comp_df : pandas.DataFrame\n        Result of the :func:`arviz.compare` method.\n    insample_dev : bool, default False\n        Plot in-sample ELPD, that is the value of the information criteria without the\n        penalization given by the effective number of parameters (p_loo or p_waic).\n    plot_standard_error : bool, default True\n        Plot the standard error of the ELPD.\n    plot_ic_diff : bool, default True\n        Plot standard error of the difference in ELPD between each model\n        and the top-ranked model.\n    order_by_rank : bool, default True\n        If True ensure the best model is used as reference.\n    legend : bool, default True\n        Add legend to figure.\n    figsize : (float, float), optional\n        If `None`, size is (6, num of models) inches.\n    title : bool, default True\n        Show a tittle with a description of how to interpret the plot.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    plot_kwargs : dict, optional\n        Optional arguments for plot elements. Currently accepts 'color_ic',\n        'marker_ic', 'color_insample_dev', 'marker_insample_dev', 'color_dse',\n        'marker_dse', 'ls_min_ic' 'color_ls_min_ic',  'fontsize'\n    ax : matplotlib_axes or bokeh_figure, optional\n        Matplotlib axes or bokeh figure.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_elpd : Plot pointwise elpd differences between two or more models.\n    compare : Compare models based on PSIS-LOO loo or WAIC waic cross-validation.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n    waic : Compute the widely applicable information criterion.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2016). Practical Bayesian model evaluation using leave-one-out\n    cross-validation and WAIC https://arxiv.org/abs/1507.04544\n\n    .. [2] McElreath R. (2022). Statistical Rethinking A Bayesian Course with Examples in\n    R and Stan, Second edition, CRC Press.\n\n    Examples\n    --------\n    Show default compare plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> model_compare = az.compare({'Centered 8 schools': az.load_arviz_data('centered_eight'),\n        >>>                  'Non-centered 8 schools': az.load_arviz_data('non_centered_eight')})\n        >>> az.plot_compare(model_compare)\n\n    Include the in-sample ELDP\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_compare(model_compare, insample_dev=True)\n\n    \n source"},{"id":141,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_density","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_density-Tuple","content":" ArviZPythonPlots.plot_density  —  Method Generate KDE plots for continuous variables and histograms for discrete ones.\n\n    Plots are truncated at their 100*(1-alpha)% highest density intervals. Plots are grouped per\n    variable and colors assigned to models.\n\n    Parameters\n    ----------\n    data : InferenceData or iterable of InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object, or an Iterator\n        returning a sequence of such objects.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    group : {\"posterior\", \"prior\"}, default \"posterior\"\n        Specifies which InferenceData group should be plotted. If \"posterior\", then the values\n        in `posterior_predictive` group are compared to the ones in `observed_data`, if \"prior\" then\n        the same comparison happens, but with the values in `prior_predictive` group.\n    data_labels : list of str, default None\n        List with names for the datasets passed as \"data.\" Useful when plotting more than one\n        dataset.  Must be the same shape as the data parameter.\n    var_names : list of str, optional\n        List of variables to plot. If multiple datasets are supplied and `var_names` is not None,\n        will print the same set of variables for each dataset. Defaults to None, which results in\n        all the variables being plotted.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    transform : callable\n        Function to transform data (defaults to `None` i.e. the identity function).\n    hdi_prob : float, default 0.94\n        Probability for the highest density interval. Should be in the interval (0, 1].\n        See :ref:`this section <common_hdi_prob>` for usage examples.\n    point_estimate : str, optional\n        Plot point estimate per variable. Values should be 'mean', 'median', 'mode' or None.\n        Defaults to 'auto' i.e. it falls back to default set in ``rcParams``.\n    colors : str or list of str, optional\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplotlib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    outline : bool, default True\n        Use a line to draw KDEs and histograms.\n    hdi_markers : str\n        A valid `matplotlib.markers` like 'v', used to indicate the limits of the highest density\n        interval. Defaults to empty string (no marker).\n    shade : float, default 0\n        Alpha blending value for the shaded area under the curve, between 0 (no shade) and 1\n        (opaque).\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when `circular` is False\n        and \"taylor\" (for now) when `circular` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    circular : bool, default False\n        If True, it interprets the values passed are from a circular variable measured in radians\n        and a circular KDE is used. Only valid for 1D KDE.\n    grid : tuple, optional\n        Number of rows and columns. Defaults to ``None``, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n\n    Examples\n    --------\n    Plot default density plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> az.plot_density([centered, non_centered])\n\n    Plot variables in a 4x5 grid\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], grid=(4, 5))\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"])\n\n    Plot a specific `az.InferenceData` group\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], group=\"prior\")\n\n    Specify highest density interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], hdi_prob=.5)\n\n    Shade plots and/or remove outlines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], outline=False, shade=.8)\n\n    Specify binwidth for kernel density estimation\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], bw=.9)\n    \n source"},{"id":142,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_dist","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_dist-Tuple","content":" ArviZPythonPlots.plot_dist  —  Method Plot distribution as histogram or kernel density estimates.\n\n    By default continuous variables are plotted using KDEs and discrete ones using histograms\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot from an unknown continuous or discrete distribution.\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE or a hexbin will be estimated.\n    color : string\n        valid matplotlib color.\n    kind : string, default \"auto\"\n        By default (\"auto\") continuous variables will use the kind defined by rcParam\n        ``plot.density_kind`` and discrete ones will use histograms.\n        To override this use \"hist\" to plot histograms and \"kde\" for KDEs.\n    cumulative : bool, default False\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE.\n    label : string\n        Text to include as part of the legend.\n    rotated : bool, default False\n        Whether to rotate the 1D KDE plot 90 degrees.\n    rug : bool, default False\n        Add a `rug plot <https://en.wikipedia.org/wiki/Rug_plot>`_ for a specific subset\n        of values. Ignored for 2D KDE.\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``is_circular`` is False\n        and \"taylor\" (for now) when ``is_circular`` is True.\n        Defaults to \"experimental\" when variable is not circular and \"taylor\" when it is.\n    quantiles : list, optional\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n    contour : bool, default True\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE.\n    fill_last : bool, default True\n        If True fill the last contour of the 2D KDE plot.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on `figsize`. Not implemented for bokeh backend.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE. Passed to :func:`arviz.plot_kde` as\n        ``plot_kwargs``.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use fill_kwargs={'alpha': 0} to disable fill).\n        Ignored for 2D KDE. Passed to :func:`arviz.plot_kde` as ``fill_kwargs``.\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if ``rug=False`` or for 2D KDE\n        Use ``space`` keyword (float) to control the position of the rugplot.\n        The larger this number the lower the rugplot. Passed to\n        :func:`arviz.plot_kde` as ``rug_kwargs``.\n    contour_kwargs : dict\n        Keywords passed to the contourplot. Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to :meth:`matplotlib.axes.Axes.contourf`. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to :meth:`matplotlib.axes.Axes.pcolormesh`. Ignored for 1D KDE.\n    hist_kwargs : dict\n        Keyword arguments used to customize the histogram. Ignored when plotting a KDE.\n        They are passed to :meth:`matplotlib.axes.Axes.hist` if using matplotlib,\n        or to :meth:`bokeh.plotting.figure.quad` if using bokeh. In bokeh case,\n        the following extra keywords are also supported:\n\n        * ``color``: replaces the ``fill_color`` and ``line_color`` of the ``quad`` method\n        * ``bins``: taken from ``hist_kwargs`` and passed to :func:`numpy.histogram` instead\n        * ``density``: normalize histogram to represent a probability density function,\n          Defaults to ``True``\n\n        * ``cumulative``: plot the cumulative counts. Defaults to ``False``.\n\n    is_circular : {False, True, \"radians\", \"degrees\"}, default False\n        Select input type {\"radians\", \"degrees\"} for circular histogram or KDE plot. If True,\n        default input type is \"radians\". When this argument is present, it interprets the\n        values passed are from a circular variable measured in radians and a circular KDE is\n        used. Inputs in \"degrees\" will undergo an internal conversion to radians. Only valid\n        for 1D KDE.\n    ax : matplotlib_axes or bokeh_figure, optional\n        Matplotlib or bokeh targets on which to plot. If not supplied, Arviz will create\n        its own plot area (and return it).\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs :dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figure\n\n    See Also\n    --------\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    plot_kde : 1D or 2D KDE plot taking into account boundary conditions.\n\n    Examples\n    --------\n    Plot an integer distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> a = np.random.poisson(4, 1000)\n        >>> az.plot_dist(a)\n\n    Plot a continuous distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> b = np.random.normal(0, 1, 1000)\n        >>> az.plot_dist(b)\n\n    Add a rug under the Gaussian distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True)\n\n    Segment into quantiles\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True, quantiles=[.25, .5, .75])\n\n    Plot as the cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dist(b, rug=True, quantiles=[.25, .5, .75], cumulative=True)\n    \n source"},{"id":143,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_dist_comparison","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_dist_comparison-Tuple","content":" ArviZPythonPlots.plot_dist_comparison  —  Method Plot to compare fitted and unfitted distributions.\n\n    The resulting plots will show the compared distributions both on\n    separate axes (particularly useful when one of them is substantially tighter\n    than another), and plotted together, displaying a grid of three plots per\n    distribution.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        containing the posterior/prior data. Refer to documentation of\n        :func:`arviz.convert_to_dataset` for details.\n    kind : {\"latent\", \"observed\"}, default \"latent\"\n        kind of plot to display The \"latent\" option includes {\"prior\", \"posterior\"},\n        and the \"observed\" option includes\n        {\"observed_data\", \"prior_predictive\", \"posterior_predictive\"}.\n    figsize : (float, float), optional\n        Figure size. If ``None`` it will be defined automatically.\n    textsize : float\n        Text size scaling factor for labels, titles and lines. If ``None`` it will be\n        autoscaled based on `figsize`.\n    var_names : str, list, list of lists, optional\n        if str, plot the variable. if list, plot all the variables in list\n        of all groups. if list of lists, plot the vars of groups in respective lists.\n        See :ref:`this section <common_var_names>` for usage examples.\n    coords : dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. See :ref:`this section <common_coords>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    transform : callable\n        Function to transform data (defaults to `None` i.e. the identity function).\n    legend : bool\n        Add legend to figure. By default True.\n    labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : (nvars, 3) array-like of matplotlib_axes, optional\n        Matplotlib axes: The ax argument should have shape (nvars, 3), where the\n        last column is for the combined before/after plots and columns 0 and 1 are\n        for the before and after plots, respectively.\n    prior_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for prior/predictive groups.\n    posterior_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for posterior/predictive groups.\n    observed_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_dist` for observed_data group.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : 2D ndarray of matplotlib_axes\n        Returned object will have shape (nvars, 3),\n        where the last column is the combined plot and the first columns are the single plots.\n\n    See Also\n    --------\n    plot_bpv : Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n\n    Examples\n    --------\n    Plot the prior/posterior plot for specified vars and coords.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('rugby')\n        >>> az.plot_dist_comparison(data, var_names=[\"defs\"], coords={\"team\" : [\"Italy\"]})\n\n    \n source"},{"id":144,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_dot","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_dot-Tuple","content":" ArviZPythonPlots.plot_dot  —  Method Plot distribution as dot plot or quantile dot plot.\n\n    This function uses the Wilkinson's Algorithm [1]_ to allot dots to bins.\n    The quantile dot plots was inspired from [2]_.\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot from an unknown continuous or discrete distribution.\n    binwidth : float, optional\n        Width of the bin for drawing the dot plot.\n    dotsize : float, default 1\n        The size of the dots relative to the bin width. The default makes dots be\n        just about as wide as the bin width.\n    stackratio : float, default 1\n        The distance between the center of the dots in the same stack relative to the bin height.\n        The default makes dots in the same stack just touch each other.\n    point_interval : bool, default False\n        Plots the point interval. Uses ``hdi_prob`` to plot the HDI interval\n    point_estimate : str, optional\n        Plot point estimate per variable. Values should be ``mean``, ``median``, ``mode`` or None.\n        Defaults to ``auto`` i.e. it falls back to default set in rcParams.\n    dotcolor : string, optional\n        The color of the dots. Should be a valid matplotlib color.\n    intervalcolor : string, optional\n        The color of the interval. Should be a valid matplotlib color.\n    linewidth : int, default None\n        Line width throughout. If None it will be autoscaled based on `figsize`.\n    markersize : int, default None\n        Markersize throughout. If None it will be autoscaled based on `figsize`.\n    markercolor : string, optional\n        The color of the marker when plot_interval is True. Should be a valid matplotlib color.\n    marker : string, default \"o\"\n        The shape of the marker. Valid for matplotlib backend.\n    hdi_prob : float, optional\n        Valid only when point_interval is True. Plots HDI for chosen percentage of density.\n        Defaults to ``stats.hdi_prob`` rcParam. See :ref:`this section <common_hdi_prob>`\n        for usage examples.\n    rotated : bool, default False\n        Whether to rotate the dot plot by 90 degrees.\n    nquantiles : int, default 50\n        Number of quantiles to plot, used for quantile dot plots.\n    quartiles : bool, default True\n        If True then the quartile interval will be plotted with the HDI.\n    figsize : (float,float), optional\n        Figure size. If ``None`` it will be defined automatically.\n    plot_kwargs : dict, optional\n        Keywords passed for customizing the dots. Passed to :class:`mpl:matplotlib.patches.Circle`\n        in matplotlib and :meth:`bokeh.plotting.figure.circle` in bokeh.\n    backend :{\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    ax : axes, optional\n        Matplotlib_axes or bokeh_figure.\n    show : bool, optional\n        Call backend show function.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n\n    References\n    ----------\n    .. [1] Leland Wilkinson (1999) Dot Plots, The American Statistician, 53:3, 276-281,\n        DOI: 10.1080/00031305.1999.10474474\n    .. [2] Matthew Kay, Tara Kola, Jessica R. Hullman,\n        and Sean A. Munson. 2016. When (ish) is My Bus? User-centered Visualizations of Uncertainty\n        in Everyday, Mobile Predictive Systems. DOI:https://doi.org/10.1145/2858036.2858558\n\n    Examples\n    --------\n    Plot dot plot for a set of data points\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> import numpy as np\n        >>> values = np.random.normal(0, 1, 500)\n        >>> az.plot_dot(values)\n\n    Manually adjust number of quantiles to plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dot(values, nquantiles=100)\n\n    Add a point interval under the dot plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dot(values, point_interval=True)\n\n    Rotate the dot plots by 90 degrees i.e swap x and y axis\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_dot(values, point_interval=True, rotated=True)\n\n    \n source"},{"id":145,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_ecdf","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_ecdf-Tuple","content":" ArviZPythonPlots.plot_ecdf  —  Method Plot ECDF or ECDF-Difference Plot with Confidence bands.\n\n    Plots of the empirical CDF estimates of an array. When `values2` argument is provided,\n    the two empirical CDFs are overlaid with the distribution of `values` on top\n    (in a darker shade) and confidence bands in a more transparent shade. Optionally, the difference\n    between the two empirical CDFs can be computed, and the PIT for a single dataset or a comparison\n    between two samples.\n\n    Notes\n    -----\n    This plot computes the confidence bands with the simulated based algorithm presented in [1]_.\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot from an unknown continuous or discrete distribution.\n    values2 : array-like, optional\n        Values to compare to the original sample.\n    cdf : callable, optional\n        Cumulative distribution function of the distribution to compare the original sample.\n    difference : bool, default False\n        If True then plot ECDF-difference plot otherwise ECDF plot.\n    pit : bool, default False\n        If True plots the ECDF or ECDF-diff of PIT of sample.\n    confidence_bands : bool, default None\n        If True plots the simultaneous or pointwise confidence bands with `1 - fpr`\n        confidence level.\n    pointwise : bool, default False\n        If True plots pointwise confidence bands otherwise simultaneous bands.\n    npoints : int, default 100\n        This denotes the granularity size of our plot i.e the number of evaluation points\n        for the ecdf or ecdf-difference plots.\n    num_trials : int, default 500\n        The number of random ECDFs to generate for constructing simultaneous confidence bands.\n    fpr : float, default 0.05\n        The type I error rate s.t `1 - fpr` denotes the confidence level of bands.\n    figsize : (float,float), optional\n        Figure size. If `None` it will be defined automatically.\n    fill_band : bool, default True\n        If True it fills in between to mark the area inside the confidence interval. Otherwise,\n        plot the border lines.\n    plot_kwargs : dict, optional\n        Additional kwargs passed to :func:`mpl:matplotlib.pyplot.step` or\n        :meth:`bokeh.plotting.figure.step`\n    fill_kwargs : dict, optional\n        Additional kwargs passed to :func:`mpl:matplotlib.pyplot.fill_between` or\n        :meth:`bokeh:bokeh.plotting.Figure.varea`\n    plot_outline_kwargs : dict, optional\n        Additional kwargs passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.line`\n    ax :axes, optional\n        Matplotlib axes or bokeh figures.\n    show : bool, optional\n        Call backend show function.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    References\n    ----------\n    .. [1] Säilynoja, T., Bürkner, P.C. and Vehtari, A., 2021. Graphical Test for\n        Discrete Uniformity and its Applications in Goodness of Fit Evaluation and\n        Multiple Sample Comparison. arXiv preprint arXiv:2103.10522.\n\n    Examples\n    --------\n    Plot ecdf plot for a given sample\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> from scipy.stats import uniform, binom, norm\n\n        >>> sample = norm(0,1).rvs(1000)\n        >>> az.plot_ecdf(sample)\n\n    Plot ecdf plot with confidence bands for comparing a given sample w.r.t a given distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> distribution = norm(0,1)\n        >>> az.plot_ecdf(sample, cdf = distribution.cdf, confidence_bands = True)\n\n    Plot ecdf-difference plot with confidence bands for comparing a given sample\n    w.r.t a given distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ecdf(sample, cdf = distribution.cdf,\n        >>>              confidence_bands = True, difference = True)\n\n    Plot ecdf plot with confidence bands for PIT of sample for comparing a given sample\n    w.r.t a given distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ecdf(sample, cdf = distribution.cdf,\n        >>>              confidence_bands = True, pit = True)\n\n    Plot ecdf-difference plot with confidence bands for PIT of sample for comparing a given\n    sample w.r.t a given distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ecdf(sample, cdf = distribution.cdf,\n        >>>              confidence_bands = True, difference = True, pit = True)\n\n    You could also plot the above w.r.t another sample rather than a given distribution.\n    For eg: Plot ecdf-difference plot with confidence bands for PIT of sample for\n    comparing a given sample w.r.t a given sample\n\n    .. plot::\n        :context: close-figs\n\n        >>> sample2 = norm(0,1).rvs(5000)\n        >>> az.plot_ecdf(sample, sample2, confidence_bands = True, difference = True, pit = True)\n\n    \n source"},{"id":146,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_elpd","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_elpd-Tuple","content":" ArviZPythonPlots.plot_elpd  —  Method Plot pointwise elpd differences between two or more models.\n\n    Pointwise model comparison based on their expected log pointwise predictive density (ELPD).\n\n    Notes\n    -----\n    The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out\n    cross-validation (LOO) or using the widely applicable information criterion (WAIC).\n    We recommend LOO in line with the work presented by [1]_.\n\n    Parameters\n    ----------\n    compare_dict : mapping of {str : ELPDData or InferenceData}\n        A dictionary mapping the model name to the object containing inference data or the result\n        of :func:`arviz.loo` or :func:`arviz.waic` functions.\n        Refer to :func:`arviz.convert_to_inference_data` for details on possible dict items.\n    color : str or array_like, default \"C0\"\n        Colors of the scatter plot. If color is a str all dots will have the same color.\n        If it is the size of the observations, each dot will have the specified color.\n        Otherwise, it will be interpreted as a list of the dims to be used for the color code.\n    xlabels : bool, default False\n        Use coords as xticklabels.\n    figsize : (float, float), optional\n        If `None`, size is (8 + numvars, 8 + numvars).\n    textsize : float, optional\n        Text size for labels. If `None` it will be autoscaled based on `figsize`.\n    coords : mapping, optional\n        Coordinates of points to plot. **All** values are used for computation, but only a\n        subset can be plotted for convenience. See :ref:`this section <common_coords>`\n        for usage examples.\n    legend : bool, default False\n        Include a legend to the plot. Only taken into account when color argument is a dim name.\n    threshold : float, optional\n        If some elpd difference is larger than ``threshold * elpd.std()``, show its label. If\n        `None`, no observations will be highlighted.\n    ic : str, optional\n        Information Criterion (\"loo\" for PSIS-LOO, \"waic\" for WAIC) used to compare models.\n        Defaults to ``rcParams[\"stats.information_criterion\"]``.\n        Only taken into account when input is :class:`arviz.InferenceData`.\n    scale : str, optional\n        Scale argument passed to :func:`arviz.loo` or :func:`arviz.waic`, see their docs for\n        details. Only taken into account when values in ``compare_dict`` are\n        :class:`arviz.InferenceData`.\n    var_name : str, optional\n        Argument passed to to :func:`arviz.loo` or :func:`arviz.waic`, see their docs for\n        details. Only taken into account when values in ``compare_dict`` are\n        :class:`arviz.InferenceData`.\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter`.\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    plot_compare : Summary plot for model comparison.\n    loo : Compute Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO-CV).\n    waic : Compute the widely applicable information criterion.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2016). Practical Bayesian model evaluation using leave-one-out\n    cross-validation and WAIC https://arxiv.org/abs/1507.04544\n\n    Examples\n    --------\n    Compare pointwise PSIS-LOO for centered and non centered models of the 8-schools problem\n    using matplotlib.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata1 = az.load_arviz_data(\"centered_eight\")\n        >>> idata2 = az.load_arviz_data(\"non_centered_eight\")\n        >>> az.plot_elpd(\n        >>>     {\"centered model\": idata1, \"non centered model\": idata2},\n        >>>     xlabels=True\n        >>> )\n\n    .. bokeh-plot::\n        :source-position: above\n\n        import arviz as az\n        idata1 = az.load_arviz_data(\"centered_eight\")\n        idata2 = az.load_arviz_data(\"non_centered_eight\")\n        az.plot_elpd(\n            {\"centered model\": idata1, \"non centered model\": idata2},\n            backend=\"bokeh\"\n        )\n\n    \n source"},{"id":147,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_energy","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_energy-Tuple","content":" ArviZPythonPlots.plot_energy  —  Method Plot energy transition distribution and marginal energy distribution in HMC algorithms.\n\n    This may help to diagnose poor exploration by gradient-based algorithms like HMC or NUTS.\n    The energy function in HMC can identify posteriors with heavy tailed distributions, that\n    in practice are challenging for sampling.\n\n    This plot is in the style of the one used in [1]_.\n\n    Parameters\n    ----------\n    data : obj\n        :class:`xarray.Dataset`, or any object that can be converted (must represent\n        ``sample_stats`` and have an ``energy`` variable).\n    kind : str, optional\n        Type of plot to display (\"kde\", \"hist\").\n    bfmi : bool, default True\n        If True add to the plot the value of the estimated Bayesian fraction of missing\n        information.\n    figsize : (float, float), optional\n        Figure size. If `None` it will be defined automatically.\n    legend : bool, default True\n        Flag for plotting legend.\n    fill_alpha : tuple, default (1, 0.75)\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque).\n    fill_color : tuple of valid matplotlib color, default ('C0', 'C5')\n        Color for Marginal energy distribution and Energy transition distribution.\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\". Defaults to \"experimental\".\n        Only works if ``kind='kde'``.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled\n        based on `figsize`.\n    fill_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_kde` (to control the shade).\n    plot_kwargs : dicts, optional\n        Additional keywords passed to :func:`arviz.plot_kde` or :func:`matplotlib.pyplot.hist`\n        (if ``type='hist'``).\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    bfmi : Calculate the estimated Bayesian fraction of missing information (BFMI).\n\n    References\n    ----------\n    .. [1] Betancourt (2016). Diagnosing Suboptimal Cotangent Disintegrations in\n    Hamiltonian Monte Carlo https://arxiv.org/abs/1604.00695\n\n    Examples\n    --------\n    Plot a default energy plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_energy(data)\n\n    Represent energy plot via histograms\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_energy(data, kind='hist')\n\n    \n source"},{"id":148,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_ess","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_ess-Tuple","content":" ArviZPythonPlots.plot_ess  —  Method Generate quantile, local, or evolution ESS plots.\n\n    The local and the quantile ESS plots are recommended for checking\n    that there are enough samples for all the explored regions of the\n    parameter space. Checking local and quantile ESS is particularly\n    relevant when working with HDI intervals as opposed to ESS bulk,\n    which is suitable for point estimates.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    kind : {\"local\", \"quantile\", \"evolution\"}, default \"local\"\n        Specify the kind of plot:\n\n        * The ``kind=\"local\"`` argument generates the ESS' local efficiency for\n          estimating quantiles of a desired posterior.\n        * The ``kind=\"quantile\"`` argument generates the ESS' local efficiency\n          for estimating small-interval probability of a desired posterior.\n        * The ``kind=\"evolution\"`` argument generates the estimated ESS'\n          with incrised number of iterations of a desired posterior.\n\n    relative : bool, default False\n        Show relative ess in plot ``ress = ess / N``.\n    coords : dict, optional\n        Coordinates of `var_names` to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n        See :ref:`this section <common_coords>` for usage examples.\n    grid : tuple, optional\n        Number of rows and columns. By default, the rows and columns are\n        automatically inferred. See :ref:`this section <common_grid>` for usage examples.\n    figsize : (float, float), optional\n        Figure size. If ``None`` it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If ``None`` it will be autoscaled\n        based on `figsize`.\n    rug : bool, default False\n        Add a `rug plot <https://en.wikipedia.org/wiki/Rug_plot>`_ for a specific subset of values.\n    rug_kind : str, default \"diverging\"\n        Variable in sample stats to use as rug mask. Must be a boolean variable.\n    n_points : int, default 20\n        Number of points for which to plot their quantile/local ess or number of subsets\n        in the evolution plot.\n    extra_methods : bool, default False\n        Plot mean and sd ESS as horizontal lines. Not taken into account if ``kind = 'evolution'``.\n    min_ess : int, default 400\n        Minimum number of ESS desired. If ``relative=True`` the line is plotted at\n        ``min_ess / n_samples`` for local and quantile kinds and as a curve following\n        the ``min_ess / n`` dependency in evolution kind.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figure, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    extra_kwargs : dict, optional\n        If evolution plot, `extra_kwargs` is used to plot ess tail and differentiate it\n        from ess bulk. Otherwise, passed to extra methods lines.\n    text_kwargs : dict, optional\n        Only taken into account when ``extra_methods=True``. kwargs passed to ax.annotate\n        for extra methods lines labels. It accepts the additional\n        key ``x`` to set ``xy=(text_kwargs[\"x\"], mcse)``\n    hline_kwargs : dict, optional\n        kwargs passed to :func:`~matplotlib.axes.Axes.axhline` or to :class:`~bokeh.models.Span`\n        depending on the backend for the horizontal minimum ESS line.\n        For relative ess evolution plots the kwargs are passed to\n        :func:`~matplotlib.axes.Axes.plot` or to :class:`~bokeh.plotting.figure.line`\n    rug_kwargs : dict\n        kwargs passed to rug plot.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :meth:`mpl:matplotlib.axes.Axes.hist` or\n        :meth:`mpl:matplotlib.axes.Axes.plot` function depending on the\n        value of `kind`.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figure\n\n    See Also\n    --------\n    ess : Calculate estimate of the effective sample size.\n\n    References\n    ----------\n    .. [1] Vehtari et al. (2019). Rank-normalization, folding, and\n        localization: An improved Rhat for assessing convergence of\n        MCMC https://arxiv.org/abs/1903.08008\n\n    Examples\n    --------\n    Plot local ESS.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"centered_eight\")\n        >>> coords = {\"school\": [\"Choate\", \"Lawrenceville\"]}\n        >>> az.plot_ess(\n        ...     idata, kind=\"local\", var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    Plot ESS evolution as the number of samples increase. When the model is converging properly,\n    both lines in this plot should be roughly linear.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"evolution\", var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    Customize local ESS plot to look like reference paper.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ess(\n        ...     idata, kind=\"local\", var_names=[\"mu\"], drawstyle=\"steps-mid\", color=\"k\",\n        ...     linestyle=\"-\", marker=None, rug=True, rug_kwargs={\"color\": \"r\"}\n        ... )\n\n    Customize ESS evolution plot to look like reference paper.\n\n    .. plot::\n        :context: close-figs\n\n        >>> extra_kwargs = {\"color\": \"lightsteelblue\"}\n        >>> az.plot_ess(\n        ...     idata, kind=\"evolution\", var_names=[\"mu\"],\n        ...     color=\"royalblue\", extra_kwargs=extra_kwargs\n        ... )\n\n    \n source"},{"id":149,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_forest","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_forest-Tuple","content":" ArviZPythonPlots.plot_forest  —  Method Forest plot to compare HDI intervals from a number of distributions.\n\n    Generate forest or ridge plots to compare distributions from a model or list of models.\n    Additionally, the function can display effective sample sizes (ess) and Rhats to visualize\n    convergence diagnostics alongside the distributions.\n\n    Parameters\n    ----------\n    data : InferenceData\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details.\n    kind : {\"foresplot\", \"ridgeplot\"}, default \"forestplot\"\n        Specify the kind of plot:\n\n        * The ``kind=\"forestplot\"`` generates credible intervals, where the central points are the\n          estimated posterior means, the thick lines are the central quartiles, and the thin lines\n          represent the :math:`100\\times`(`hdi_prob`)% highest density intervals.\n        * The ``kind=\"ridgeplot\"`` option generates density plots (kernel density estimate or\n          histograms) in the same graph. Ridge plots can be configured to have different overlap,\n          truncation bounds and quantile markers.\n\n    model_names : list of str, optional\n        List with names for the models in the list of data. Useful when plotting more that one\n        dataset.\n    var_names : list of str, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot. See :ref:`this section <common_var_names>` for usage examples.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See :ref:`this section <common_combine_dims>` for usage examples.\n    filter_vars : {None, \"like\", \"regex\"}, default None\n        If `None` (default), interpret `var_names` as the real variables names. If \"like\",\n        interpret `var_names` as substrings of the real variables names. If \"regex\",\n        interpret `var_names` as regular expressions on the real variables names. See\n        :ref:`this section <common_filter_vars>` for usage examples.\n    transform : callable, optional\n        Function to transform data (defaults to None i.e.the identity function).\n    coords : dict, optional\n        Coordinates of ``var_names`` to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n        See :ref:`this section <common_coords>` for usage examples.\n    combined : bool, default False\n        Flag for combining multiple chains into a single chain. If False, chains will\n        be plotted separately. See :ref:`this section <common_combine>` for usage examples.\n    hdi_prob : float, default 0.94\n        Plots highest posterior density interval for chosen percentage of density.\n        See :ref:`this section <common_ hdi_prob>` for usage examples.\n    rope : tuple or dictionary of tuples\n        Lower and upper values of the Region of Practical Equivalence. If a list with one interval\n        only is provided, the ROPE will be displayed across the y-axis. If more than one\n        interval is provided the length of the list should match the number of variables.\n    quartiles : bool, default True\n        Flag for plotting the interquartile range, in addition to the ``hdi_prob`` intervals.\n    r_hat : bool, default False\n        Flag for plotting Split R-hat statistics. Requires 2 or more chains.\n    ess : bool, default False\n        Flag for plotting the effective sample size.\n    colors : list or string, optional\n        list with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically chose a color per model from the matplotlibs\n        cycle. If a single color is passed, eg 'k', 'C2', 'red' this color will be used for all\n        models. Defaults to 'cycle'.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If `None` it will be autoscaled based\n        on ``figsize``.\n    linewidth : int, optional\n        Line width throughout. If `None` it will be autoscaled based on ``figsize``.\n    markersize : int, optional\n        Markersize throughout. If `None` it will be autoscaled based on ``figsize``.\n    legend : bool, optional\n        Show a legend with the color encoded model information.\n        Defaults to True, if there are multiple models.\n    labeller : Labeller, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ridgeplot_alpha: float, optional\n        Transparency for ridgeplot fill.  If ``ridgeplot_alpha=0``, border is colored by model,\n        otherwise a `black` outline is used.\n    ridgeplot_overlap : float, default 2\n        Overlap height for ridgeplots.\n    ridgeplot_kind : string, optional\n        By default (\"auto\") continuous variables are plotted using KDEs and discrete ones using\n        histograms. To override this use \"hist\" to plot histograms and \"density\" for KDEs.\n    ridgeplot_truncate : bool, default True\n        Whether to truncate densities according to the value of ``hdi_prob``.\n    ridgeplot_quantiles : list, optional\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n    figsize : (float, float), optional\n        Figure size. If `None`, it will be defined automatically.\n    ax : axes, optional\n        :class:`matplotlib.axes.Axes` or :class:`bokeh.plotting.Figure`.\n    backend : {\"matplotlib\", \"bokeh\"}, default \"matplotlib\"\n        Select plotting backend.\n    backend_config : dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in ``rcParams``.\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :class:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    1D ndarray of matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    plot_posterior : Plot Posterior densities in the style of John K. Kruschke's book.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    summary : Create a data frame with summary statistics.\n\n    Examples\n    --------\n    Forestplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered_data = az.load_arviz_data('non_centered_eight')\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='forestplot',\n        >>>                            var_names=[\"^the\"],\n        >>>                            filter_vars=\"regex\",\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Forestplot with multiple datasets\n\n    .. plot::\n        :context: close-figs\n\n        >>> centered_data = az.load_arviz_data('centered_eight')\n        >>> axes = az.plot_forest([non_centered_data, centered_data],\n        >>>                            model_names = [\"non centered eight\", \"centered eight\"],\n        >>>                            kind='forestplot',\n        >>>                            var_names=[\"^the\"],\n        >>>                            filter_vars=\"regex\",\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools models')\n\n    Forestplot with ropes\n\n    .. plot::\n        :context: close-figs\n\n        >>> rope = {'theta': [{'school': 'Choate', 'rope': (2, 4)}], 'mu': [{'rope': (-2, 2)}]}\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            rope=rope,\n        >>>                            var_names='~tau',\n        >>>                            combined=True,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n\n    Ridgeplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_overlap=3,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Ridgeplot non-truncated and with quantiles\n\n    .. plot::\n        :context: close-figs\n\n        >>> axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_truncate=False,\n        >>>                            ridgeplot_quantiles=[.25, .5, .75],\n        >>>                            ridgeplot_overlap=0.7,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n    \n source"},{"id":150,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_hdi","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_hdi-Tuple","content":" ArviZPythonPlots.plot_hdi  —  Method \n    Plot HDI intervals for regression data.\n\n    Parameters\n    ----------\n    x : array-like\n        Values to plot.\n    y : array-like, optional\n        Values from which to compute the HDI. Assumed shape ``(chain, draw, \\*shape)``.\n        Only optional if ``hdi_data`` is present.\n    hdi_data : array_like, optional\n        Precomputed HDI values to use. Assumed shape is ``(*x.shape, 2)``.\n    hdi_prob : float, optional\n        Probability for the highest density interval. Defaults to ``stats.hdi_prob`` rcParam.\n    color : str, optional\n        Color used for the limits of the HDI and fill. Should be a valid matplotlib color.\n    circular : bool, optional\n        Whether to compute the HDI taking into account ``x`` is a circular variable\n        (in the range [-np.pi, np.pi]) or not. Defaults to False (i.e non-circular variables).\n    smooth : boolean, optional\n        If True the result will be smoothed by first computing a linear interpolation of the data\n        over a regular grid and then applying the Savitzky-Golay filter to the interpolated data.\n        Defaults to True.\n    smooth_kwargs : dict, optional\n        Additional keywords modifying the Savitzky-Golay filter. See\n        :func:`scipy:scipy.signal.savgol_filter` for details.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    fill_kwargs : dict, optional\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.fill_between`\n        (use ``fill_kwargs={'alpha': 0}`` to disable fill) or to\n        :meth:`bokeh.plotting.Figure.patch`.\n    plot_kwargs : dict, optional\n        HDI limits keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh.plotting.Figure.patch`.\n    hdi_kwargs : dict, optional\n        Keyword arguments passed to :func:`~arviz.hdi`. Ignored if ``hdi_data`` is present.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    backend : {\"matplotlib\",\"bokeh\"}, optional\n        Select plotting backend.\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh.plotting.Figure.patch`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    hdi : Calculate highest density interval (HDI) of array for given probability.\n\n    Examples\n    --------\n    Plot HDI interval of simulated regression data using `y` argument:\n\n    .. plot::\n        :context: close-figs\n\n        >>> import numpy as np\n        >>> import arviz as az\n        >>> x_data = np.random.normal(0, 1, 100)\n        >>> y_data = np.random.normal(2 + x_data * 0.5, 0.5, size=(2, 50, 100))\n        >>> az.plot_hdi(x_data, y_data)\n\n    ``plot_hdi`` can also be given precalculated values with the argument ``hdi_data``. This example\n    shows how to use :func:`~arviz.hdi` to precalculate the values and pass these values to\n    ``plot_hdi``. Similarly to an example in ``hdi`` we are using the ``input_core_dims``\n    argument of :func:`~arviz.wrap_xarray_ufunc` to manually define the dimensions over which\n    to calculate the HDI.\n\n    .. plot::\n        :context: close-figs\n\n        >>> hdi_data = az.hdi(y_data, input_core_dims=[[\"draw\"]])\n        >>> ax = az.plot_hdi(x_data, hdi_data=hdi_data[0], color=\"r\", fill_kwargs={\"alpha\": .2})\n        >>> az.plot_hdi(x_data, hdi_data=hdi_data[1], color=\"k\", ax=ax, fill_kwargs={\"alpha\": .2})\n\n    ``plot_hdi`` can also be used with Inference Data objects. Here we use the posterior predictive\n    to plot the HDI interval.\n\n    .. plot::\n        :context: close-figs\n\n        >>> X = np.random.normal(0,1,100)\n        >>> Y = np.random.normal(2 + X * 0.5, 0.5, size=(2,10,100))\n        >>> idata = az.from_dict(posterior={\"y\": Y}, constant_data={\"x\":X})\n        >>> x_data = idata.constant_data.x\n        >>> y_data = idata.posterior.y\n        >>> az.plot_hdi(x_data, y_data)\n\n    \n source"},{"id":151,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_kde","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_kde-Tuple","content":" ArviZPythonPlots.plot_kde  —  Method 1D or 2D KDE plot taking into account boundary conditions.\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE will be estimated\n    cumulative : bool\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE\n    rug : bool\n        If True adds a rugplot. Defaults to False. Ignored for 2D KDE\n    label : string\n        Text to include as part of the legend\n    bw : float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``is_circular`` is False\n        and \"taylor\" (for now) when ``is_circular`` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    adaptive : bool, optional.\n        If True, an adaptative bandwidth is used. Only valid for 1D KDE.\n        Defaults to False.\n    quantiles : list\n        Quantiles in ascending order used to segment the KDE.\n        Use [.25, .5, .75] for quartiles. Defaults to None.\n    rotated : bool\n        Whether to rotate the 1D KDE plot 90 degrees.\n    contour : bool\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE.\n        Defaults to True.\n    hdi_probs : list\n        Plots highest density credibility regions for the provided probabilities for a 2D KDE.\n        Defaults to matplotlib chosen levels with no fixed probability associated.\n    fill_last : bool\n        If True fill the last contour of the 2D KDE plot. Defaults to False.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``. Not implemented for bokeh backend.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE. See :meth:`mpl:matplotlib.axes.Axes.plot`\n        or :meth:`bokeh:bokeh.plotting.Figure.line` for a description of accepted values.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use ``fill_kwargs={'alpha': 0}``\n        to disable fill). Ignored for 2D KDE. Passed to\n        :meth:`bokeh.plotting.Figure.patch`.\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if ``rug=False`` or for 2D KDE\n        Use ``space`` keyword (float) to control the position of the rugplot. The larger this number\n        the lower the rugplot. Passed to :class:`bokeh:bokeh.models.glyphs.Scatter`.\n    contour_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.contour`\n        to draw contour lines or :meth:`bokeh.plotting.Figure.patch`.\n        Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.contourf`\n        to draw filled contours. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to :meth:`mpl:matplotlib.axes.Axes.pcolormesh` or\n        :meth:`bokeh.plotting.Figure.image`.\n        Ignored for 1D KDE.\n    is_circular : {False, True, \"radians\", \"degrees\"}. Default False.\n        Select input type {\"radians\", \"degrees\"} for circular histogram or KDE plot. If True,\n        default input type is \"radians\". When this argument is present, it interprets ``values``\n        is a circular variable measured in radians and a circular KDE is used. Inputs in\n        \"degrees\" will undergo an internal conversion to radians.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    legend : bool\n        Add legend to the figure. By default True.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n    return_glyph : bool, optional\n        Internal argument to return glyphs for bokeh\n\n    Returns\n    -------\n    axes : matplotlib.Axes or bokeh.plotting.Figure\n        Object containing the kde plot\n    glyphs : list, optional\n        Bokeh glyphs present in plot.  Only provided if ``return_glyph`` is True.\n\n    See Also\n    --------\n    kde : One dimensional density estimation.\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n\n    Examples\n    --------\n    Plot default KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> mu_posterior = np.concatenate(non_centered.posterior[\"mu\"].values)\n        >>> tau_posterior = np.concatenate(non_centered.posterior[\"tau\"].values)\n        >>> az.plot_kde(mu_posterior)\n\n\n    Plot KDE with rugplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rug=True)\n\n    Plot KDE with adaptive bandwidth\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, adaptive=True)\n\n    Plot KDE with a different bandwidth estimator\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, bw=\"scott\")\n\n    Plot KDE with a bandwidth specified manually\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, bw=0.4)\n\n    Plot KDE for a circular variable\n\n    .. plot::\n        :context: close-figs\n\n        >>> rvs = np.random.vonmises(mu=np.pi, kappa=2, size=500)\n        >>> az.plot_kde(rvs, is_circular=True)\n\n\n    Plot a cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, cumulative=True)\n\n\n\n    Rotate plot 90 degrees\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rotated=True)\n\n\n    Plot 2d contour KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior)\n\n\n    Plot 2d contour KDE, without filling and contour lines using viridis cmap\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior,\n        ...             contour_kwargs={\"colors\":None, \"cmap\":plt.cm.viridis},\n        ...             contourf_kwargs={\"alpha\":0});\n\n    Plot 2d contour KDE, set the number of levels to 3.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(\n        ...     mu_posterior, values2=tau_posterior,\n        ...     contour_kwargs={\"levels\":3}, contourf_kwargs={\"levels\":3}\n        ... );\n\n    Plot 2d contour KDE with 30%, 60% and 90% HDI contours.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, hdi_probs=[0.3, 0.6, 0.9])\n\n    Plot 2d smooth KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, contour=False)\n\n    \n source"},{"id":152,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_khat","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_khat-Tuple","content":" ArviZPythonPlots.plot_khat  —  Method \n    Plot Pareto tail indices for diagnosing convergence.\n\n    Parameters\n    ----------\n    khats : ELPDData containing Pareto shapes information or array of\n        Pareto tail indices.\n    color : str or array_like, optional\n        Colors of the scatter plot, if color is a str all dots will\n        have the same color, if it is the size of the observations,\n        each dot will have the specified color, otherwise, it will be\n        interpreted as a list of the dims to be used for the color\n        code. If Matplotlib c argument is passed, it will override\n        the color argument\n    xlabels : bool, optional\n        Use coords as xticklabels\n    show_hlines : bool, optional\n        Show the horizontal lines, by default at the values [0, 0.5, 0.7, 1].\n    show_bins : bool, optional\n        Show the percentage of khats falling in each bin, as delimited by hlines.\n    bin_format : str, optional\n        The string is used as formatting guide calling ``bin_format.format(count, pct)``.\n    threshold : float, optional\n        Show the labels of k values larger than threshold. Defaults to `None`,\n        no observations will be highlighted.\n    hover_label : bool, optional\n        Show the datapoint label when hovering over it with the mouse. Requires an interactive\n        backend.\n    hover_format : str, optional\n        String used to format the hover label via ``hover_format.format(idx, coord_label)``\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    coords : mapping, optional\n        Coordinates of points to plot. **All** values are used for computation, but only a\n        a subset can be plotted for convenience.\n    legend : bool, optional\n        Include a legend to the plot. Only taken into account when color argument is a dim name.\n    markersize : int, optional\n        markersize for scatter plot. Defaults to `None` in which case it will\n        be chosen based on autoscaling for figsize.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    hlines_kwargs : dictionary, optional\n        Additional keywords passed to\n        :meth:`matplotlib.axes.Axes.hlines`.\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n    kwargs :\n        Additional keywords passed to\n        :meth:`matplotlib.axes.Axes.scatter`.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    psislw : Pareto smoothed importance sampling (PSIS).\n\n    Examples\n    --------\n    Plot estimated pareto shape parameters showing how many fall in each category.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> radon = az.load_arviz_data(\"radon\")\n        >>> loo_radon = az.loo(radon, pointwise=True)\n        >>> az.plot_khat(loo_radon, show_bins=True)\n\n    Show xlabels\n\n    .. plot::\n        :context: close-figs\n\n        >>> centered_eight = az.load_arviz_data(\"centered_eight\")\n        >>> khats = az.loo(centered_eight, pointwise=True).pareto_k\n        >>> az.plot_khat(khats, xlabels=True, threshold=1)\n\n    Use custom color scheme\n\n    .. plot::\n        :context: close-figs\n\n        >>> counties = radon.posterior.County[radon.constant_data.county_idx].values\n        >>> colors = [\n        ...     \"blue\" if county[-1] in (\"A\", \"N\") else \"green\" for county in counties\n        ... ]\n        >>> az.plot_khat(loo_radon, color=colors)\n\n    Notes\n    -----\n    The Generalized Pareto distribution (GPD) may be used to diagnose\n    convergence rates for importance sampling.  GPD has parameters\n    offset, scale, and shape. The shape parameter is usually denoted\n    with ``k``. ``k`` also tells how many finite moments the\n    distribution has. The pre-asymptotic convergence rate of\n    importance sampling can be estimated based on the fractional\n    number of finite moments of the importance ratio distribution. GPD\n    is fitted to the largest importance ratios and the estimated shape\n    parameter ``k``, i.e., ``\\hat{k}`` can then be used as a diagnostic\n    (most importantly if ``\\hat{k} > 0.7``, then the convergence rate\n    is impractically low). See [1]_.\n\n    References\n    ----------\n    .. [1] Vehtari, A., Simpson, D., Gelman, A., Yao, Y., Gabry, J.,\n        2019. Pareto Smoothed Importance Sampling. arXiv:1507.02646 [stat].\n\n    \n source"},{"id":153,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_lm","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_lm-Tuple","content":" ArviZPythonPlots.plot_lm  —  Method Posterior predictive and mean plots for regression-like data.\n\n    Parameters\n    ----------\n    y : str or DataArray or ndarray\n        If str, variable name from ``observed_data``.\n    idata : InferenceData, Optional\n        Optional only if ``y`` is not str.\n    x : str, tuple of strings, DataArray or array-like, optional\n        If str or tuple, variable name from ``constant_data``.\n        If ndarray, could be 1D, or 2D for multiple plots.\n        If None, coords name of ``y`` (``y`` should be DataArray).\n    y_model : str or Sequence, Optional\n        If str, variable name from ``posterior``.\n        Its dimensions should be same as ``y`` plus added chains and draws.\n    y_hat : str, Optional\n        If str, variable name from ``posterior_predictive``.\n        Its dimensions should be same as ``y`` plus added chains and draws.\n    num_samples : int, Optional, Default 50\n        Significant if ``kind_pp`` is \"samples\" or ``kind_model`` is \"lines\".\n        Number of samples to be drawn from posterior predictive or\n    kind_pp : {\"samples\", \"hdi\"}, Default \"samples\"\n        Options to visualize uncertainty in data.\n    kind_model : {\"lines\", \"hdi\"}, Default \"lines\"\n        Options to visualize uncertainty in mean of the data.\n    plot_dim : str, Optional\n        Necessary if ``y`` is multidimensional.\n    backend : str, Optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    y_kwargs : dict, optional\n        Passed to :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib\n        and :meth:`bokeh:bokeh.plotting.Figure.circle` in bokeh\n    y_hat_plot_kwargs : dict, optional\n        Passed to :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib\n        and :meth:`bokeh:bokeh.plotting.Figure.circle` in bokeh\n    y_hat_fill_kwargs : dict, optional\n        Passed to :func:`arviz.plot_hdi`\n    y_model_plot_kwargs : dict, optional\n        Passed to :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib\n        and :meth:`bokeh:bokeh.plotting.Figure.line` in bokeh\n    y_model_fill_kwargs : dict, optional\n        Significant if ``kind_model`` is \"hdi\". Passed to :func:`arviz.plot_hdi`\n    y_model_mean_kwargs : dict, optional\n        Passed to :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib\n        and :meth:`bokeh:bokeh.plotting.Figure.line` in bokeh\n    backend_kwargs : dict, optional\n        These are kwargs specific to the backend being used. Passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be\n        autoscaled based on ``figsize``.\n    axes : 2D numpy array-like of matplotlib_axes or bokeh_figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    show : bool, optional\n        Call backend show function.\n    legend : bool, optional\n        Add legend to figure. By default True.\n    grid : bool, optional\n        Add grid to figure. By default True.\n\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_ts : Plot timeseries data\n    plot_ppc : Plot for posterior/prior predictive checks\n\n    Examples\n    --------\n    Plot regression default plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> import numpy as np\n        >>> import xarray as xr\n        >>> idata = az.load_arviz_data('regression1d')\n        >>> x = xr.DataArray(np.linspace(0, 1, 100))\n        >>> idata.posterior[\"y_model\"] = idata.posterior[\"intercept\"] + idata.posterior[\"slope\"]*x\n        >>> az.plot_lm(idata=idata, y=\"y\", x=x)\n\n    Plot regression data and mean uncertainty\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_lm(idata=idata, y=\"y\", x=x, y_model=\"y_model\")\n\n    Plot regression data and mean uncertainty in hdi form\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_lm(\n        ...     idata=idata, y=\"y\", x=x, y_model=\"y_model\", kind_pp=\"hdi\", kind_model=\"hdi\"\n        ... )\n\n    Plot regression data for multi-dimensional y using plot_dim\n\n    .. plot::\n        :context: close-figs\n\n        >>> data = az.from_dict(\n        ...     observed_data = { \"y\": np.random.normal(size=(5, 7)) },\n        ...     posterior_predictive = {\"y\": np.random.randn(4, 1000, 5, 7) / 2},\n        ...     dims={\"y\": [\"dim1\", \"dim2\"]},\n        ...     coords={\"dim1\": range(5), \"dim2\": range(7)}\n        ... )\n        >>> az.plot_lm(idata=data, y=\"y\", plot_dim=\"dim1\")\n    \n source"},{"id":154,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_loo_pit","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_loo_pit-Tuple","content":" ArviZPythonPlots.plot_loo_pit  —  Method Plot Leave-One-Out (LOO) probability integral transformation (PIT) predictive checks.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        :class:`arviz.InferenceData` object.\n    y : array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat : array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as y plus an\n        extra dimension at the end of size n_samples (chains and draws stacked). If str or\n        None, ``idata`` must contain the posterior predictive group. If None, ``y_hat`` is taken\n        equal to y, thus, y must be str too.\n    log_weights : array or DataArray\n        Smoothed log_weights. It must have the same shape as ``y_hat``\n    ecdf : bool, optional\n        Plot the difference between the LOO-PIT Empirical Cumulative Distribution Function\n        (ECDF) and the uniform CDF instead of LOO-PIT kde.\n        In this case, instead of overlaying uniform distributions, the beta ``hdi_prob``\n        around the theoretical uniform CDF is shown. This approximation only holds\n        for large S and ECDF values not very close to 0 nor 1. For more information, see\n        `Vehtari et al. (2019)`, `Appendix G <https://avehtari.github.io/rhat_ess/rhat_ess.html>`_.\n    ecdf_fill : bool, optional\n        Use :meth:`matplotlib.axes.Axes.fill_between` to mark the area\n        inside the credible interval. Otherwise, plot the\n        border lines.\n    n_unif : int, optional\n        Number of datasets to simulate and overlay from the uniform distribution.\n    use_hdi : bool, optional\n        Compute expected hdi values instead of overlaying the sampled uniform distributions.\n    hdi_prob : float, optional\n        Probability for the highest density interval. Works with ``use_hdi=True`` or ``ecdf=True``.\n    figsize : (float, float), optional\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize : int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    labeller : Labeller, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    color : str or array_like, optional\n        Color of the LOO-PIT estimated pdf plot. If ``plot_unif_kwargs`` has no \"color\" key,\n        a slightly lighter color than this argument will be used for the uniform kde lines.\n        This will ensure that LOO-PIT kde and uniform kde have different default colors.\n    legend : bool, optional\n        Show the legend of the figure.\n    ax : axes, optional\n        Matplotlib axes or bokeh figures.\n    plot_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot`\n        for LOO-PIT line (kde or ECDF)\n    plot_unif_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot` for\n        overlaid uniform distributions or for beta credible interval\n        lines if ``ecdf=True``\n    hdi_kwargs : dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.axhspan`\n    fill_kwargs : dict, optional\n        Additional kwargs passed to :meth:`matplotlib.axes.Axes.fill_between`\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib_axes or bokeh_figures\n\n    See Also\n    --------\n    plot_bpv : Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n    loo_pit : Compute leave one out (PSIS-LOO) probability integral transform (PIT) values.\n\n    References\n    ----------\n    * Gabry et al. (2017) see https://arxiv.org/abs/1709.01449\n    * https://mc-stan.org/bayesplot/reference/PPC-loo.html\n    * Gelman et al. BDA (2014) Section 6.3\n\n    Examples\n    --------\n    Plot LOO-PIT predictive checks overlaying the KDE of the LOO-PIT values to several\n    realizations of uniform variable sampling with the same number of observations.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"radon\")\n        >>> az.plot_loo_pit(idata=idata, y=\"y\")\n\n    Fill the area containing the 94% highest density interval of the difference between uniform\n    variables empirical CDF and the real uniform CDF. A LOO-PIT ECDF clearly outside of these\n    theoretical boundaries indicates that the observations and the posterior predictive\n    samples do not follow the same distribution.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_loo_pit(idata=idata, y=\"y\", ecdf=True)\n\n    \n source"},{"id":155,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_mcse","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_mcse-Tuple","content":" ArviZPythonPlots.plot_mcse  —  Method Plot quantile or local Monte Carlo Standard Error.\n\n    Parameters\n    ----------\n    idata : obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names : list of variable names, optional\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars : {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        `pandas.filter`.\n    coords : dict, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    errorbar : bool, optional\n        Plot quantile value +/- mcse instead of plotting mcse.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize : (float, float), optional\n        Figure size. If None it will be defined automatically.\n    textsize : float, optional\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    extra_methods : bool, optional\n        Plot mean and sd MCSE as horizontal lines. Only taken into account when\n        ``errorbar=False``.\n    rug : bool\n        Plot rug plot of values diverging or that reached the max tree depth.\n    rug_kind : bool\n        Variable in sample stats to use as rug mask. Must be a boolean variable.\n    n_points : int\n        Number of points for which to plot their quantile/local ess or number of subsets\n        in the evolution plot.\n    labeller : Labeller, optional\n        Class providing the method `make_label_vert` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax : 2D array-like of matplotlib_axes or bokeh_figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    rug_kwargs : dict\n        kwargs passed to rug plot in\n        :meth:`mpl:matplotlib.axes.Axes.plot` or :class:`bokeh:bokeh.models.glyphs.Scatter`.\n    extra_kwargs : dict, optional\n        kwargs passed as extra method lines in\n        :meth:`mpl:matplotlib.axes.Axes.axhline` or :class:`bokeh:bokeh.models.Span`\n    text_kwargs : dict, optional\n        kwargs passed to :meth:`mpl:matplotlib.axes.Axes.annotate` for extra methods lines labels.\n        It accepts the additional key ``x`` to set ``xy=(text_kwargs[\"x\"], mcse)``.\n        text_kwargs are ignored for the bokeh plotting backend.\n    backend : str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs : bool, optional\n        These are kwargs specific to the backend being passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :meth:`mpl:matplotlib.axes.Axes.hist` or\n        :meth:`mpl:matplotlib.axes.Axes.plot` in matplotlib depending on the value of `kind`.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    :func:`arviz.mcse`: Calculate Markov Chain Standard Error statistic.\n\n    References\n    ----------\n    * Vehtari et al. (2019) see https://arxiv.org/abs/1903.08008\n\n    Examples\n    --------\n    Plot quantile Monte Carlo Standard Error.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data(\"centered_eight\")\n        >>> coords = {\"school\": [\"Deerfield\", \"Lawrenceville\"]}\n        >>> az.plot_mcse(\n        ...     idata, var_names=[\"mu\", \"theta\"], coords=coords\n        ... )\n\n    \n source"},{"id":156,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_pair","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_pair-Tuple","content":" ArviZPythonPlots.plot_pair  —  Method \n    Plot a scatter, kde and/or hexbin matrix with (optional) marginals on the diagonal.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    group: str, optional\n        Specifies which InferenceData group should be plotted.  Defaults to 'posterior'.\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`.\n    marginals: bool, optional\n        If True pairplot will include marginal distributions for every variable\n    figsize: figure size tuple\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    kind : str or List[str]\n        Type of plot to display (scatter, kde and/or hexbin)\n    gridsize: int or (int, int), optional\n        Only works for ``kind=hexbin``. The number of hexagons in the x-direction.\n        The corresponding number of hexagons in the y-direction is chosen\n        such that the hexagons are approximately regular. Alternatively, gridsize\n        can be a tuple with two elements specifying the number of hexagons\n        in the x-direction and the y-direction.\n    divergences: Boolean\n        If True divergences will be plotted in a different color, only if group is either 'prior'\n        or 'posterior'.\n    colorbar: bool\n        If True a colorbar will be included as part of the plot (Defaults to False).\n        Only works when ``kind=hexbin``\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    divergences_kwargs: dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter` for divergences\n    scatter_kwargs:\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter` when using scatter kind\n    kde_kwargs: dict, optional\n        Additional keywords passed to :func:`arviz.plot_kde` when using kde kind\n    hexbin_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.hexbin` when\n        using hexbin kind\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    marginal_kwargs: dict, optional\n        Additional keywords passed to :func:`arviz.plot_dist`, modifying the\n        marginal distributions plotted in the diagonal.\n    point_estimate: str, optional\n        Select point estimate from 'mean', 'mode' or 'median'. The point estimate will be\n        plotted using a scatter marker and vertical/horizontal lines.\n    point_estimate_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.axvline`,\n        :meth:`matplotlib.axes.Axes.axhline` (matplotlib) or\n        :class:`bokeh:bokeh.models.Span` (bokeh)\n    point_estimate_marker_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.scatter`\n        or :meth:`bokeh:bokeh.plotting.Figure.square` in point\n        estimate plot. Not available in bokeh\n    reference_values: dict, optional\n        Reference values for the plotted variables. The Reference values will be plotted\n        using a scatter marker\n    reference_values_kwargs: dict, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.circle` in reference values plot\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    Examples\n    --------\n    KDE Pair Plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> coords = {'school': ['Choate', 'Deerfield']}\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu', 'tau'],\n        >>>             kind='kde',\n        >>>             coords=coords,\n        >>>             divergences=True,\n        >>>             textsize=18)\n\n    Hexbin pair plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu'],\n        >>>             coords=coords,\n        >>>             textsize=18,\n        >>>             kind='hexbin')\n\n    Pair plot showing divergences and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        ...             var_names=['^t', 'mu'],\n        ...             filter_vars=\"regex\",\n        ...             coords=coords,\n        ...             divergences=True,\n        ...             textsize=18)\n    \n source"},{"id":157,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_parallel","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_parallel-Tuple","content":" ArviZPythonPlots.plot_parallel  —  Method \n    Plot parallel coordinates plot showing posterior points with and without divergences.\n\n    Described by https://arxiv.org/abs/1709.01449\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names\n        Variables to be plotted, if `None` all variables are plotted. Can be used to change the\n        order of the plotted variables. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: mapping, optional\n        Coordinates of ``var_names`` to be plotted.\n        Passed to :meth:`xarray.Dataset.sel`.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    legend: bool\n        Flag for plotting legend (defaults to True)\n    colornd: valid matplotlib color\n        color for non-divergent points. Defaults to 'k'\n    colord: valid matplotlib color\n        color for divergent points. Defaults to 'C1'\n    shadend: float\n        Alpha blending value for non-divergent points, between 0 (invisible) and 1 (opaque).\n        Defaults to .025\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    norm_method: str\n        Method for normalizing the data. Methods include normal, minmax and rank.\n        Defaults to none.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes.\n        Defaults to value set in ``rcParams``.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_pair : Plot a scatter, kde and/or hexbin matrix with (optional) marginals on the diagonal.\n    plot_trace : Plot distribution (histogram or kernel density estimates) and sampled values\n                 or rank plot\n\n    Examples\n    --------\n    Plot default parallel plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_parallel(data, var_names=[\"mu\", \"tau\"])\n\n\n    Plot parallel plot with normalization\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"normal\")\n\n    Plot parallel plot with minmax\n\n    .. plot::\n        :context: close-figs\n\n        >>> ax = az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"minmax\")\n        >>> ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n\n    Plot parallel plot with rank\n\n    .. plot::\n        :context: close-figs\n\n        >>> ax = az.plot_parallel(data, var_names=[\"theta\", \"tau\", \"mu\"], norm_method=\"rank\")\n        >>> ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n    \n source"},{"id":158,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_posterior","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_posterior-Tuple","content":" ArviZPythonPlots.plot_posterior  —  Method Plot Posterior densities in the style of John K. Kruschke's book.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to the documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names\n        Variables to be plotted, two variables are required. Prefix the variables with ``~``\n        when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on ``figsize``.\n    hdi_prob: float, optional\n        Plots highest density interval for chosen percentage of density.\n        Use 'hide' to hide the highest density interval. Defaults to 0.94.\n    multimodal: bool\n        If true (default) it may compute more than one credible interval if the distribution is\n        multimodal and the modes are well separated.\n    skipna : bool\n        If true ignores nan values when computing the hdi and point estimates. Defaults to false.\n    round_to: int, optional\n        Controls formatting of floats. Defaults to 2 or the integer part, whichever is bigger.\n    point_estimate: Optional[str]\n        Plot point estimate per variable. Values should be 'mean', 'median', 'mode' or None.\n        Defaults to 'auto' i.e. it falls back to default set in rcParams.\n    group: str, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n    rope: tuple or dictionary of tuples\n        Lower and upper values of the Region Of Practical Equivalence. If a list is provided, its\n        length should match the number of variables.\n    ref_val: float or dictionary of floats\n        display the percentage below and above the values in ref_val. Must be None (default),\n        a constant, a list or a dictionary like see an example below. If a list is provided, its\n        length should match the number of variables.\n    rope_color: str, optional\n        Specifies the color of ROPE and displayed percentage within ROPE\n    ref_val_color: str, optional\n        Specifies the color of the displayed percentage\n    kind: str\n        Type of plot to display (kde or hist) For discrete variables this argument is ignored and\n        a histogram is always used. Defaults to rcParam ``plot.density_kind``\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when `circular` is False\n        and \"taylor\" (for now) when `circular` is True.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is. Only works if `kind == kde`.\n    circular: bool, optional\n        If True, it interprets the values passed are from a circular variable measured in radians\n        and a circular KDE is used. Only valid for 1D KDE. Defaults to False.\n        Only works if `kind == kde`.\n    bins: integer or sequence or 'auto', optional\n        Controls the number of bins,accepts the same keywords :func:`matplotlib.pyplot.hist` does.\n        Only works if `kind == hist`. If None (default) it will use `auto` for continuous variables\n        and `range(xmin, xmax + 1)` for discrete variables.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`\n    show: bool, optional\n        Call backend show function.\n    **kwargs\n        Passed as-is to :func:`matplotlib.pyplot.hist` or :func:`matplotlib.pyplot.plot` function\n        depending on the value of `kind`.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_dist : Plot distribution as histogram or kernel density estimates.\n    plot_density : Generate KDE plots for continuous variables and histograms for discrete ones.\n    plot_forest : Forest plot to compare HDI intervals from a number of distributions.\n\n    Examples\n    --------\n    Show a default kernel density plot following style of John Kruschke\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_posterior(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'])\n\n    Plot Region of Practical Equivalence (rope) and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', '^the'], filter_vars=\"regex\", rope=(-1, 1))\n\n    Plot Region of Practical Equivalence for selected distributions\n\n    .. plot::\n        :context: close-figs\n\n        >>> rope = {'mu': [{'rope': (-2, 2)}], 'theta': [{'school': 'Choate', 'rope': (2, 4)}]}\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], rope=rope)\n\n    Using `coords` argument to plot only a subset of data\n\n    .. plot::\n        :context: close-figs\n\n        >>> coords = {\"school\": [\"Choate\",\"Phillips Exeter\"]}\n        >>> az.plot_posterior(data, var_names=[\"mu\", \"theta\"], coords=coords)\n\n    Add reference lines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], ref_val=0)\n\n    Show point estimate of distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu', 'theta'], point_estimate='mode')\n\n    Show reference values using variable names and coordinates\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, ref_val= {\"theta\": [{\"school\": \"Deerfield\", \"ref_val\": 4},\n        ...                                             {\"school\": \"Choate\", \"ref_val\": 3}]})\n\n    Show reference values using a list\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, ref_val=[1] + [5] * 8 + [1])\n\n\n    Plot posterior as a histogram\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'], kind='hist')\n\n    Change size of highest density interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_posterior(data, var_names=['mu'], hdi_prob=.75)\n    \n source"},{"id":159,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_ppc","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_ppc-Tuple","content":" ArviZPythonPlots.plot_ppc  —  Method \n    Plot for posterior/prior predictive checks.\n\n    Parameters\n    ----------\n    data: az.InferenceData object\n        :class:`arviz.InferenceData` object containing the observed and posterior/prior\n        predictive data.\n    kind: str\n        Type of plot to display (\"kde\", \"cumulative\", or \"scatter\"). Defaults to `kde`.\n    alpha: float\n        Opacity of posterior/prior predictive density curves.\n        Defaults to 0.2 for ``kind = kde`` and cumulative, for scatter defaults to 0.7.\n    mean: bool\n        Whether or not to plot the mean posterior/prior predictive distribution.\n        Defaults to ``True``.\n    observed: bool, default True\n        Whether or not to plot the observed data.\n    observed: bool, default False\n        Whether or not to plot a rug plot for the observed data. Only valid if `observed` is\n        `True` and for kind `kde` or `cumulative`.\n    color: str\n        Valid matplotlib ``color``. Defaults to ``C0``.\n    color: list\n        List with valid matplotlib colors corresponding to the posterior/prior predictive\n        distribution, observed data and mean of the posterior/prior predictive distribution.\n        Defaults to [\"C0\", \"k\", \"C1\"].\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None, it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None, it will be\n        autoscaled based on ``figsize``.\n    data_pairs: dict\n        Dictionary containing relations between observed data and posterior/prior predictive data.\n        Dictionary structure:\n\n        - key = data var_name\n        - value = posterior/prior predictive var_name\n\n        For example, ``data_pairs = {'y' : 'y_hat'}``\n        If None, it will assume that the observed data and the posterior/prior\n        predictive data have the same variable name.\n    var_names: list of variable names\n        Variables to be plotted, if `None` all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None.\n    flatten: list\n        List of dimensions to flatten in ``observed_data``. Only flattens across the coordinates\n        specified in the ``coords`` argument. Defaults to flattening all of the dimensions.\n    flatten_pp: list\n        List of dimensions to flatten in posterior_predictive/prior_predictive. Only flattens\n        across the coordinates specified in the ``coords`` argument. Defaults to flattening all\n        of the dimensions. Dimensions should match flatten excluding dimensions for ``data_pairs``\n        parameters. If ``flatten`` is defined and ``flatten_pp`` is None, then\n        ``flatten_pp = flatten``.\n    num_pp_samples: int\n        The number of posterior/prior predictive samples to plot. For ``kind`` = 'scatter' and\n        ``animation = False`` if defaults to a maximum of 5 samples and will set jitter to 0.7.\n        unless defined. Otherwise it defaults to all provided samples.\n    random_seed: int\n        Random number generator seed passed to ``numpy.random.seed`` to allow\n        reproducibility of the plot. By default, no seed will be provided\n        and the plot will change each call if a random sample is specified\n        by ``num_pp_samples``.\n    jitter: float\n        If ``kind`` is \"scatter\", jitter will add random uniform noise to the height\n        of the ppc samples and observed data. By default 0.\n    animated: bool\n        Create an animation of one posterior/prior predictive sample per frame.\n        Defaults to ``False``. Only works with matploblib backend.\n        To run animations inside a notebook you have to use the `nbAgg` matplotlib's backend.\n        Try with `%matplotlib notebook` or  `%matplotlib  nbAgg`. You can switch back to the\n        default matplotlib's backend with `%matplotlib  inline` or `%matplotlib  auto`.\n        If switching back and forth between matplotlib's backend, you may need to run twice the cell\n        with the animation.\n        If you experience problems rendering the animation try setting\n        `animation_kwargs({'blit':False}`) or changing the matplotlib's backend (e.g. to TkAgg)\n        If you run the animation from a script write `ax, ani = az.plot_ppc(.)`\n    animation_kwargs : dict\n        Keywords passed to  :class:`matplotlib.animation.FuncAnimation`. Ignored with\n        matplotlib backend.\n    legend : bool\n        Add legend to figure. By default ``True``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_pp_label`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    group: {\"prior\", \"posterior\"}, optional\n        Specifies which InferenceData group should be plotted. Defaults to 'posterior'.\n        Other value can be 'prior'.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_bpv: Plot Bayesian p-value for observed data and Posterior/Prior predictive.\n    plot_lm: Posterior predictive and mean plots for regression-like data.\n    plot_ppc: plot for posterior/prior predictive checks.\n    plot_ts: Plot timeseries data.\n\n    Examples\n    --------\n    Plot the observed data KDE overlaid on posterior predictive KDEs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('radon')\n        >>> az.plot_ppc(data, data_pairs={\"y\":\"y\"})\n\n    Plot the overlay with empirical CDFs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='cumulative')\n\n    Use the ``coords`` and ``flatten`` parameters to plot selected variable dimensions\n    across multiple plots. We will now modify the dimension ``obs_id`` to contain\n    indicate the name of the county where the measure was taken. The change has to\n    be done on both ``posterior_predictive`` and ``observed_data`` groups, which is\n    why we will use :meth:`~arviz.InferenceData.map` to apply the same function to\n    both groups. Afterwards, we will select the counties to be plotted with the\n    ``coords`` arg.\n\n    .. plot::\n        :context: close-figs\n\n        >>> obs_county = data.posterior[\"County\"][data.constant_data[\"county_idx\"]]\n        >>> data = data.assign_coords(obs_id=obs_county, groups=\"observed_vars\")\n        >>> az.plot_ppc(data, coords={'obs_id': ['ANOKA', 'BELTRAMI']}, flatten=[])\n\n    Plot the overlay using a stacked scatter plot that is particularly useful\n    when the sample sizes are small.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='scatter', flatten=[],\n        >>>             coords={'obs_id': ['AITKIN', 'BELTRAMI']})\n\n    Plot random posterior predictive sub-samples.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, num_pp_samples=30, random_seed=7)\n    \n source"},{"id":160,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_rank","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_rank-Tuple","content":" ArviZPythonPlots.plot_rank  —  Method Plot rank order statistics of chains.\n\n    From the paper: Rank plots are histograms of the ranked posterior draws (ranked over all\n    chains) plotted separately for each chain.\n    If all of the chains are targeting the same posterior, we expect the ranks in each chain to be\n    uniform, whereas if one chain has a different location or scale parameter, this will be\n    reflected in the deviation from uniformity. If rank plots of all chains look similar, this\n    indicates good mixing of the chains.\n\n    This plot was introduced by Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter,\n    Paul-Christian Burkner (2019): Rank-normalization, folding, and localization: An improved R-hat\n    for assessing convergence of MCMC. arXiv preprint https://arxiv.org/abs/1903.08008\n\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object.\n        Refer to documentation of  :func:`arviz.convert_to_dataset` for details\n    var_names: string or list of variable names\n        Variables to be plotted. Prefix the variables by ``~`` when you want to exclude\n        them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e.the identity function)\n    coords: mapping, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    bins: None or passed to np.histogram\n        Binning strategy used for histogram. By default uses twice the result of Sturges' formula.\n        See :func:`numpy.histogram` documentation for, other available arguments.\n    kind: string\n        If bars (defaults), ranks are represented as stacked histograms (one per chain). If vlines\n        ranks are represented as vertical lines above or below ``ref_line``.\n    colors: string or list of strings\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplotlib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    ref_line: boolean\n        Whether to include a dashed line showing where a uniform distribution would lie\n    labels: bool\n        whether to plot or not the x and y labels, defaults to True\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, ArviZ will create\n        its own array of plot areas (and return it).\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    ref_line_kwargs : dict, optional\n        Reference line keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.axhline` or\n        :class:`bokeh:bokeh.models.Span`.\n    bar_kwargs : dict, optional\n        Bars keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.bar` or\n        :meth:`bokeh:bokeh.plotting.Figure.vbar`.\n    vlines_kwargs : dict, optional\n        Vlines keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.vlines` or\n        :meth:`bokeh:bokeh.plotting.Figure.multi_line`.\n    marker_vlines_kwargs : dict, optional\n        Marker for the vlines keyword arguments, passed to :meth:`mpl:matplotlib.axes.Axes.plot` or\n        :meth:`bokeh:bokeh.plotting.Figure.circle`.\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`. For additional documentation\n        check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_trace : Plot distribution (histogram or kernel density estimates) and\n                 sampled values or rank plot.\n\n    Examples\n    --------\n    Show a default rank plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_rank(data)\n\n    Recreate Figure 13 from the arxiv preprint\n\n    .. plot::\n        :context: close-figs\n\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_rank(data, var_names='tau')\n\n    Use vlines to compare results for centered vs noncentered models\n\n    .. plot::\n        :context: close-figs\n\n        >>> import matplotlib.pyplot as plt\n        >>> centered_data = az.load_arviz_data('centered_eight')\n        >>> noncentered_data = az.load_arviz_data('non_centered_eight')\n        >>> _, ax = plt.subplots(1, 2, figsize=(12, 3))\n        >>> az.plot_rank(centered_data, var_names=\"mu\", kind='vlines', ax=ax[0])\n        >>> az.plot_rank(noncentered_data, var_names=\"mu\", kind='vlines', ax=ax[1])\n\n    Change the aesthetics using kwargs\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_rank(noncentered_data, var_names=\"mu\", kind=\"vlines\",\n        >>>              vlines_kwargs={'lw':0}, marker_vlines_kwargs={'lw':3});\n    \n source"},{"id":161,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_separation","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_separation-Tuple","content":" ArviZPythonPlots.plot_separation  —  Method Separation plot for binary outcome models.\n\n    Model predictions are sorted and plotted using a color code according to\n    the observed data.\n\n    Parameters\n    ----------\n    idata : InferenceData\n        :class:`arviz.InferenceData` object.\n    y : array, DataArray or str\n        Observed data. If str, ``idata`` must be present and contain the observed data group\n    y_hat : array, DataArray or str\n        Posterior predictive samples for ``y``. It must have the same shape as ``y``. If str or\n        None, ``idata`` must contain the posterior predictive group.\n    y_hat_line : bool, optional\n        Plot the sorted ``y_hat`` predictions.\n    expected_events : bool, optional\n        Plot the total number of expected events.\n    figsize : figure size tuple, optional\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int, optional\n        Text size for labels. If None it will be autoscaled based on ``figsize``.\n    color : str, optional\n        Color to assign to the positive class. The negative class will be plotted using the\n        same color and an `alpha=0.3` transparency.\n    legend : bool, optional\n        Show the legend of the figure.\n    ax: axes, optional\n        Matplotlib axes or bokeh figures.\n    plot_kwargs : dict, optional\n        Additional keywords passed to :meth:`mpl:matplotlib.axes.Axes.bar` or\n        :meth:`bokeh:bokeh.plotting.Figure.vbar` for separation plot.\n    y_hat_line_kwargs : dict, optional\n        Additional keywords passed to ax.plot for ``y_hat`` line.\n    exp_events_kwargs : dict, optional\n        Additional keywords passed to ax.scatter for ``expected_events`` marker.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show : bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes : matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_ppc : Plot for posterior/prior predictive checks.\n\n    References\n    ----------\n    .. [1] Greenhill, B. *et al.*, The Separation Plot: A New Visual Method\n       for Evaluating the Fit of Binary Models, *American Journal of\n       Political Science*, (2011) see https://doi.org/10.1111/j.1540-5907.2011.00525.x\n\n    Examples\n    --------\n    Separation plot for a logistic regression model.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> idata = az.load_arviz_data('classification10d')\n        >>> az.plot_separation(idata=idata, y='outcome', y_hat='outcome', figsize=(8, 1))\n\n    \n source"},{"id":162,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_trace","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_trace-Tuple","content":" ArviZPythonPlots.plot_trace  —  Method Plot distribution (histogram or kernel density estimates) and sampled values or rank plot.\n\n    If `divergences` data is available in `sample_stats`, will plot the location of divergences as\n    dashed vertical lines.\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: str or list of str, optional\n        One or more variables to be plotted. Prefix the variables by ``~`` when you want\n        to exclude them from the plot.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    coords: dict of {str: slice or array_like}, optional\n        Coordinates of var_names to be plotted. Passed to :meth:`xarray.Dataset.sel`\n    divergences: {\"bottom\", \"top\", None}, optional\n        Plot location of divergences on the traceplots.\n    kind: {\"trace\", \"rank_bars\", \"rank_vlines\"}, optional\n        Choose between plotting sampled values per iteration and rank plots.\n    transform: callable, optional\n        Function to transform data (defaults to None i.e.the identity function)\n    figsize: tuple of (float, float), optional\n        If None, size is (12, variables * 2)\n    rug: bool, optional\n        If True adds a rugplot of samples. Defaults to False. Ignored for 2D KDE.\n        Only affects continuous variables.\n    lines: list of tuple of (str, dict, array_like), optional\n        List of (var_name, {'coord': selection}, [line, positions]) to be overplotted as\n        vertical lines on the density and horizontal lines on the trace.\n    circ_var_names : str or list of str, optional\n        List of circular variables to account for when plotting KDE.\n    circ_var_units : str\n        Whether the variables in ``circ_var_names`` are in \"degrees\" or \"radians\".\n    compact: bool, optional\n        Plot multidimensional variables in a single plot.\n    compact_prop: str or dict {str: array_like}, optional\n         Defines the property name and the property values to distinguish different\n        dimensions with compact=True.\n        When compact=True it defaults to color, it is\n        ignored otherwise.\n    combined: bool, optional\n        Flag for combining multiple chains into a single line. If False (default), chains will be\n        plotted separately.\n    chain_prop: str or dict {str: array_like}, optional\n        Defines the property name and the property values to distinguish different chains.\n        If compact=True it defaults to linestyle,\n        otherwise it uses the color to distinguish\n        different chains.\n    legend: bool, optional\n        Add a legend to the figure with the chain color code.\n    plot_kwargs, fill_kwargs, rug_kwargs, hist_kwargs: dict, optional\n        Extra keyword arguments passed to :func:`arviz.plot_dist`. Only affects continuous\n        variables.\n    trace_kwargs: dict, optional\n        Extra keyword arguments passed to :meth:`matplotlib.axes.Axes.plot`\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    rank_kwargs : dict, optional\n        Extra keyword arguments passed to :func:`arviz.plot_rank`\n    axes: axes, optional\n        Matplotlib axes or bokeh figures.\n    backend: {\"matplotlib\", \"bokeh\"}, optional\n        Select plotting backend.\n    backend_config: dict, optional\n        Currently specifies the bounds to use for bokeh axes. Defaults to value set in rcParams.\n    backend_kwargs: dict, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or\n        :func:`bokeh.plotting.figure`.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_rank : Plot rank order statistics of chains.\n\n    Examples\n    --------\n    Plot a subset variables and select them with partial naming\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('non_centered_eight')\n        >>> coords = {'school': ['Choate', 'Lawrenceville']}\n        >>> az.plot_trace(data, var_names=('theta'), filter_vars=\"like\", coords=coords)\n\n    Show all dimensions of multidimensional variables in the same plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(data, compact=True)\n\n    Display a rank plot instead of trace\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(data, var_names=[\"mu\", \"tau\"], kind=\"rank_bars\")\n\n    Combine all chains into one distribution and select variables with regular expressions\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_trace(\n        >>>     data, var_names=('^theta'), filter_vars=\"regex\", coords=coords, combined=True\n        >>> )\n\n\n    Plot reference lines against distribution and trace\n\n    .. plot::\n        :context: close-figs\n\n        >>> lines = (('theta_t',{'school': \"Choate\"}, [-1]),)\n        >>> az.plot_trace(data, var_names=('theta_t', 'theta'), coords=coords, lines=lines)\n\n    \n source"},{"id":163,"pagetitle":"Plotting functions","title":"ArviZPythonPlots.plot_violin","ref":"/ArviZPythonPlots/stable/api/plots/#ArviZPythonPlots.plot_violin-Tuple","content":" ArviZPythonPlots.plot_violin  —  Method Plot posterior of traces as violin plot.\n\n    Notes\n    -----\n    If multiple chains are provided for a variable they will be combined\n\n    Parameters\n    ----------\n    data: obj\n        Any object that can be converted to an :class:`arviz.InferenceData` object\n        Refer to documentation of :func:`arviz.convert_to_dataset` for details\n    var_names: list of variable names, optional\n        Variables to be plotted, if None all variable are plotted. Prefix the\n        variables by ``~`` when you want to exclude them from the plot.\n    combine_dims : set_like of str, optional\n        List of dimensions to reduce. Defaults to reducing only the \"chain\" and \"draw\" dimensions.\n        See the :ref:`this section <common_combine_dims>` for usage examples.\n    filter_vars: {None, \"like\", \"regex\"}, optional, default=None\n        If `None` (default), interpret var_names as the real variables names. If \"like\",\n        interpret var_names as substrings of the real variables names. If \"regex\",\n        interpret var_names as regular expressions on the real variables names. A la\n        ``pandas.filter``.\n    transform: callable\n        Function to transform data (defaults to None i.e. the identity function).\n    quartiles: bool, optional\n        Flag for plotting the interquartile range, in addition to the ``hdi_prob`` * 100%\n        intervals. Defaults to ``True``.\n    rug: bool\n        If ``True`` adds a jittered rugplot. Defaults to ``False``.\n    side : {\"both\", \"left\", \"right\"}, default \"both\"\n        If ``both``, both sides of the violin plot are rendered. If ``left`` or ``right``, only\n        the respective side is rendered. By separately plotting left and right halfs with\n        different data, split violin plots can be achieved.\n    hdi_prob: float, optional\n        Plots highest posterior density interval for chosen percentage of density.\n        Defaults to 0.94.\n    shade: float\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque). Defaults to 0.\n    bw: float or str, optional\n        If numeric, indicates the bandwidth and must be positive.\n        If str, indicates the method to estimate the bandwidth and must be\n        one of \"scott\", \"silverman\", \"isj\" or \"experimental\" when ``circular`` is ``False``\n        and \"taylor\" (for now) when ``circular`` is ``True``.\n        Defaults to \"default\" which means \"experimental\" when variable is not circular\n        and \"taylor\" when it is.\n    circular: bool, optional.\n        If ``True``, it interprets `values` is a circular variable measured in radians\n        and a circular KDE is used. Defaults to ``False``.\n    grid : tuple\n        Number of rows and columns. Defaults to None, the rows and columns are\n        automatically inferred.\n    figsize: tuple\n        Figure size. If None it will be defined automatically.\n    textsize: int\n        Text size of the point_estimates, axis ticks, and highest density interval. If None it will\n        be autoscaled based on ``figsize``.\n    labeller : labeller instance, optional\n        Class providing the method ``make_label_vert`` to generate the labels in the plot titles.\n        Read the :ref:`label_guide` for more details and usage examples.\n    sharex: bool\n        Defaults to ``True``, violinplots share a common x-axis scale.\n    sharey: bool\n        Defaults to ``True``, violinplots share a common y-axis scale.\n    ax: numpy array-like of matplotlib axes or bokeh figures, optional\n        A 2D array of locations into which to plot the densities. If not supplied, Arviz will create\n        its own array of plot areas (and return it).\n    shade_kwargs: dicts, optional\n        Additional keywords passed to :meth:`matplotlib.axes.Axes.fill_between`, or\n        :meth:`matplotlib.axes.Axes.barh` to control the shade.\n    rug_kwargs: dict\n        Keywords passed to the rug plot. If true only the right half side of the violin will be\n        plotted.\n    backend: str, optional\n        Select plotting backend {\"matplotlib\",\"bokeh\"}. Default to \"matplotlib\".\n    backend_kwargs: bool, optional\n        These are kwargs specific to the backend being used, passed to\n        :func:`matplotlib.pyplot.subplots` or :func:`bokeh.plotting.figure`.\n        For additional documentation check the plotting method of the backend.\n    show: bool, optional\n        Call backend show function.\n\n    Returns\n    -------\n    axes: matplotlib axes or bokeh figures\n\n    See Also\n    --------\n    plot_forest: Forest plot to compare HDI intervals from a number of distributions.\n\n    Examples\n    --------\n    Show a default violin plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_violin(data)\n\n    \n source"},{"id":166,"pagetitle":"rcParams","title":"rcParams","ref":"/ArviZPythonPlots/stable/api/rcparams/#rcparams-api","content":" rcParams ArviZPythonPlots.rcParams ArviZPythonPlots.rc_context"},{"id":167,"pagetitle":"rcParams","title":"Reference","ref":"/ArviZPythonPlots/stable/api/rcparams/#Reference","content":" Reference"},{"id":168,"pagetitle":"rcParams","title":"ArviZPythonPlots.rcParams","ref":"/ArviZPythonPlots/stable/api/rcparams/#ArviZPythonPlots.rcParams","content":" ArviZPythonPlots.rcParams  —  Constant Class to contain ArviZ default parameters.\n\n    It is implemented as a dict with validation when setting items.\n    \n source"},{"id":169,"pagetitle":"rcParams","title":"ArviZPythonPlots.rc_context","ref":"/ArviZPythonPlots/stable/api/rcparams/#ArviZPythonPlots.rc_context-Tuple","content":" ArviZPythonPlots.rc_context  —  Method \n    Return a context manager for managing rc settings.\n\n    Parameters\n    ----------\n    rc : dict, optional\n        Mapping containing the rcParams to modify temporally.\n    fname : str, optional\n        Filename of the file containing the rcParams to use inside the rc_context.\n\n    Examples\n    --------\n    This allows one to do::\n\n        with az.rc_context(fname='pystan.rc'):\n            idata = az.load_arviz_data(\"radon\")\n            az.plot_posterior(idata, var_names=[\"gamma\"])\n\n    The plot would have settings from 'screen.rc'\n\n    A dictionary can also be passed to the context manager::\n\n        with az.rc_context(rc={'plot.max_subplots': None}, fname='pystan.rc'):\n            idata = az.load_arviz_data(\"radon\")\n            az.plot_posterior(idata, var_names=[\"gamma\"])\n\n    The 'rc' dictionary takes precedence over the settings loaded from\n    'fname'. Passing a dictionary only is also valid.\n    \n source"},{"id":172,"pagetitle":"Plotting styles","title":"Plotting styles","ref":"/ArviZPythonPlots/stable/api/style/#style-api","content":" Plotting styles ArviZPythonPlots.styles ArviZPythonPlots.use_style"},{"id":173,"pagetitle":"Plotting styles","title":"Reference","ref":"/ArviZPythonPlots/stable/api/style/#Reference","content":" Reference"},{"id":174,"pagetitle":"Plotting styles","title":"ArviZPythonPlots.styles","ref":"/ArviZPythonPlots/stable/api/style/#ArviZPythonPlots.styles-Tuple{}","content":" ArviZPythonPlots.styles  —  Method styles() -> Vector{String} Get all available matplotlib styles for use with  use_style source"},{"id":175,"pagetitle":"Plotting styles","title":"ArviZPythonPlots.use_style","ref":"/ArviZPythonPlots/stable/api/style/#ArviZPythonPlots.use_style-Tuple{Any}","content":" ArviZPythonPlots.use_style  —  Method use_style(style::String)\nuse_style(style::Vector{String}) Use matplotlib style settings from a style specification  style . The style name of \"default\" is reserved for reverting back to the default style settings. ArviZ-specific styles include  [\"arviz-whitegrid\", \"arviz-darkgrid\", \"arviz-colors\", \"arviz-white\", \"arviz-doc\"] . To see all available style specifications, use  styles() . If a  Vector  of styles is provided, they are applied from first to last. source"},{"id":178,"pagetitle":"Examples gallery","title":"Example Gallery","ref":"/ArviZPythonPlots/stable/examples/#Example-Gallery","content":" Example Gallery"},{"id":179,"pagetitle":"Examples gallery","title":"Autocorrelation Plot","ref":"/ArviZPythonPlots/stable/examples/#Autocorrelation-Plot","content":" Autocorrelation Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_autocorr(data; var_names=[\"tau\", \"mu\"])\ngcf() See  plot_autocorr"},{"id":180,"pagetitle":"Examples gallery","title":"Bayes Factor Plot","ref":"/ArviZPythonPlots/stable/examples/#Bayes-Factor-Plot","content":" Bayes Factor Plot using ArviZ, ArviZPythonPlots\n\nuse_style(\"arviz-darkgrid\")\n\nidata = from_namedtuple((a = 1 .+ randn(5_000) ./ 2,), prior=(a = randn(5_000),))\nplot_bf(idata; var_name=\"a\", ref_val=0)\ngcf() See  plot_bf"},{"id":181,"pagetitle":"Examples gallery","title":"Bayesian P-Value Posterior Plot","ref":"/ArviZPythonPlots/stable/examples/#Bayesian-P-Value-Posterior-Plot","content":" Bayesian P-Value Posterior Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nplot_bpv(data)\ngcf() See  plot_bpv"},{"id":182,"pagetitle":"Examples gallery","title":"Bayesian P-Value with Median T Statistic Posterior Plot","ref":"/ArviZPythonPlots/stable/examples/#Bayesian-P-Value-with-Median-T-Statistic-Posterior-Plot","content":" Bayesian P-Value with Median T Statistic Posterior Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nplot_bpv(data; kind=\"t_stat\", t_stat=\"0.5\")\ngcf() See  plot_bpv"},{"id":183,"pagetitle":"Examples gallery","title":"Compare Plot","ref":"/ArviZPythonPlots/stable/examples/#Compare-Plot","content":" Compare Plot using ArviZ, ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nmodel_compare = compare(\n    (\n        var\"Centered 8 schools\" = load_example_data(\"centered_eight\"),\n        var\"Non-centered 8 schools\" = load_example_data(\"non_centered_eight\"),\n    ),\n)\nplot_compare(model_compare; figsize=(12, 4))\ngcf() See  compare ,  plot_compare"},{"id":184,"pagetitle":"Examples gallery","title":"Density Plot","ref":"/ArviZPythonPlots/stable/examples/#Density-Plot","content":" Density Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered_data = load_example_data(\"centered_eight\")\nnon_centered_data = load_example_data(\"non_centered_eight\")\nplot_density(\n    [centered_data, non_centered_data];\n    data_labels=[\"Centered\", \"Non Centered\"],\n    var_names=[\"theta\"],\n    shade=0.1,\n)\ngcf() See  plot_density"},{"id":185,"pagetitle":"Examples gallery","title":"Dist Plot","ref":"/ArviZPythonPlots/stable/examples/#Dist-Plot","content":" Dist Plot using ArviZPythonPlots, Distributions, Random\n\nRandom.seed!(308)\n\nuse_style(\"arviz-darkgrid\")\n\na = rand(Poisson(4), 1000)\nb = rand(Normal(0, 1), 1000)\n_, ax = subplots(1, 2; figsize=(10, 4))\nplot_dist(a; color=\"C1\", label=\"Poisson\", ax=ax[0])\nplot_dist(b; color=\"C2\", label=\"Gaussian\", ax=ax[1])\ngcf() See  plot_dist"},{"id":186,"pagetitle":"Examples gallery","title":"Dot Plot","ref":"/ArviZPythonPlots/stable/examples/#Dot-Plot","content":" Dot Plot using ArviZPythonPlots\n\nuse_style(\"arviz-darkgrid\")\n\ndata = randn(1000)\nplot_dot(data; dotcolor=\"C1\", point_interval=true)\ntitle(\"Gaussian Distribution\")\ngcf() See  plot_dot"},{"id":187,"pagetitle":"Examples gallery","title":"ECDF Plot","ref":"/ArviZPythonPlots/stable/examples/#ECDF-Plot","content":" ECDF Plot using ArviZPythonPlots, Distributions\n\nuse_style(\"arviz-darkgrid\")\n\nsample = randn(1_000)\ndist = Normal()\nplot_ecdf(sample; cdf=x -> cdf(dist, x), confidence_bands=true)\ngcf() See  plot_ecdf"},{"id":188,"pagetitle":"Examples gallery","title":"ELPD Plot","ref":"/ArviZPythonPlots/stable/examples/#ELPD-Plot","content":" ELPD Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nd1 = load_example_data(\"centered_eight\")\nd2 = load_example_data(\"non_centered_eight\")\nplot_elpd(Dict(\"Centered eight\" => d1, \"Non centered eight\" => d2); xlabels=true)\ngcf() See  plot_elpd"},{"id":189,"pagetitle":"Examples gallery","title":"Energy Plot","ref":"/ArviZPythonPlots/stable/examples/#Energy-Plot","content":" Energy Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_energy(data; figsize=(12, 8))\ngcf() See  plot_energy"},{"id":190,"pagetitle":"Examples gallery","title":"ESS Evolution Plot","ref":"/ArviZPythonPlots/stable/examples/#ESS-Evolution-Plot","content":" ESS Evolution Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nplot_ess(idata; var_names=[\"b\"], kind=\"evolution\")\ngcf() See  plot_ess"},{"id":191,"pagetitle":"Examples gallery","title":"ESS Local Plot","ref":"/ArviZPythonPlots/stable/examples/#ESS-Local-Plot","content":" ESS Local Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"non_centered_eight\")\nplot_ess(idata; var_names=[\"mu\"], kind=\"local\", marker=\"_\", ms=20, mew=2, rug=true)\ngcf() See  plot_ess"},{"id":192,"pagetitle":"Examples gallery","title":"ESS Quantile Plot","ref":"/ArviZPythonPlots/stable/examples/#ESS-Quantile-Plot","content":" ESS Quantile Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nplot_ess(idata; var_names=[\"sigma\"], kind=\"quantile\", color=\"C4\")\ngcf() See  plot_ess"},{"id":193,"pagetitle":"Examples gallery","title":"Forest Plot","ref":"/ArviZPythonPlots/stable/examples/#Forest-Plot","content":" Forest Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered_data = load_example_data(\"centered_eight\")\nnon_centered_data = load_example_data(\"non_centered_eight\")\nplot_forest(\n    [centered_data, non_centered_data];\n    model_names=[\"Centered\", \"Non Centered\"],\n    var_names=[\"mu\"],\n)\ntitle(\"Estimated theta for eight schools model\")\ngcf() See  plot_forest"},{"id":194,"pagetitle":"Examples gallery","title":"Ridge Plot","ref":"/ArviZPythonPlots/stable/examples/#Ridge-Plot","content":" Ridge Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nrugby_data = load_example_data(\"rugby\")\nplot_forest(\n    rugby_data;\n    kind=\"ridgeplot\",\n    var_names=[\"defs\"],\n    linewidth=4,\n    combined=true,\n    ridgeplot_overlap=1.5,\n    colors=\"blue\",\n    figsize=(9, 4),\n)\ntitle(\"Relative defensive strength\\nof Six Nation rugby teams\")\ngcf() See  plot_forest"},{"id":195,"pagetitle":"Examples gallery","title":"Plot HDI","ref":"/ArviZPythonPlots/stable/examples/#Plot-HDI","content":" Plot HDI using Random\nusing ArviZPythonPlots\n\nRandom.seed!(308)\n\nuse_style(\"arviz-darkgrid\")\n\nx_data = randn(100)\ny_data = 2 .+ x_data .* 0.5\ny_data_rep = 0.5 .* randn(200, 100) .+ transpose(y_data)\n\nplot(x_data, y_data; color=\"C6\")\nplot_hdi(x_data, y_data_rep; color=\"k\", plot_kwargs=Dict(\"ls\" => \"--\"))\ngcf() See  plot_hdi"},{"id":196,"pagetitle":"Examples gallery","title":"Joint Plot","ref":"/ArviZPythonPlots/stable/examples/#Joint-Plot","content":" Joint Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_pair(\n    data;\n    var_names=[\"theta\"],\n    coords=Dict(\"school\" => [\"Choate\", \"Phillips Andover\"]),\n    kind=\"hexbin\",\n    marginals=true,\n    figsize=(10, 10),\n)\ngcf() See  plot_pair"},{"id":197,"pagetitle":"Examples gallery","title":"KDE Plot","ref":"/ArviZPythonPlots/stable/examples/#KDE-Plot","content":" KDE Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\n\n## Combine different posterior draws from different chains\nobs = data.posterior_predictive.obs\nsize_obs = size(obs)\ny_hat = reshape(obs, prod(size_obs[1:2]), size_obs[3:end]...)\n\nplot_kde(\n    y_hat;\n    label=\"Estimated Effect\\n of SAT Prep\",\n    rug=true,\n    plot_kwargs=Dict(\"linewidth\" => 2, \"color\" => \"black\"),\n    rug_kwargs=Dict(\"color\" => \"black\"),\n)\ngcf() See  plot_kde"},{"id":198,"pagetitle":"Examples gallery","title":"2d KDE","ref":"/ArviZPythonPlots/stable/examples/#2d-KDE","content":" 2d KDE using Random\nusing ArviZPythonPlots\n\nRandom.seed!(308)\n\nuse_style(\"arviz-darkgrid\")\n\nplot_kde(rand(100), rand(100))\ngcf() See  plot_kde"},{"id":199,"pagetitle":"Examples gallery","title":"KDE Quantiles Plot","ref":"/ArviZPythonPlots/stable/examples/#KDE-Quantiles-Plot","content":" KDE Quantiles Plot using Random\nusing Distributions\nusing ArviZPythonPlots\n\nRandom.seed!(308)\n\nuse_style(\"arviz-darkgrid\")\n\ndist = rand(Beta(rand(Uniform(0.5, 10)), 5), 1000)\nplot_kde(dist; quantiles=[0.25, 0.5, 0.75])\ngcf() See  plot_kde"},{"id":200,"pagetitle":"Examples gallery","title":"Pareto Shape Plot","ref":"/ArviZPythonPlots/stable/examples/#Pareto-Shape-Plot","content":" Pareto Shape Plot using ArviZ, ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\nloo_data = loo(idata)\nplot_khat(loo_data; show_bins=true)\ngcf() See  loo ,  plot_khat"},{"id":201,"pagetitle":"Examples gallery","title":"LOO-PIT ECDF Plot","ref":"/ArviZPythonPlots/stable/examples/#LOO-PIT-ECDF-Plot","content":" LOO-PIT ECDF Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"radon\")\n\nplot_loo_pit(idata; y=\"y\", ecdf=true, color=\"maroon\")\ngcf() See  loo_pit ,  plot_loo_pit"},{"id":202,"pagetitle":"Examples gallery","title":"LOO-PIT Overlay Plot","ref":"/ArviZPythonPlots/stable/examples/#LOO-PIT-Overlay-Plot","content":" LOO-PIT Overlay Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\nidata = load_example_data(\"non_centered_eight\")\nplot_loo_pit(; idata, y=\"obs\", color=\"indigo\")\ngcf() See  loo_pit ,  plot_loo_pit"},{"id":203,"pagetitle":"Examples gallery","title":"Quantile Monte Carlo Standard Error Plot","ref":"/ArviZPythonPlots/stable/examples/#Quantile-Monte-Carlo-Standard-Error-Plot","content":" Quantile Monte Carlo Standard Error Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_mcse(data; var_names=[\"tau\", \"mu\"], rug=true, extra_methods=true)\ngcf() See  plot_mcse"},{"id":204,"pagetitle":"Examples gallery","title":"Quantile MCSE Errobar Plot","ref":"/ArviZPythonPlots/stable/examples/#Quantile-MCSE-Errobar-Plot","content":" Quantile MCSE Errobar Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"radon\")\nplot_mcse(data; var_names=[\"sigma_a\"], color=\"C4\", errorbar=true)\ngcf() See  plot_mcse"},{"id":205,"pagetitle":"Examples gallery","title":"Pair Plot","ref":"/ArviZPythonPlots/stable/examples/#Pair-Plot","content":" Pair Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered; var_names=[\"theta\", \"mu\", \"tau\"], coords, divergences=true, textsize=22\n)\ngcf() See  plot_pair"},{"id":206,"pagetitle":"Examples gallery","title":"Hexbin Pair Plot","ref":"/ArviZPythonPlots/stable/examples/#Hexbin-Pair-Plot","content":" Hexbin Pair Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[\"theta\", \"mu\", \"tau\"],\n    kind=\"hexbin\",\n    coords,\n    colorbar=true,\n    divergences=true,\n)\ngcf() See  plot_pair"},{"id":207,"pagetitle":"Examples gallery","title":"KDE Pair Plot","ref":"/ArviZPythonPlots/stable/examples/#KDE-Pair-Plot","content":" KDE Pair Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[\"theta\", \"mu\", \"tau\"],\n    kind=\"kde\",\n    coords,\n    divergences=true,\n    textsize=22,\n)\ngcf() See  plot_pair"},{"id":208,"pagetitle":"Examples gallery","title":"Point Estimate Pair Plot","ref":"/ArviZPythonPlots/stable/examples/#Point-Estimate-Pair-Plot","content":" Point Estimate Pair Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ncentered = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\", \"Deerfield\"])\nplot_pair(\n    centered;\n    var_names=[\"mu\", \"theta\"],\n    kind=[\"scatter\", \"kde\"],\n    kde_kwargs=Dict(\"fill_last\" => false),\n    marginals=true,\n    coords,\n    point_estimate=\"median\",\n    figsize=(10, 8),\n)\ngcf() See  plot_pair"},{"id":209,"pagetitle":"Examples gallery","title":"Parallel Plot","ref":"/ArviZPythonPlots/stable/examples/#Parallel-Plot","content":" Parallel Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nax = plot_parallel(data; var_names=[\"theta\", \"tau\", \"mu\"])\nax.set_xticklabels(ax.get_xticklabels(); rotation=70)\ndraw()\ngcf() See  plot_parallel"},{"id":210,"pagetitle":"Examples gallery","title":"Posterior Plot","ref":"/ArviZPythonPlots/stable/examples/#Posterior-Plot","content":" Posterior Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\ncoords = Dict(\"school\" => [\"Choate\"])\nplot_posterior(data; var_names=[\"mu\", \"theta\"], coords, rope=(-1, 1))\ngcf() See  plot_posterior"},{"id":211,"pagetitle":"Examples gallery","title":"Posterior Predictive Check Plot","ref":"/ArviZPythonPlots/stable/examples/#Posterior-Predictive-Check-Plot","content":" Posterior Predictive Check Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_ppc(data; data_pairs=Dict(\"obs\" => \"obs\"), alpha=0.03, figsize=(12, 6), textsize=14)\ngcf() See  plot_ppc"},{"id":212,"pagetitle":"Examples gallery","title":"Posterior Predictive Check Cumulative Plot","ref":"/ArviZPythonPlots/stable/examples/#Posterior-Predictive-Check-Cumulative-Plot","content":" Posterior Predictive Check Cumulative Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_ppc(data; alpha=0.3, kind=\"cumulative\", figsize=(12, 6), textsize=14)\ngcf() See  plot_ppc"},{"id":213,"pagetitle":"Examples gallery","title":"Rank Plot","ref":"/ArviZPythonPlots/stable/examples/#Rank-Plot","content":" Rank Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"centered_eight\")\nplot_rank(data; var_names=[\"tau\", \"mu\"])\ngcf() See  plot_rank"},{"id":214,"pagetitle":"Examples gallery","title":"Regression Plot","ref":"/ArviZPythonPlots/stable/examples/#Regression-Plot","content":" Regression Plot using ArviZ, ArviZPythonPlots, ArviZExampleData, DimensionalData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"regression1d\")\nx = range(0, 1; length=100)\nposterior = data.posterior\nconstant_data = convert_to_dataset((; x); default_dims=())\ny_model = broadcast_dims(muladd, posterior.intercept, posterior.slope, constant_data.x)\nposterior = merge(posterior, (; y_model))\ndata = merge(data, InferenceData(; posterior, constant_data))\nplot_lm(\"y\"; idata=data, x=\"x\", y_model=\"y_model\")\ngcf() See  plot_lm"},{"id":215,"pagetitle":"Examples gallery","title":"Separation Plot","ref":"/ArviZPythonPlots/stable/examples/#Separation-Plot","content":" Separation Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"classification10d\")\nplot_separation(data; y=\"outcome\", y_hat=\"outcome\", figsize=(8, 1))\ngcf() See  plot_separation"},{"id":216,"pagetitle":"Examples gallery","title":"Trace Plot","ref":"/ArviZPythonPlots/stable/examples/#Trace-Plot","content":" Trace Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_trace(data; var_names=[\"tau\", \"mu\"])\ngcf() See  plot_trace"},{"id":217,"pagetitle":"Examples gallery","title":"Violin Plot","ref":"/ArviZPythonPlots/stable/examples/#Violin-Plot","content":" Violin Plot using ArviZPythonPlots, ArviZExampleData\n\nuse_style(\"arviz-darkgrid\")\n\ndata = load_example_data(\"non_centered_eight\")\nplot_violin(data; var_names=[\"mu\", \"tau\"])\ngcf() See  plot_violin"},{"id":218,"pagetitle":"Examples gallery","title":"Styles","ref":"/ArviZPythonPlots/stable/examples/#Styles","content":" Styles using ArviZPythonPlots, Distributions, PythonCall\n\nx = range(0, 1; length=100)\ndist = pdf.(Beta(2, 5), x)\n\nstyle_list = [\n    \"default\",\n    [\"default\", \"arviz-colors\"],\n    \"arviz-darkgrid\",\n    \"arviz-whitegrid\",\n    \"arviz-white\",\n    \"arviz-grayscale\",\n    [\"arviz-white\", \"arviz-redish\"],\n    [\"arviz-white\", \"arviz-bluish\"],\n    [\"arviz-white\", \"arviz-orangish\"],\n    [\"arviz-white\", \"arviz-brownish\"],\n    [\"arviz-white\", \"arviz-purplish\"],\n    [\"arviz-white\", \"arviz-cyanish\"],\n    [\"arviz-white\", \"arviz-greenish\"],\n    [\"arviz-white\", \"arviz-royish\"],\n    [\"arviz-white\", \"arviz-viridish\"],\n    [\"arviz-white\", \"arviz-plasmish\"],\n    \"arviz-doc\",\n    \"arviz-docgrid\",\n]\n\nfig = figure(; figsize=(20, 10))\nfor (idx, style) in enumerate(style_list)\n    pywith(pyplot.style.context(style; after_reset=true)) do _\n        ax = fig.add_subplot(5, 4, idx; label=idx)\n        colors = pyplot.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n        for i in 0:(length(colors) - 1)\n            ax.plot(x, dist .- i, \"C$i\"; label=\"C$i\")\n        end\n        ax.set_title(style)\n        ax.set_xlabel(\"x\")\n        ax.set_ylabel(\"f(x)\"; rotation=0, labelpad=15)\n        ax.set_xticklabels([])\n    end\nend\ntight_layout()\ngcf()"},{"id":221,"pagetitle":"Home","title":"PosteriorStats","ref":"/PosteriorStats/stable/#PosteriorStats","content":" PosteriorStats PosteriorStats implements widely-used and well-characterized statistical analyses for the Bayesian workflow. These functions generally estimate properties of posterior and/or posterior predictive distributions. The default implementations defined here operate on Monte Carlo samples. See the  API  for details."},{"id":222,"pagetitle":"Home","title":"Extending this package","ref":"/PosteriorStats/stable/#Extending-this-package","content":" Extending this package The methods defined here are intended to be extended by two types of packages. packages that implement data types for storing Monte Carlo samples packages that implement other representations for posterior distributions than Monte Carlo draws"},{"id":225,"pagetitle":"API","title":"API","ref":"/PosteriorStats/stable/api/#API","content":" API"},{"id":226,"pagetitle":"API","title":"Summary statistics","ref":"/PosteriorStats/stable/api/#Summary-statistics","content":" Summary statistics"},{"id":227,"pagetitle":"API","title":"PosteriorStats.SummaryStats","ref":"/PosteriorStats/stable/api/#PosteriorStats.SummaryStats","content":" PosteriorStats.SummaryStats  —  Type struct SummaryStats{D, V<:(AbstractVector)} A container for a column table of values computed by  summarize . This object implements the Tables and TableTraits column table interfaces. It has a custom  show  method. SummaryStats  behaves like an  OrderedDict  of columns, where the columns can be accessed using either  Symbol s or a 1-based integer index. name::String : The name of the collection of summary statistics, used as the table title in display. data::Any : The summary statistics for each parameter. It must implement the Tables interface. parameter_names::AbstractVector : Names of the parameters SummaryStats([name::String,] data[, parameter_names])\nSummaryStats(data[, parameter_names]; name::String=\"SummaryStats\") Construct a  SummaryStats  from tabular  data  with optional stats  name  and  param_names . data  must not contain a column  :parameter , as this is reserved for the parameter names, which are always in the first column. source"},{"id":228,"pagetitle":"API","title":"PosteriorStats.default_diagnostics","ref":"/PosteriorStats/stable/api/#PosteriorStats.default_diagnostics","content":" PosteriorStats.default_diagnostics  —  Function default_diagnostics(focus=Statistics.mean; kwargs...) Default diagnostics to be computed with  summarize . The value of  focus  determines the diagnostics to be returned: Statistics.mean :  mcse_mean ,  mcse_std ,  ess_tail ,  ess_bulk ,  rhat Statistics.median :  mcse_median ,  ess_tail ,  ess_bulk ,  rhat source"},{"id":229,"pagetitle":"API","title":"PosteriorStats.default_stats","ref":"/PosteriorStats/stable/api/#PosteriorStats.default_stats","content":" PosteriorStats.default_stats  —  Function default_stats(focus=Statistics.mean; prob_interval=0.94, kwargs...) Default statistics to be computed with  summarize . The value of  focus  determines the statistics to be returned: Statistics.mean :  mean ,  std ,  hdi_3% ,  hdi_97% Statistics.median :  median ,  mad ,  eti_3% ,  eti_97% If  prob_interval  is set to a different value than the default, then different HDI and ETI statistics are computed accordingly.  hdi  refers to the highest-density interval, while  eti  refers to the equal-tailed interval (i.e. the credible interval computed from symmetric quantiles). See also:  hdi source"},{"id":230,"pagetitle":"API","title":"PosteriorStats.default_summary_stats","ref":"/PosteriorStats/stable/api/#PosteriorStats.default_summary_stats","content":" PosteriorStats.default_summary_stats  —  Function default_summary_stats(focus=Statistics.mean; kwargs...) Combinatiton of  default_stats  and  default_diagnostics  to be used with  summarize . source"},{"id":231,"pagetitle":"API","title":"PosteriorStats.summarize","ref":"/PosteriorStats/stable/api/#PosteriorStats.summarize","content":" PosteriorStats.summarize  —  Function summarize(data, stats_funs...; name=\"SummaryStats\", [var_names]) -> SummaryStats Compute the summary statistics in  stats_funs  on each param in  data . stats_funs  is a collection of functions that reduces a matrix with shape  (draws, chains)  to a scalar or a collection of scalars. Alternatively, an item in  stats_funs  may be a  Pair  of the form  name => fun  specifying the name to be used for the statistic or of the form  (name1, ...) => fun  when the function returns a collection. When the function returns a collection, the names in this latter format must be provided. If no stats functions are provided, then those specified in  default_summary_stats  are computed. var_names  specifies the names of the parameters in  data . If not provided, the names are inferred from  data . To support computing summary statistics from a custom object, overload this method specifying the type of  data . See also  SummaryStats ,  default_summary_stats ,  default_stats ,  default_diagnostics . Examples Compute  mean ,  std  and the Monte Carlo standard error (MCSE) of the mean estimate: julia> using Statistics, StatsBase\n\njulia> x = randn(1000, 4, 3) .+ reshape(0:10:20, 1, 1, :);\n\njulia> summarize(x, mean, std, :mcse_mean => sem; name=\"Mean/Std\")\nMean/Std\n       mean    std  mcse_mean\n 1   0.0003  0.990      0.016\n 2  10.02    0.988      0.016\n 3  19.98    0.988      0.016 Avoid recomputing the mean by using  mean_and_std , and provide parameter names: julia> summarize(x, (:mean, :std) => mean_and_std, mad; var_names=[:a, :b, :c])\nSummaryStats\n         mean    std    mad\n a   0.000305  0.990  0.978\n b  10.0       0.988  0.995\n c  20.0       0.988  0.979 Note that when an estimator and its MCSE are both computed, the MCSE is used to determine the number of significant digits that will be displayed. julia> summarize(x; var_names=[:a, :b, :c])\nSummaryStats\n       mean   std  hdi_3%  hdi_97%  mcse_mean  mcse_std  ess_tail  ess_bulk  r ⋯\n a   0.0003  0.99   -1.92     1.78      0.016     0.012      3567      3663  1 ⋯\n b  10.02    0.99    8.17    11.9       0.016     0.011      3841      3906  1 ⋯\n c  19.98    0.99   18.1     21.9       0.016     0.012      3892      3749  1 ⋯\n                                                                1 column omitted Compute just the statistics with an 89% HDI on all parameters, and provide the parameter names: julia> summarize(x, default_stats(; prob_interval=0.89)...; var_names=[:a, :b, :c])\nSummaryStats\n         mean    std  hdi_5.5%  hdi_94.5%\n a   0.000305  0.990     -1.63       1.52\n b  10.0       0.988      8.53      11.6\n c  20.0       0.988     18.5       21.6 Compute the summary stats focusing on  Statistics.median : julia> summarize(x, default_summary_stats(median)...; var_names=[:a, :b, :c])\nSummaryStats\n    median    mad  eti_3%  eti_97%  mcse_median  ess_tail  ess_median  rhat\n a   0.004  0.978   -1.83     1.89        0.020      3567        3336  1.00\n b  10.02   0.995    8.17    11.9         0.023      3841        3787  1.00\n c  19.99   0.979   18.1     21.9         0.020      3892        3829  1.00 source"},{"id":232,"pagetitle":"API","title":"General statistics","ref":"/PosteriorStats/stable/api/#General-statistics","content":" General statistics"},{"id":233,"pagetitle":"API","title":"PosteriorStats.hdi","ref":"/PosteriorStats/stable/api/#PosteriorStats.hdi","content":" PosteriorStats.hdi  —  Function hdi(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) Estimate the unimodal highest density interval (HDI) of  samples  for the probability  prob . The HDI is the minimum width Bayesian credible interval (BCI). That is, it is the smallest possible interval containing  (100*prob) % of the probability mass. [Hyndman1996] samples  is an array of shape  (draws[, chains[, params...]]) . If multiple parameters are present, then  lower  and  upper  are arrays with the shape  (params...,) , computed separately for each marginal. This implementation uses the algorithm of  [ChenShao1999] . Note Any default value of  prob  is arbitrary. The default value of  prob=0.94  instead of a more common default like  prob=0.95  is chosen to reminder the user of this arbitrariness. Examples Here we calculate the 83% HDI for a normal random variable: julia> x = randn(2_000);\n\njulia> hdi(x; prob=0.83) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => -1.38266\n  :upper => 1.25982 We can also calculate the HDI for a 3-dimensional array of samples: julia> x = randn(1_000, 1, 1) .+ reshape(0:5:10, 1, 1, :);\n\njulia> hdi(x) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :lower => [-1.9674, 3.0326, 8.0326]\n  :upper => [1.90028, 6.90028, 11.9003] source"},{"id":234,"pagetitle":"API","title":"PosteriorStats.hdi!","ref":"/PosteriorStats/stable/api/#PosteriorStats.hdi!","content":" PosteriorStats.hdi!  —  Function hdi!(samples::AbstractArray{<:Real}; prob=0.94) -> (; lower, upper) A version of  hdi  that sorts  samples  in-place while computing the HDI. source"},{"id":235,"pagetitle":"API","title":"LOO and WAIC","ref":"/PosteriorStats/stable/api/#LOO-and-WAIC","content":" LOO and WAIC"},{"id":236,"pagetitle":"API","title":"PosteriorStats.AbstractELPDResult","ref":"/PosteriorStats/stable/api/#PosteriorStats.AbstractELPDResult","content":" PosteriorStats.AbstractELPDResult  —  Type abstract type AbstractELPDResult An abstract type representing the result of an ELPD computation. Every subtype stores estimates of both the expected log predictive density ( elpd ) and the effective number of parameters  p , as well as standard errors and pointwise estimates of each, from which other relevant estimates can be computed. Subtypes implement the following functions: elpd_estimates information_criterion source"},{"id":237,"pagetitle":"API","title":"PosteriorStats.PSISLOOResult","ref":"/PosteriorStats/stable/api/#PosteriorStats.PSISLOOResult","content":" PosteriorStats.PSISLOOResult  —  Type Results of Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO). See also:  loo ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates psis_result : Pareto-smoothed importance sampling (PSIS) results source"},{"id":238,"pagetitle":"API","title":"PosteriorStats.WAICResult","ref":"/PosteriorStats/stable/api/#PosteriorStats.WAICResult","content":" PosteriorStats.WAICResult  —  Type Results of computing the widely applicable information criterion (WAIC). See also:  waic ,  AbstractELPDResult estimates : Estimates of the expected log pointwise predictive density (ELPD) and effective number of parameters (p) pointwise : Pointwise estimates source"},{"id":239,"pagetitle":"API","title":"PosteriorStats.elpd_estimates","ref":"/PosteriorStats/stable/api/#PosteriorStats.elpd_estimates","content":" PosteriorStats.elpd_estimates  —  Function elpd_estimates(result::AbstractELPDResult; pointwise=false) -> (; elpd, elpd_mcse, lpd) Return the (E)LPD estimates from the  result . source"},{"id":240,"pagetitle":"API","title":"PosteriorStats.information_criterion","ref":"/PosteriorStats/stable/api/#PosteriorStats.information_criterion","content":" PosteriorStats.information_criterion  —  Function information_criterion(elpd, scale::Symbol) Compute the information criterion for the given  scale  from the  elpd  estimate. scale  must be one of  (:deviance, :log, :negative_log) . See also:  loo ,  waic source information_criterion(result::AbstractELPDResult, scale::Symbol; pointwise=false) Compute information criterion for the given  scale  from the existing ELPD  result . scale  must be one of  (:deviance, :log, :negative_log) . If  pointwise=true , then pointwise estimates are returned. source"},{"id":241,"pagetitle":"API","title":"PosteriorStats.loo","ref":"/PosteriorStats/stable/api/#PosteriorStats.loo","content":" PosteriorStats.loo  —  Function loo(log_likelihood; reff=nothing, kwargs...) -> PSISLOOResult{<:NamedTuple,<:NamedTuple} Compute the Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO).  [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . Keywords reff::Union{Real,AbstractArray{<:Real}} : The relative effective sample size(s) of the  likelihood  values. If an array, it must have the same data dimensions as the corresponding log-likelihood variable. If not provided, then this is estimated using  MCMCDiagnosticTools.ess . kwargs : Remaining keywords are forwarded to [ PSIS.psis ]. See also:  PSISLOOResult ,  waic Examples Manually compute  $R_\\mathrm{eff}$  and calculate PSIS-LOO of a model: julia> using ArviZExampleData, MCMCDiagnosticTools\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> reff = ess(log_like; kind=:basic, split_chains=1, relative=true);\n\njulia> loo(log_like; reff)\nPSISLOOResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.34\n\nand PSISResult with 500 draws, 4 chains, and 8 parameters\nPareto shape (k) diagnostic values:\n                    Count      Min. ESS\n (-Inf, 0.5]  good  7 (87.5%)  151\n  (0.5, 0.7]  okay  1 (12.5%)  446 source"},{"id":242,"pagetitle":"API","title":"PosteriorStats.waic","ref":"/PosteriorStats/stable/api/#PosteriorStats.waic","content":" PosteriorStats.waic  —  Function waic(log_likelihood::AbstractArray) -> WAICResult{<:NamedTuple,<:NamedTuple} Compute the widely applicable information criterion (WAIC). [Watanabe2010] [Vehtari2017] [LOOFAQ] log_likelihood  must be an array of log-likelihood values with shape  (chains, draws[, params...]) . See also:  WAICResult ,  loo Examples Calculate WAIC of a model: julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> waic(log_like)\nWAICResult with estimates\n elpd  elpd_mcse    p  p_mcse\n  -31        1.4  0.9    0.33 source"},{"id":243,"pagetitle":"API","title":"Model comparison","ref":"/PosteriorStats/stable/api/#Model-comparison","content":" Model comparison"},{"id":244,"pagetitle":"API","title":"PosteriorStats.ModelComparisonResult","ref":"/PosteriorStats/stable/api/#PosteriorStats.ModelComparisonResult","content":" PosteriorStats.ModelComparisonResult  —  Type ModelComparisonResult Result of model comparison using ELPD. This struct implements the Tables and TableTraits interfaces. Each field returns a collection of the corresponding entry for each model: name : Names of the models, if provided. rank : Ranks of the models (ordered by decreasing ELPD) elpd_diff : ELPD of a model subtracted from the largest ELPD of any model elpd_diff_mcse : Monte Carlo standard error of the ELPD difference weight : Model weights computed with  weights_method elpd_result :  AbstactELPDResult s for each model, which can be used to access useful stats like ELPD estimates, pointwise estimates, and Pareto shape values for PSIS-LOO weights_method : Method used to compute model weights with  model_weights source"},{"id":245,"pagetitle":"API","title":"PosteriorStats.compare","ref":"/PosteriorStats/stable/api/#PosteriorStats.compare","content":" PosteriorStats.compare  —  Function compare(models; kwargs...) -> ModelComparisonResult Compare models based on their expected log pointwise predictive density (ELPD). The ELPD is estimated either by Pareto smoothed importance sampling leave-one-out cross-validation (LOO) or using the widely applicable information criterion (WAIC). We recommend loo. Read more theory here - in a paper by some of the leading authorities on model comparison dx.doi.org/10.1111/1467-9868.00353 Arguments models : a  Tuple ,  NamedTuple , or  AbstractVector  whose values are either  AbstractELPDResult  entries or any argument to  elpd_method . Keywords weights_method::AbstractModelWeightsMethod=Stacking() : the method to be used to weight the models. See  model_weights  for details elpd_method=loo : a method that computes an  AbstractELPDResult  from an argument in  models . sort::Bool=true : Whether to sort models by decreasing ELPD. Returns ModelComparisonResult : A container for the model comparison results. The fields contain a similar collection to  models . Examples Compare the centered and non centered models of the eight school problem using the defaults:  loo  and  Stacking  weights. A custom  myloo  method formates the inputs as expected by  loo . julia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> function myloo(idata)\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n\njulia> mc = compare(models; elpd_method=myloo)\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\nModelComparisonResult with Stacking weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0       1.0  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067     0.0  0.9   ⋯\n                                                                1 column omitted\njulia> mc.weight |> pairs\npairs(::NamedTuple) with 2 entries:\n  :non_centered => 1.0\n  :centered     => 5.34175e-19 Compare the same models from pre-computed PSIS-LOO results and computing  BootstrappedPseudoBMA  weights: julia> elpd_results = mc.elpd_result;\n\njulia> compare(elpd_results; weights_method=BootstrappedPseudoBMA())\nModelComparisonResult with BootstrappedPseudoBMA weights\n               rank  elpd  elpd_mcse  elpd_diff  elpd_diff_mcse  weight    p   ⋯\n non_centered     1   -31        1.4       0              0.0      0.52  0.9   ⋯\n centered         2   -31        1.4       0.06           0.067    0.48  0.9   ⋯\n                                                                1 column omitted source"},{"id":246,"pagetitle":"API","title":"PosteriorStats.model_weights","ref":"/PosteriorStats/stable/api/#PosteriorStats.model_weights","content":" PosteriorStats.model_weights  —  Function model_weights(elpd_results; method=Stacking())\nmodel_weights(method::AbstractModelWeightsMethod, elpd_results) Compute weights for each model in  elpd_results  using  method . elpd_results  is a  Tuple ,  NamedTuple , or  AbstractVector  with  AbstractELPDResult  entries. The weights are returned in the same type of collection. Stacking  is the recommended approach, as it performs well even when the true data generating process is not included among the candidate models. See  [YaoVehtari2018]  for details. See also:  AbstractModelWeightsMethod ,  compare Examples Compute  Stacking  weights for two models: julia> using ArviZExampleData\n\njulia> models = (\n           centered=load_example_data(\"centered_eight\"),\n           non_centered=load_example_data(\"non_centered_eight\"),\n       );\n\njulia> elpd_results = map(models) do idata\n           log_like = PermutedDimsArray(idata.log_likelihood.obs, (2, 3, 1))\n           return loo(log_like)\n       end;\n┌ Warning: 1 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/.julia/packages/PSIS/...\n\njulia> model_weights(elpd_results; method=Stacking()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 5.34175e-19\n  :non_centered => 1.0 Now we compute  BootstrappedPseudoBMA  weights for the same models: julia> model_weights(elpd_results; method=BootstrappedPseudoBMA()) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :centered     => 0.483723\n  :non_centered => 0.516277 source The following model weighting methods are available"},{"id":247,"pagetitle":"API","title":"PosteriorStats.AbstractModelWeightsMethod","ref":"/PosteriorStats/stable/api/#PosteriorStats.AbstractModelWeightsMethod","content":" PosteriorStats.AbstractModelWeightsMethod  —  Type abstract type AbstractModelWeightsMethod An abstract type representing methods for computing model weights. Subtypes implement  model_weights (method, elpd_results) . source"},{"id":248,"pagetitle":"API","title":"PosteriorStats.BootstrappedPseudoBMA","ref":"/PosteriorStats/stable/api/#PosteriorStats.BootstrappedPseudoBMA","content":" PosteriorStats.BootstrappedPseudoBMA  —  Type struct BootstrappedPseudoBMA{R<:Random.AbstractRNG, T<:Real} <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging using Akaike-type weighting with the Bayesian bootstrap (pseudo-BMA+) [YaoVehtari2018] . The Bayesian bootstrap stabilizes the model weights. BootstrappedPseudoBMA(; rng=Random.default_rng(), samples=1_000, alpha=1)\nBootstrappedPseudoBMA(rng, samples, alpha) Construct the method. rng::Random.AbstractRNG : The random number generator to use for the Bayesian bootstrap samples::Int64 : The number of samples to draw for bootstrapping alpha::Real : The shape parameter in the Dirichlet distribution used for the Bayesian bootstrap. The default (1) corresponds to a uniform distribution on the simplex. See also:  Stacking source"},{"id":249,"pagetitle":"API","title":"PosteriorStats.PseudoBMA","ref":"/PosteriorStats/stable/api/#PosteriorStats.PseudoBMA","content":" PosteriorStats.PseudoBMA  —  Type struct PseudoBMA <: AbstractModelWeightsMethod Model weighting method using pseudo Bayesian Model Averaging (pseudo-BMA) and Akaike-type weighting. PseudoBMA(; regularize=false)\nPseudoBMA(regularize) Construct the method with optional regularization of the weights using the standard error of the ELPD estimate. Note This approach is not recommended, as it produces unstable weight estimates. It is recommended to instead use  BootstrappedPseudoBMA  to stabilize the weights or  Stacking . For details, see  [YaoVehtari2018] . See also:  Stacking source"},{"id":250,"pagetitle":"API","title":"PosteriorStats.Stacking","ref":"/PosteriorStats/stable/api/#PosteriorStats.Stacking","content":" PosteriorStats.Stacking  —  Type struct Stacking{O<:Optim.AbstractOptimizer} <: AbstractModelWeightsMethod Model weighting using stacking of predictive distributions [YaoVehtari2018] . Stacking(; optimizer=Optim.LBFGS(), options=Optim.Options()\nStacking(optimizer[, options]) Construct the method, optionally customizing the optimization. optimizer::Optim.AbstractOptimizer : The optimizer to use for the optimization of the weights. The optimizer must support projected gradient optimization via a  manifold  field. options::Optim.Options : The Optim options to use for the optimization of the weights. See also:  BootstrappedPseudoBMA source"},{"id":251,"pagetitle":"API","title":"Predictive checks","ref":"/PosteriorStats/stable/api/#Predictive-checks","content":" Predictive checks"},{"id":252,"pagetitle":"API","title":"PosteriorStats.loo_pit","ref":"/PosteriorStats/stable/api/#PosteriorStats.loo_pit","content":" PosteriorStats.loo_pit  —  Function loo_pit(y, y_pred, log_weights; kwargs...) -> Union{Real,AbstractArray} Compute leave-one-out probability integral transform (LOO-PIT) checks. Arguments y : array of observations with shape  (params...,) y_pred : array of posterior predictive samples with shape  (draws, chains, params...) . log_weights : array of normalized log LOO importance weights with shape  (draws, chains, params...) . Keywords is_discrete : If not provided, then it is set to  true  iff elements of  y  and  y_pred  are all integer-valued. If  true , then data are smoothed using  smooth_data  to make them non-discrete before estimating LOO-PIT values. kwargs : Remaining keywords are forwarded to  smooth_data  if data is discrete. Returns pitvals : LOO-PIT values with same size as  y . If  y  is a scalar, then  pitvals  is a scalar. LOO-PIT is a marginal posterior predictive check. If  $y_{-i}$  is the array  $y$  of observations with the  $i$ th observation left out, and  $y_i^*$  is a posterior prediction of the  $i$ th observation, then the LOO-PIT value for the  $i$ th observation is defined as \\[P(y_i^* \\le y_i \\mid y_{-i}) = \\int_{-\\infty}^{y_i} p(y_i^* \\mid y_{-i}) \\mathrm{d} y_i^*\\] The LOO posterior predictions and the corresponding observations should have similar distributions, so if conditional predictive distributions are well-calibrated, then all LOO-PIT values should be approximately uniformly distributed on  $[0, 1]$ . [Gabry2019] Examples Calculate LOO-PIT values using as test quantity the observed values themselves. julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"centered_eight\");\n\njulia> y = idata.observed_data.obs;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.obs, (:draw, :chain, :school));\n\njulia> log_like = PermutedDimsArray(idata.log_likelihood.obs, (:draw, :chain, :school));\n\njulia> log_weights = loo(log_like).psis_result.log_weights;\n\njulia> loo_pit(y, y_pred, log_weights)\n╭───────────────────────────────╮\n│ 8-element DimArray{Float64,1} │\n├───────────────────────────────┴──────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n└──────────────────────────────────────────────────────────────────────────────┘\n \"Choate\"            0.943511\n \"Deerfield\"         0.63797\n \"Phillips Andover\"  0.316697\n \"Phillips Exeter\"   0.582252\n \"Hotchkiss\"         0.295321\n \"Lawrenceville\"     0.403318\n \"St. Paul's\"        0.902508\n \"Mt. Hermon\"        0.655275 Calculate LOO-PIT values using as test quantity the square of the difference between each observation and  mu . julia> using Statistics\n\njulia> mu = idata.posterior.mu;\n\njulia> T = y .- median(mu);\n\njulia> T_pred = y_pred .- mu;\n\njulia> loo_pit(T .^ 2, T_pred .^ 2, log_weights)\n╭───────────────────────────────╮\n│ 8-element DimArray{Float64,1} │\n├───────────────────────────────┴──────────────────────────────────────── dims ┐\n  ↓ school Categorical{String} [Choate, Deerfield, …, St. Paul's, Mt. Hermon] Unordered\n└──────────────────────────────────────────────────────────────────────────────┘\n \"Choate\"            0.873577\n \"Deerfield\"         0.243686\n \"Phillips Andover\"  0.357563\n \"Phillips Exeter\"   0.149908\n \"Hotchkiss\"         0.435094\n \"Lawrenceville\"     0.220627\n \"St. Paul's\"        0.775086\n \"Mt. Hermon\"        0.296706 source"},{"id":253,"pagetitle":"API","title":"PosteriorStats.r2_score","ref":"/PosteriorStats/stable/api/#PosteriorStats.r2_score","content":" PosteriorStats.r2_score  —  Function r2_score(y_true::AbstractVector, y_pred::AbstractArray) -> (; r2, r2_std) $R²$  for linear Bayesian regression models. [GelmanGoodrich2019] Arguments y_true : Observed data of length  noutputs y_pred : Predicted data with size  (ndraws[, nchains], noutputs) Examples julia> using ArviZExampleData\n\njulia> idata = load_example_data(\"regression1d\");\n\njulia> y_true = idata.observed_data.y;\n\njulia> y_pred = PermutedDimsArray(idata.posterior_predictive.y, (:draw, :chain, :y_dim_0));\n\njulia> r2_score(y_true, y_pred) |> pairs\npairs(::NamedTuple) with 2 entries:\n  :r2     => 0.683197\n  :r2_std => 0.0368838 source"},{"id":254,"pagetitle":"API","title":"Utilities","ref":"/PosteriorStats/stable/api/#Utilities","content":" Utilities"},{"id":255,"pagetitle":"API","title":"PosteriorStats.smooth_data","ref":"/PosteriorStats/stable/api/#PosteriorStats.smooth_data","content":" PosteriorStats.smooth_data  —  Function smooth_data(y; dims=:, interp_method=CubicSpline, offset_frac=0.01) Smooth  y  along  dims  using  interp_method . interp_method  is a 2-argument callabale that takes the arguments  y  and  x  and returns a DataInterpolations.jl interpolation method, defaulting to a cubic spline interpolator. offset_frac  is the fraction of the length of  y  to use as an offset when interpolating. source Hyndman1996 Rob J. Hyndman (1996) Computing and Graphing Highest Density Regions,             Amer. Stat., 50(2): 120-6.             DOI:  10.1080/00031305.1996.10474359 jstor . ChenShao1999 Ming-Hui Chen & Qi-Man Shao (1999)              Monte Carlo Estimation of Bayesian Credible and HPD Intervals,              J Comput. Graph. Stat., 8:1, 69-92.              DOI:  10.1080/10618600.1999.10474802 jstor . Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html Watanabe2010 Watanabe, S. Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. 11(116):3571−3594, 2010. https://jmlr.csail.mit.edu/papers/v11/watanabe10a.html Vehtari2017 Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). doi:  10.1007/s11222-016-9696-4  arXiv:  1507.04544 LOOFAQ Aki Vehtari. Cross-validation FAQ. https://mc-stan.org/loo/articles/online-only/faq.html YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 YaoVehtari2018 Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman.                Using Stacking to Average Bayesian Predictive Distributions.                2018. Bayesian Analysis. 13, 3, 917–1007.                doi:  10.1214/17-BA1091                 arXiv:  1704.02030 Gabry2019 Gabry, J., Simpson, D., Vehtari, A., Betancourt, M. & Gelman, A. Visualization in Bayesian Workflow. J. R. Stat. Soc. Ser. A Stat. Soc. 182, 389–402 (2019). doi:  10.1111/rssa.12378  arXiv:  1709.01449 GelmanGoodrich2019 Andrew Gelman, Ben Goodrich, Jonah Gabry & Aki Vehtari (2019) R-squared for Bayesian Regression Models, The American Statistician, 73:3, 307-9, DOI:  10.1080/00031305.2018.1549100 ."},{"id":258,"pagetitle":"Home","title":"PSIS","ref":"/PSIS/stable/#PSIS","content":" PSIS PSIS.jl implements the Pareto smoothed importance sampling (PSIS) algorithm from  [VehtariSimpson2021] . Given a set of importance weights used in some estimator, PSIS both improves the reliability of the estimates by smoothing the importance weights and acts as a diagnostic of the reliability of the estimates. See  psis  for details."},{"id":259,"pagetitle":"Home","title":"Example","ref":"/PSIS/stable/#Example","content":" Example In this example, we use PSIS to smooth log importance ratios for importance sampling 30 isotropic Student  $t$ -distributed parameters using standard normal distributions as proposals. using PSIS, Distributions\nproposal = Normal()\ntarget = TDist(7)\nndraws, nchains, nparams = (1_000, 1, 30)\nx = rand(proposal, ndraws, nchains, nparams)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) ┌ Warning: 8 parameters had Pareto shape values 0.7 < k ≤ 1. Resulting importance sampling estimates are likely to be unstable.\n└ @ PSIS ~/work/PSIS.jl/PSIS.jl/src/core.jl:323\n┌ Warning: 1 parameters had Pareto shape values k > 1. Corresponding importance sampling estimates are likely to be unstable and are unlikely to converge with additional samples.\n└ @ PSIS ~/work/PSIS.jl/PSIS.jl/src/core.jl:326 PSISResult with 1000 draws, 1 chains, and 30 parameters\nPareto shape (k) diagnostic values:\n                         Count       Min. ESS \n (-Inf, 0.5]  good       7 (23.3%)  959\n  (0.5, 0.7]  okay       14 (46.7%)   927\n    (0.7, 1]  bad         8 (26.7%)   ——\n    (1, Inf)  very bad    1 (3.3%)    —— As indicated by the warnings, this is a poor choice of a proposal distribution, and estimates are unlikely to converge (see  PSISResult  for an explanation of the shape thresholds). When running PSIS with many parameters, it is useful to plot the Pareto shape values to diagnose convergence. See  Plotting PSIS results  for examples. VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO]"},{"id":262,"pagetitle":"API","title":"API","ref":"/PSIS/stable/api/#API","content":" API"},{"id":263,"pagetitle":"API","title":"Core functionality","ref":"/PSIS/stable/api/#Core-functionality","content":" Core functionality"},{"id":264,"pagetitle":"API","title":"PSIS.PSISResult","ref":"/PSIS/stable/api/#PSIS.PSISResult","content":" PSIS.PSISResult  —  Type PSISResult Result of Pareto-smoothed importance sampling (PSIS) using  psis . Properties log_weights : un-normalized Pareto-smoothed log weights weights : normalized Pareto-smoothed weights (allocates a copy) pareto_shape : Pareto  $k=ξ$  shape parameter nparams : number of parameters in  log_weights ndraws : number of draws in  log_weights nchains : number of chains in  log_weights reff : the ratio of the effective sample size of the unsmoothed importance ratios and the actual sample size. ess : estimated effective sample size of estimate of mean using smoothed importance samples (see  ess_is ) tail_length : length of the upper tail of  log_weights  that was smoothed tail_dist : the generalized Pareto distribution that was fit to the tail of  log_weights . Note that the tail weights are scaled to have a maximum of 1, so  tail_dist * exp(maximum(log_ratios))  is the corresponding fit directly to the tail of  log_ratios . normalized::Bool :indicates whether  log_weights  are log-normalized along the sample dimensions. Diagnostic The  pareto_shape  parameter  $k=ξ$  of the generalized Pareto distribution  tail_dist  can be used to diagnose reliability and convergence of estimates using the importance weights  [VehtariSimpson2021] . if  $k < \\frac{1}{3}$ , importance sampling is stable, and importance sampling (IS) and PSIS both are reliable. if  $k ≤ \\frac{1}{2}$ , then the importance ratio distributon has finite variance, and the central limit theorem holds. As  $k$  approaches the upper bound, IS becomes less reliable, while PSIS still works well but with a higher RMSE. if  $\\frac{1}{2} < k ≤ 0.7$ , then the variance is infinite, and IS can behave quite poorly. However, PSIS works well in this regime. if  $0.7 < k ≤ 1$ , then it quickly becomes impractical to collect enough importance weights to reliably compute estimates, and importance sampling is not recommended. if  $k > 1$ , then neither the variance nor the mean of the raw importance ratios exists. The convergence rate is close to zero, and bias can be large with practical sample sizes. See  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":265,"pagetitle":"API","title":"PSIS.psis","ref":"/PSIS/stable/api/#PSIS.psis","content":" PSIS.psis  —  Function psis(log_ratios, reff = 1.0; kwargs...) -> PSISResult\npsis!(log_ratios, reff = 1.0; kwargs...) -> PSISResult Compute Pareto smoothed importance sampling (PSIS) log weights  [VehtariSimpson2021] . While  psis  computes smoothed log weights out-of-place,  psis!  smooths them in-place. Arguments log_ratios : an array of logarithms of importance ratios, with size  (draws, [chains, [parameters...]]) , where  chains>1  would be used when chains are generated using Markov chain Monte Carlo. reff::Union{Real,AbstractArray} : the ratio(s) of effective sample size of  log_ratios  and the actual sample size  reff = ess/(draws * chains) , used to account for autocorrelation, e.g. due to Markov chain Monte Carlo. If an array, it must have the size  (parameters...,)  to match  log_ratios . Keywords warn=true : If  true , warning messages are delivered normalize=true : If  true , the log-weights will be log-normalized so that  exp.(log_weights)  sums to 1 along the sample dimensions. Returns result : a  PSISResult  object containing the results of the Pareto-smoothing. A warning is raised if the Pareto shape parameter  $k ≥ 0.7$ . See  PSISResult  for details and  PSISPlots.paretoshapeplot  for a diagnostic plot. source"},{"id":266,"pagetitle":"API","title":"PSIS.ess_is","ref":"/PSIS/stable/api/#PSIS.ess_is","content":" PSIS.ess_is  —  Function ess_is(weights; reff=1) Estimate effective sample size (ESS) for importance sampling over the sample dimensions. Given normalized weights  $w_{1:n}$ , the ESS is estimated using the L2-norm of the weights: \\[\\mathrm{ESS}(w_{1:n}) = \\frac{r_{\\mathrm{eff}}}{\\sum_{i=1}^n w_i^2}\\] where  $r_{\\mathrm{eff}}$  is the relative efficiency of the  log_weights . ess_is(result::PSISResult; bad_shape_nan=true) Estimate ESS for Pareto-smoothed importance sampling. Note ESS estimates for Pareto shape values  $k > 0.7$ , which are unreliable and misleadingly high, are set to  NaN . To avoid this, set  bad_shape_nan=false . source"},{"id":267,"pagetitle":"API","title":"Plotting","ref":"/PSIS/stable/api/#Plotting","content":" Plotting"},{"id":268,"pagetitle":"API","title":"PSIS.PSISPlots","ref":"/PSIS/stable/api/#PSIS.PSISPlots","content":" PSIS.PSISPlots  —  Module A module defining  paretoshapeplot  for plotting Pareto shape values with Plots.jl source"},{"id":269,"pagetitle":"API","title":"PSIS.PSISPlots.paretoshapeplot","ref":"/PSIS/stable/api/#PSIS.PSISPlots.paretoshapeplot","content":" PSIS.PSISPlots.paretoshapeplot  —  Function paretoshapeplot(values; showlines=false, ...)\nparetoshapeplot!(values; showlines=false, kwargs...) Plot shape parameters of fitted Pareto tail distributions for diagnosing convergence. values  may be either a vector of Pareto shape parameters or a  PSIS.PSISResult . If  showlines==true , horizontal lines indicating relevant Pareto shape thresholds are drawn. See  PSIS.PSISResult  for an explanation of the thresholds. All remaining  kwargs  are forwarded to the plotting function. See  psis ,  PSISResult . Examples using PSIS, Distributions, Plots\nproposal = Normal()\ntarget = TDist(7)\nx = rand(proposal, 1_000, 100)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios)\nparetoshapeplot(result) We can also plot the Pareto shape parameters directly: paretoshapeplot(result.pareto_shape) We can also use  plot  directly: plot(result.pareto_shape; showlines=true) source VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO] VehtariSimpson2021 Vehtari A, Simpson D, Gelman A, Yao Y, Gabry J. (2021). Pareto smoothed importance sampling.  arXiv:1507.02646v7  [stat.CO]"},{"id":272,"pagetitle":"Internal","title":"Internal","ref":"/PSIS/stable/internal/#Internal","content":" Internal"},{"id":273,"pagetitle":"Internal","title":"PSIS.GeneralizedPareto","ref":"/PSIS/stable/internal/#PSIS.GeneralizedPareto","content":" PSIS.GeneralizedPareto  —  Type GeneralizedPareto{T<:Real} The generalized Pareto distribution. This is equivalent to  Distributions.GeneralizedPareto  and can be converted to one with  convert(Distributions.GeneralizedPareto, d) . Constructor GeneralizedPareto(μ, σ, k) Construct the generalized Pareto distribution (GPD) with location parameter  $μ$ , scale parameter  $σ$  and shape parameter  $k$ . Note The shape parameter  $k$  is equivalent to the commonly used shape parameter  $ξ$ . This is the same parameterization used by  [VehtariSimpson2021]  and is related to that used by  [ZhangStephens2009]  as  $k \\mapsto -k$ . source"},{"id":274,"pagetitle":"Internal","title":"PSIS.fit_gpd","ref":"/PSIS/stable/internal/#PSIS.fit_gpd-Tuple{AbstractArray}","content":" PSIS.fit_gpd  —  Method fit_gpd(x; μ=0, kwargs...) Fit a  GeneralizedPareto  with location  μ  to the data  x . The fit is performed using the Empirical Bayes method of  [ZhangStephens2009] . Keywords prior_adjusted::Bool=true , If  true , a weakly informative Normal prior centered on  $\\frac{1}{2}$  is used for the shape  $k$ . sorted::Bool=issorted(x) : If  true ,  x  is assumed to be sorted. If  false , a sorted copy of  x  is made. min_points::Int=30 : The minimum number of quadrature points to use when estimating the posterior mean of  $\\theta = \\frac{\\xi}{\\sigma}$ . source ZhangStephens2009 Jin Zhang & Michael A. Stephens (2009) A New and Efficient Estimation Method for the Generalized Pareto Distribution, Technometrics, 51:3, 316-325, DOI:  10.1198/tech.2009.08017"},{"id":277,"pagetitle":"Plotting","title":"Plotting PSIS results","ref":"/PSIS/stable/plotting/#Plotting-PSIS-results","content":" Plotting PSIS results PSIS.jl includes plotting recipes for  PSISResult  using any Plots.jl backend, as well as the utility plotting function  PSISPlots.paretoshapeplot . We demonstrate this with a simple example. using PSIS, Distributions\nproposal = Normal()\ntarget = TDist(7)\nndraws, nchains, nparams = (1_000, 1, 20)\nx = rand(proposal, ndraws, nchains, nparams)\nlog_ratios = logpdf.(target, x) .- logpdf.(proposal, x)\nresult = psis(log_ratios) PSISResult with 1000 draws, 1 chains, and 20 parameters\nPareto shape (k) diagnostic values:\n                     Count       Min. ESS \n (-Inf, 0.5]  good   4 (20.0%)  959\n  (0.5, 0.7]  okay   10 (50.0%)   927\n    (0.7, 1]  bad     6 (30.0%)   ——"},{"id":278,"pagetitle":"Plotting","title":"Plots.jl","ref":"/PSIS/stable/plotting/#Plots.jl","content":" Plots.jl PSISResult  objects can be plotted directly: using Plots\nplot(result; showlines=true, marker=:+, legend=false, linewidth=2) This is equivalent to calling  PSISPlots.paretoshapeplot(result; kwargs...) ."},{"id":281,"pagetitle":"Home","title":"MCMCDiagnosticTools","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools","content":" MCMCDiagnosticTools MCMCDiagnosticTools provides functionality for diagnosing samples generated using Markov Chain Monte Carlo."},{"id":282,"pagetitle":"Home","title":"Background","ref":"/MCMCDiagnosticTools/stable/#Background","content":" Background Some methods were originally part of  Mamba.jl  and then  MCMCChains.jl . This package is a joint collaboration between the  Turing  and  ArviZ  projects."},{"id":283,"pagetitle":"Home","title":"Effective sample size and  $\\widehat{R}$","ref":"/MCMCDiagnosticTools/stable/#Effective-sample-size-and-\\\\widehat{R}","content":" Effective sample size and  $\\widehat{R}$"},{"id":284,"pagetitle":"Home","title":"MCMCDiagnosticTools.ess","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.ess","content":" MCMCDiagnosticTools.ess  —  Function ess(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind=:bulk,\n    relative::Bool=false,\n    autocov_method=AutocovMethod(),\n    split_chains::Int=2,\n    maxlag::Int=250,\n    kwargs...\n) Estimate the effective sample size (ESS) of the  samples  of shape  (draws, [chains[, parameters...]])  with the  autocov_method . Optionally, the  kind  of ESS estimate to be computed can be specified (see below). Some  kind s accept additional  kwargs . If  relative  is  true , the relative ESS is returned, i.e.  ess / (draws * chains) . split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. There must be at least 3 draws in each chain after splitting. maxlag  indicates the maximum lag for which autocovariance is computed and must be greater than 0. For a given estimand, it is recommended that the ESS is at least  100 * chains  and that  $\\widehat{R} < 1.01$ . [VehtariGelman2021] See also:  AutocovMethod ,  FFTAutocovMethod ,  BDAAutocovMethod ,  rhat ,  ess_rhat ,  mcse Kinds of ESS estimates If  kind  isa a  Symbol , it may take one of the following values: :bulk : basic ESS computed on rank-normalized draws. This kind diagnoses poor convergence   in the bulk of the distribution due to trends or different locations of the chains. :tail : minimum of the quantile-ESS for the symmetric quantiles where    tail_prob=0.1  is the probability in the tails. This kind diagnoses poor convergence in   the tails of the distribution. If this kind is chosen,  kwargs  may contain a    tail_prob  keyword. :basic : basic ESS, equivalent to specifying  kind=Statistics.mean . Note While Bulk-ESS is conceptually related to basic ESS, it is well-defined even if the chains do not have finite variance. [VehtariGelman2021]  For each parameter, rank-normalization proceeds by first ranking the inputs using \"tied ranking\" and then transforming the ranks to normal quantiles so that the result is standard normally distributed. This transform is monotonic. Otherwise,  kind  specifies one of the following estimators, whose ESS is to be estimated: Statistics.mean Statistics.median Statistics.std StatsBase.mad Base.Fix2(Statistics.quantile, p::Real) source"},{"id":285,"pagetitle":"Home","title":"MCMCDiagnosticTools.rhat","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rhat","content":" MCMCDiagnosticTools.rhat  —  Function rhat(samples::AbstractArray{Union{Real,Missing}}; kind::Symbol=:rank, split_chains=2) Compute the  $\\widehat{R}$  diagnostics for each parameter in  samples  of shape  (draws, [chains[, parameters...]]) . [VehtariGelman2021] kind  indicates the kind of  $\\widehat{R}$  to compute (see below). split_chains  indicates the number of chains each chain is split into. When  split_chains > 1 , then the diagnostics check for within-chain convergence. When  d = mod(draws, split_chains) > 0 , i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first  d  splits within each chain. See also  ess ,  ess_rhat ,  rstar Kinds of  $\\widehat{R}$ The following  kind s are supported: :rank : maximum of  $\\widehat{R}$  with  kind=:bulk  and  kind=:tail . :bulk : basic  $\\widehat{R}$  computed on rank-normalized draws. This kind diagnoses   poor convergence in the bulk of the distribution due to trends or different locations of   the chains. :tail :  $\\widehat{R}$  computed on draws folded around the median and then   rank-normalized. This kind diagnoses poor convergence in the tails of the distribution   due to different scales of the chains. :basic : Classic  $\\widehat{R}$ . source"},{"id":286,"pagetitle":"Home","title":"MCMCDiagnosticTools.ess_rhat","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.ess_rhat","content":" MCMCDiagnosticTools.ess_rhat  —  Function ess_rhat(\n    samples::AbstractArray{<:Union{Missing,Real}};\n    kind::Symbol=:rank,\n    kwargs...,\n) -> NamedTuple{(:ess, :rhat)} Estimate the effective sample size and  $\\widehat{R}$  of the  samples  of shape  (draws, [chains[, parameters...]]) . When both ESS and  $\\widehat{R}$  are needed, this method is often more efficient than calling  ess  and  rhat  separately. See  rhat  for a description of supported  kind s and  ess  for a description of  kwargs . source The following  autocov_method s are supported:"},{"id":287,"pagetitle":"Home","title":"MCMCDiagnosticTools.AutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.AutocovMethod","content":" MCMCDiagnosticTools.AutocovMethod  —  Type AutocovMethod <: AbstractAutocovMethod The  AutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021]  and uses the biased estimator of the autocovariance, as discussed by  [Geyer1992] . source"},{"id":288,"pagetitle":"Home","title":"MCMCDiagnosticTools.FFTAutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.FFTAutocovMethod","content":" MCMCDiagnosticTools.FFTAutocovMethod  —  Type FFTAutocovMethod <: AbstractAutocovMethod The  FFTAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. The algorithm is the same as the one of  AutocovMethod  but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation. Info To be able to use this method, you have to load a package that implements the  AbstractFFTs.jl  interface such as  FFTW.jl  or  FastTransforms.jl . source"},{"id":289,"pagetitle":"Home","title":"MCMCDiagnosticTools.BDAAutocovMethod","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.BDAAutocovMethod","content":" MCMCDiagnosticTools.BDAAutocovMethod  —  Type BDAAutocovMethod <: AbstractAutocovMethod The  BDAAutocovMethod  uses a standard algorithm for estimating the mean autocovariance of MCMC chains. It is is based on the discussion by  [VehtariGelman2021] . and uses the variogram estimator of the autocorrelation function discussed by  [BDA3] . source"},{"id":290,"pagetitle":"Home","title":"Monte Carlo standard error","ref":"/MCMCDiagnosticTools/stable/#Monte-Carlo-standard-error","content":" Monte Carlo standard error"},{"id":291,"pagetitle":"Home","title":"MCMCDiagnosticTools.mcse","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.mcse","content":" MCMCDiagnosticTools.mcse  —  Function mcse(samples::AbstractArray{<:Union{Missing,Real}}; kind=Statistics.mean, kwargs...) Estimate the Monte Carlo standard errors (MCSE) of the estimator  kind  applied to  samples  of shape  (draws, [chains[, parameters...]]) . See also:  ess Kinds of MCSE estimates The estimator whose MCSE should be estimated is specified with  kind .  kind  must accept a vector of the same  eltype  as  samples  and return a real estimate. For the following estimators, the effective sample size  ess  and an estimate of the asymptotic variance are used to compute the MCSE, and  kwargs  are forwarded to  ess : Statistics.mean Statistics.median Statistics.std Base.Fix2(Statistics.quantile, p::Real) For other estimators, the subsampling bootstrap method (SBM) [FlegalJones2011] [Flegal2012]  is used as a fallback, and the only accepted  kwargs  are  batch_size , which indicates the size of the overlapping batches used to estimate the MCSE, defaulting to  floor(Int, sqrt(draws * chains)) . Note that SBM tends to underestimate the MCSE, especially for highly autocorrelated chains. One should verify that autocorrelation is low by checking the bulk- and tail-ESS values. source"},{"id":292,"pagetitle":"Home","title":"R⋆ diagnostic","ref":"/MCMCDiagnosticTools/stable/#R-diagnostic","content":" R⋆ diagnostic"},{"id":293,"pagetitle":"Home","title":"MCMCDiagnosticTools.rstar","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rstar","content":" MCMCDiagnosticTools.rstar  —  Function rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples,\n    chain_indices::AbstractVector{Int};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the table  samples  with the  classifier . samples  must be either an  AbstractMatrix , an  AbstractVector , or a table (i.e. implements the Tables.jl interface) whose rows are draws and whose columns are parameters. chain_indices  indicates the chain ids of each row of  samples . This method supports ragged chains, i.e. chains of nonequal lengths. source rstar(\n    rng::Random.AbstractRNG=Random.default_rng(),\n    classifier,\n    samples::AbstractArray{<:Real};\n    subset::Real=0.7,\n    split_chains::Int=2,\n    verbosity::Int=0,\n) Compute the  $R^*$  convergence statistic of the  samples  with the  classifier . samples  is an array of draws with the shape  (draws, [chains[, parameters...]]) .` This implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari. The  classifier  has to be a supervised classifier of the MLJ framework (see the  MLJ documentation  for a list of supported models). It is trained with a  subset  of the samples from each chain. Each chain is split into  split_chains  separate chains to additionally check for within-chain convergence. The training of the classifier can be inspected by adjusting the  verbosity  level. If the classifier is deterministic, i.e., if it predicts a class, the value of the  $R^*$  statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the  $R^*$  statistic is returned (algorithm 2). Note The correctness of the statistic depends on the convergence of the  classifier  used internally in the statistic. Examples julia> using MLJBase, MLJIteration, EvoTrees, Statistics, StatisticalMeasures\n\njulia> samples = fill(4.0, 100, 3, 2); One can compute the distribution of the  $R^*$  statistic (algorithm 2) with a probabilistic classifier. For instance, we can use a gradient-boosted trees model with  nrounds = 100  sequentially stacked trees and learning rate  eta = 0.05 : julia> model = EvoTreeClassifier(; nrounds=100, eta=0.05);\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 Note, however, that it is recommended to determine  nrounds  based on early-stopping. With the MLJ framework, this can be achieved in the following way (see the  MLJ documentation  for additional explanations): julia> model = IteratedModel(;\n           model=EvoTreeClassifier(; eta=0.05),\n           iteration_parameter=:nrounds,\n           resampling=Holdout(),\n           measures=log_loss,\n           controls=[Step(5), Patience(2), NumberLimit(100)],\n           retrain=true,\n       );\n\njulia> distribution = rstar(model, samples);\n\njulia> round(mean(distribution); digits=2)\n1.0f0 For deterministic classifiers, a single  $R^*$  statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models. julia> evotree_deterministic = Pipeline(model; operation=predict_mode);\n\njulia> value = rstar(evotree_deterministic, samples);\n\njulia> round(value; digits=2)\n1.0 References Lambert, B., & Vehtari, A. (2020).  $R^*$ : A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers. source"},{"id":294,"pagetitle":"Home","title":"Bayesian fraction of missing information","ref":"/MCMCDiagnosticTools/stable/#Bayesian-fraction-of-missing-information","content":" Bayesian fraction of missing information"},{"id":295,"pagetitle":"Home","title":"MCMCDiagnosticTools.bfmi","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.bfmi","content":" MCMCDiagnosticTools.bfmi  —  Function bfmi(energy::AbstractVector{<:Real}) -> Real\nbfmi(energy::AbstractMatrix{<:Real}; dims::Int=1) -> AbstractVector{<:Real} Calculate the estimated Bayesian fraction of missing information (BFMI). When sampling with Hamiltonian Monte Carlo (HMC), BFMI quantifies how well momentum resampling matches the marginal energy distribution. The current advice is that values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may change. A BFMI value below the threshold often indicates poor adaptation of sampling parameters or that the target distribution has heavy tails that were not well explored by the Markov chain. For more information, see Section 6.1 of  [Betancourt2018]  or  [Betancourt2016]  for a complete account. energy  is either a vector of Hamiltonian energies of draws or a matrix of energies of draws for multiple chains.  dims  indicates the dimension in  energy  that contains the draws. The default  dims=1  assumes  energy  has the shape  draws  or  (draws, chains) . If a different shape is provided,  dims  must be set accordingly. If  energy  is a vector, a single BFMI value is returned. Otherwise, a vector of BFMI values for each chain is returned. source"},{"id":296,"pagetitle":"Home","title":"Other diagnostics","ref":"/MCMCDiagnosticTools/stable/#Other-diagnostics","content":" Other diagnostics Note These diagnostics are older and less widely used."},{"id":297,"pagetitle":"Home","title":"MCMCDiagnosticTools.discretediag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.discretediag","content":" MCMCDiagnosticTools.discretediag  —  Function discretediag(samples::AbstractArray{<:Real,3}; frac=0.3, method=:weiss, nsim=1_000) Compute discrete diagnostic on  samples  with shape  (draws, chains, parameters) . method  can be one of  :weiss ,  :hangartner ,  :DARBOOT ,  :MCBOOT ,  :billinsgley , and  :billingsleyBOOT . References Benjamin E. Deonovic, & Brian J. Smith. (2017). Convergence diagnostics for MCMC draws of a categorical variable. source"},{"id":298,"pagetitle":"Home","title":"MCMCDiagnosticTools.gelmandiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gelmandiag","content":" MCMCDiagnosticTools.gelmandiag  —  Function gelmandiag(samples::AbstractArray{<:Real,3}; alpha::Real=0.95) Compute the Gelman, Rubin and Brooks diagnostics  [Gelman1992] [Brooks1998]  on  samples  with shape  (draws, chains, parameters) .  Values of the diagnostic’s potential scale reduction factor (PSRF) that are close to one suggest convergence.  As a rule-of-thumb, convergence is rejected if the 97.5 percentile of a PSRF is greater than 1.2. source"},{"id":299,"pagetitle":"Home","title":"MCMCDiagnosticTools.gelmandiag_multivariate","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gelmandiag_multivariate","content":" MCMCDiagnosticTools.gelmandiag_multivariate  —  Function gelmandiag_multivariate(samples::AbstractArray{<:Real,3}; alpha::Real=0.05) Compute the multivariate Gelman, Rubin and Brooks diagnostics on  samples  with shape  (draws, chains, parameters) . source"},{"id":300,"pagetitle":"Home","title":"MCMCDiagnosticTools.gewekediag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.gewekediag","content":" MCMCDiagnosticTools.gewekediag  —  Function gewekediag(x::AbstractVector{<:Real}; first::Real=0.1, last::Real=0.5, kwargs...) Compute the Geweke diagnostic  [Geweke1991]  from the  first  and  last  proportion of samples  x . The diagnostic is designed to asses convergence of posterior means estimated with autocorrelated samples.  It computes a normal-based test statistic comparing the sample means in two windows containing proportions of the first and last iterations.  Users should ensure that there is sufficient separation between the two windows to assume that their samples are independent.  A non-significant test p-value indicates convergence.  Significant p-values indicate non-convergence and the possible need to discard initial samples as a burn-in sequence or to simulate additional samples. kwargs  are forwarded to  mcse . source"},{"id":301,"pagetitle":"Home","title":"MCMCDiagnosticTools.heideldiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.heideldiag","content":" MCMCDiagnosticTools.heideldiag  —  Function heideldiag(\n    x::AbstractVector{<:Real}; alpha::Real=0.05, eps::Real=0.1, start::Int=1, kwargs...\n) Compute the Heidelberger and Welch diagnostic  [Heidelberger1983] . This diagnostic tests for non-convergence (non-stationarity) and whether ratios of estimation interval halfwidths to means are within a target ratio. Stationarity is rejected (0) for significant test p-values. Halfwidth tests are rejected (0) if observed ratios are greater than the target, as is the case for  s2  and  beta[1] . kwargs  are forwarded to  mcse . source"},{"id":302,"pagetitle":"Home","title":"MCMCDiagnosticTools.rafterydiag","ref":"/MCMCDiagnosticTools/stable/#MCMCDiagnosticTools.rafterydiag","content":" MCMCDiagnosticTools.rafterydiag  —  Function rafterydiag(\n    x::AbstractVector{<:Real}; q=0.025, r=0.005, s=0.95, eps=0.001, range=1:length(x)\n) Compute the Raftery and Lewis diagnostic  [Raftery1992] . This diagnostic is used to determine the number of iterations required to estimate a specified quantile  q  within a desired degree of accuracy.  The diagnostic is designed to determine the number of autocorrelated samples required to estimate a specified quantile  $\\theta_q$ , such that  $\\Pr(\\theta \\le \\theta_q) = q$ , within a desired degree of accuracy. In particular, if  $\\hat{\\theta}_q$  is the estimand and  $\\Pr(\\theta \\le \\hat{\\theta}_q) = \\hat{P}_q$  the estimated cumulative probability, then accuracy is specified in terms of  r  and  s , where  $\\Pr(q - r < \\hat{P}_q < q + r) = s$ . Thinning may be employed in the calculation of the diagnostic to satisfy its underlying assumptions. However, users may not want to apply the same (or any) thinning when estimating posterior summary statistics because doing so results in a loss of information. Accordingly, sample sizes estimated by the diagnostic tend to be conservative (too large). Furthermore, the argument  r  specifies the margin of error for estimated cumulative probabilities and  s  the probability for the margin of error.  eps  specifies the tolerance within which the probabilities of transitioning from initial to retained iterations are within the equilibrium probabilities for the chain. This argument determines the number of samples to discard as a burn-in sequence and is typically left at its default value. source VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 Geyer1992 Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483. VehtariGelman2021 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved  $\\widehat {R}$  for assessing convergence of MCMC. Bayesian Analysis. doi:  10.1214/20-BA1221  arXiv:  1903.08008 BDA3 Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis. CRC press. FlegalJones2011 Flegal JM, Jones GL. (2011) Implementing MCMC: estimating with confidence.                 Handbook of Markov Chain Monte Carlo. pp. 175-97.                  pdf Flegal2012 Flegal JM. (2012) Applicability of subsampling bootstrap methods in Markov chain Monte Carlo.            Monte Carlo and Quasi-Monte Carlo Methods 2010. pp. 363-72.            doi:  10.1007/978-3-642-27440-4_18 Betancourt2018 Betancourt M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo.  arXiv:1701.02434v2  [stat.ME] Betancourt2016 Betancourt M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo.  arXiv:1604.00695v1  [stat.ME] Gelman1992 Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical science, 7(4), 457-472. Brooks1998 Brooks, S. P., & Gelman, A. (1998). General methods for monitoring convergence of iterative simulations. Journal of computational and graphical statistics, 7(4), 434-455. Geweke1991 Geweke, J. F. (1991). Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments (No. 148). Federal Reserve Bank of Minneapolis. Heidelberger1983 Heidelberger, P., & Welch, P. D. (1983). Simulation run length control in the presence of an initial transient. Operations Research, 31(6), 1109-1144. Raftery1992 A L Raftery and S Lewis. Bayesian Statistics, chapter How Many Iterations in the Gibbs Sampler? Volume 4. Oxford University Press, New York, 1992."},{"id":305,"pagetitle":"Home","title":"InferenceObjects","ref":"/InferenceObjects/stable/#InferenceObjects","content":" InferenceObjects InferenceObjects.jl is a Julia implementation of the  InferenceData schema  for storing results of Bayesian inference. Its purpose is to serve the following three goals: Usefulness in the analysis of Bayesian inference results. Reproducibility of Bayesian inference analysis. Interoperability between different inference backends and programming languages. The implementation consists primarily of the  InferenceData  and  Dataset  structures. InferenceObjects also provides the function  convert_to_inference_data , which may be overloaded by inference packages to define how various inference outputs can be converted to an  InferenceData . For examples of how  InferenceData  can be used, see the  ArviZ.jl documentation ."},{"id":308,"pagetitle":"Dataset","title":"Dataset","ref":"/InferenceObjects/stable/dataset/#Dataset","content":" Dataset InferenceObjects.Dataset InferenceObjects.convert_to_dataset InferenceObjects.namedtuple_to_dataset"},{"id":309,"pagetitle":"Dataset","title":"Type definition","ref":"/InferenceObjects/stable/dataset/#Type-definition","content":" Type definition"},{"id":310,"pagetitle":"Dataset","title":"InferenceObjects.Dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.Dataset","content":" InferenceObjects.Dataset  —  Type Dataset{L} <: DimensionalData.AbstractDimStack{L} Container of dimensional arrays sharing some dimensions. This type is an  DimensionalData.AbstractDimStack  that implements the same interface as  DimensionalData.DimStack  and has identical usage. When a  Dataset  is passed to Python, it is converted to an  xarray.Dataset  without copying the data. That is, the Python object shares the same memory as the Julia object. However, if an  xarray.Dataset  is passed to Julia, its data must be copied. Constructors Dataset(data::DimensionalData.AbstractDimArray...)\nDataset(data::Tuple{Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(data::NamedTuple{Keys,Vararg{<:DimensionalData.AbstractDimArray}})\nDataset(\n    data::NamedTuple,\n    dims::Tuple{Vararg{DimensionalData.Dimension}};\n    metadata=DimensionalData.NoMetadata(),\n) In most cases, use  convert_to_dataset  to create a  Dataset  instead of directly using a constructor. source"},{"id":311,"pagetitle":"Dataset","title":"General conversion","ref":"/InferenceObjects/stable/dataset/#General-conversion","content":" General conversion"},{"id":312,"pagetitle":"Dataset","title":"InferenceObjects.convert_to_dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.convert_to_dataset","content":" InferenceObjects.convert_to_dataset  —  Function convert_to_dataset(obj; group = :posterior, kwargs...) -> Dataset Convert a supported object to a  Dataset . In most cases, this function calls  convert_to_inference_data  and returns the corresponding  group . source"},{"id":313,"pagetitle":"Dataset","title":"InferenceObjects.namedtuple_to_dataset","ref":"/InferenceObjects/stable/dataset/#InferenceObjects.namedtuple_to_dataset","content":" InferenceObjects.namedtuple_to_dataset  —  Function namedtuple_to_dataset(data; kwargs...) -> Dataset Convert  NamedTuple  mapping variable names to arrays to a  Dataset . Any non-array values will be converted to a 0-dimensional array. Keywords attrs::AbstractDict{<:AbstractString} : a collection of metadata to attach to the dataset, in addition to defaults. Values should be JSON serializable. library::Union{String,Module} : library used for performing inference. Will be attached to the  attrs  metadata. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. source"},{"id":314,"pagetitle":"Dataset","title":"DimensionalData","ref":"/InferenceObjects/stable/dataset/#DimensionalData","content":" DimensionalData As a  DimensionalData.AbstractDimStack ,  Dataset  also implements the  AbstractDimStack  API and can be used like a  DimStack . See  DimensionalData's documentation  for example usage."},{"id":315,"pagetitle":"Dataset","title":"Tables inteface","ref":"/InferenceObjects/stable/dataset/#Tables-inteface","content":" Tables inteface Dataset  implements the  Tables  interface. This allows  Dataset s to be used as sources for any function that can accept a table. For example, it's straightforward to: write to CSV with CSV.jl flatten to a DataFrame with DataFrames.jl plot with StatsPlots.jl plot with AlgebraOfGraphics.jl"},{"id":318,"pagetitle":"InferenceData","title":"InferenceData","ref":"/InferenceObjects/stable/inference_data/#InferenceData","content":" InferenceData InferenceObjects.InferenceData Base.cat Base.getindex Base.getproperty Base.merge Base.propertynames Base.setindex InferenceObjects.convert_to_inference_data InferenceObjects.from_dict InferenceObjects.from_namedtuple InferenceObjects.from_netcdf InferenceObjects.to_netcdf"},{"id":319,"pagetitle":"InferenceData","title":"Type definition","ref":"/InferenceObjects/stable/inference_data/#Type-definition","content":" Type definition"},{"id":320,"pagetitle":"InferenceData","title":"InferenceObjects.InferenceData","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.InferenceData","content":" InferenceObjects.InferenceData  —  Type InferenceData{group_names,group_types} Container for inference data storage using DimensionalData. This object implements the  InferenceData schema . Internally, groups are stored in a  NamedTuple , which can be accessed using  parent(::InferenceData) . Constructors InferenceData(groups::NamedTuple)\nInferenceData(; groups...) Construct an inference data from either a  NamedTuple  or keyword arguments of groups. Groups must be  Dataset  objects. Instead of directly creating an  InferenceData , use the exported  from_xyz  functions or  convert_to_inference_data . source"},{"id":321,"pagetitle":"InferenceData","title":"Property interface","ref":"/InferenceObjects/stable/inference_data/#Property-interface","content":" Property interface"},{"id":322,"pagetitle":"InferenceData","title":"Base.getproperty","ref":"/InferenceObjects/stable/inference_data/#Base.getproperty","content":" Base.getproperty  —  Function getproperty(data::InferenceData, name::Symbol) -> Dataset Get group with the specified  name . source"},{"id":323,"pagetitle":"InferenceData","title":"Base.propertynames","ref":"/InferenceObjects/stable/inference_data/#Base.propertynames","content":" Base.propertynames  —  Function propertynames(data::InferenceData) -> Tuple{Symbol} Get names of groups source"},{"id":324,"pagetitle":"InferenceData","title":"Indexing interface","ref":"/InferenceObjects/stable/inference_data/#Indexing-interface","content":" Indexing interface"},{"id":325,"pagetitle":"InferenceData","title":"Base.getindex","ref":"/InferenceObjects/stable/inference_data/#Base.getindex","content":" Base.getindex  —  Function Base.getindex(data::InferenceData, groups::Symbol; coords...) -> Dataset\nBase.getindex(data::InferenceData, groups; coords...) -> InferenceData Return a new  InferenceData  containing the specified groups sliced to the specified coords. coords  specifies a dimension name mapping to an index, a  DimensionalData.Selector , or an  IntervalSets.AbstractInterval . If one or more groups lack the specified dimension, a warning is raised but can be ignored. All groups that contain the dimension must also contain the specified indices, or an exception will be raised. Examples Select data from all groups for just the specified id values. julia> using InferenceObjects, DimensionalData\n\njulia> idata = from_namedtuple(\n           (θ=randn(4, 100, 4), τ=randn(4, 100));\n           prior=(θ=randn(4, 100, 4), τ=randn(4, 100)),\n           observed_data=(y=randn(4),),\n           dims=(θ=[:id], y=[:id]),\n           coords=(id=[\"a\", \"b\", \"c\", \"d\"],),\n       )\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b, c, d] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×4)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\"\n\njulia> idata_sel = idata[id=At([\"a\", \"b\"])]\nInferenceData with groups:\n  > posterior\n  > prior\n  > observed_data\n\njulia> idata_sel.posterior\nDataset with dimensions:\n  Dim{:chain} Sampled 1:4 ForwardOrdered Regular Points,\n  Dim{:draw} Sampled 1:100 ForwardOrdered Regular Points,\n  Dim{:id} Categorical String[a, b] ForwardOrdered\nand 2 layers:\n  :θ Float64 dims: Dim{:chain}, Dim{:draw}, Dim{:id} (4×100×2)\n  :τ Float64 dims: Dim{:chain}, Dim{:draw} (4×100)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:15:21.4\" Select data from just the posterior, returning a  Dataset  if the indices index more than one element from any of the variables: julia> idata[:observed_data, id=At([\"a\"])]\nDataset with dimensions:\n  Dim{:id} Categorical String[a] ForwardOrdered\nand 1 layer:\n  :y Float64 dims: Dim{:id} (1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2022-08-11T11:19:25.982\" Note that if a single index is provided, the behavior is still to slice so that the dimension is preserved. source"},{"id":326,"pagetitle":"InferenceData","title":"Base.setindex","ref":"/InferenceObjects/stable/inference_data/#Base.setindex","content":" Base.setindex  —  Function Base.setindex(data::InferenceData, group::Dataset, name::Symbol) -> InferenceData Create a new  InferenceData  containing the  group  with the specified  name . If a group with  name  is already in  data , it is replaced. source"},{"id":327,"pagetitle":"InferenceData","title":"Iteration interface","ref":"/InferenceObjects/stable/inference_data/#Iteration-interface","content":" Iteration interface InferenceData  also implements the same iteration interface as its underlying  NamedTuple . That is, iterating over an  InferenceData  iterates over its groups."},{"id":328,"pagetitle":"InferenceData","title":"General conversion","ref":"/InferenceObjects/stable/inference_data/#General-conversion","content":" General conversion"},{"id":329,"pagetitle":"InferenceData","title":"InferenceObjects.convert_to_inference_data","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.convert_to_inference_data","content":" InferenceObjects.convert_to_inference_data  —  Function convert_to_inference_data(obj; group, kwargs...) -> InferenceData Convert a supported object to an  InferenceData  object. If  obj  converts to a single dataset,  group  specifies which dataset in the resulting  InferenceData  that is. See  convert_to_dataset Arguments obj  can be many objects. Basic supported types are: InferenceData : return unchanged Dataset / DimensionalData.AbstractDimStack : add to  InferenceData  as the only group NamedTuple / AbstractDict : create a  Dataset  as the only group AbstractArray{<:Real} : create a  Dataset  as the only group, given an arbitrary name, if the name is not set More specific types may be documented separately. Keywords group::Symbol = :posterior : If  obj  converts to a single dataset, assign the resulting dataset to this group. dims : a collection mapping variable names to collections of objects containing dimension names. Acceptable such objects are: Symbol : dimension name Type{<:DimensionsionalData.Dimension} : dimension type DimensionsionalData.Dimension : dimension, potentially with indices Nothing : no dimension name provided, dimension name is automatically generated coords : a collection indexable by dimension name specifying the indices of the given dimension. If indices for a dimension in  dims  are provided, they are used even if the dimension contains its own indices. If a dimension is missing, its indices are automatically generated. kwargs : remaining keywords forwarded to converter functions source"},{"id":330,"pagetitle":"InferenceData","title":"InferenceObjects.from_dict","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_dict","content":" InferenceObjects.from_dict  —  Function from_dict(posterior::AbstractDict; kwargs...) -> InferenceData Convert a  Dict  to an  InferenceData . Arguments posterior : The data to be converted. Its strings must be  Symbol  or  AbstractString , and its values must be arrays. Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior::Dict=nothing : Draws from the prior prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata = Dict(\n    :x => rand(ndraws, nchains),\n    :y => randn(2, ndraws, nchains),\n    :z => randn(3, 2, ndraws, nchains),\n)\nidata = from_dict(data) source"},{"id":331,"pagetitle":"InferenceData","title":"InferenceObjects.from_namedtuple","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_namedtuple","content":" InferenceObjects.from_namedtuple  —  Function from_namedtuple(posterior::NamedTuple; kwargs...) -> InferenceData\nfrom_namedtuple(posterior::Vector{Vector{<:NamedTuple}}; kwargs...) -> InferenceData\nfrom_namedtuple(\n    posterior::NamedTuple,\n    sample_stats::Any,\n    posterior_predictive::Any,\n    predictions::Any,\n    log_likelihood::Any;\n    kwargs...\n) -> InferenceData Convert a  NamedTuple  or container of  NamedTuple s to an  InferenceData . If containers are passed, they are flattened into a single  NamedTuple  with array elements whose first dimensions correspond to the dimensions of the containers. Arguments posterior : The data to be converted. It may be of the following types: ::NamedTuple : The keys are the variable names and the values are arrays with dimensions  (ndraws, nchains[, sizes...]) . ::Vector{Vector{<:NamedTuple}} : A vector of length  nchains  whose elements have length  ndraws . Keywords posterior_predictive::Any=nothing : Draws from the posterior predictive distribution sample_stats::Any=nothing : Statistics of the posterior sampling process predictions::Any=nothing : Out-of-sample predictions for the posterior. prior=nothing : Draws from the prior. Accepts the same types as  posterior . prior_predictive::Any=nothing : Draws from the prior predictive distribution sample_stats_prior::Any=nothing : Statistics of the prior sampling process observed_data::NamedTuple : Observed data on which the  posterior  is conditional. It should only contain data which is modeled as a random variable. Keys are parameter names and values. constant_data::NamedTuple : Model constants, data included in the model which is not modeled as a random variable. Keys are parameter names and values. predictions_constant_data::NamedTuple : Constants relevant to the model predictions (i.e. new  x  values in a linear regression). log_likelihood : Pointwise log-likelihood for the data. It is recommended to use this argument as a  NamedTuple  whose keys are observed variable names and whose values are log likelihood arrays. library : Name of library that generated the draws coords : Map from named dimension to named indices dims : Map from variable name to names of its dimensions Returns InferenceData : The data with groups corresponding to the provided data Note If a  NamedTuple  is provided for  observed_data ,  constant_data , or predictions constant data`, any non-array values (e.g. integers) are converted to 0-dimensional arrays. Examples using InferenceObjects\nnchains = 2\nndraws = 100\n\ndata1 = (\n    x=rand(ndraws, nchains), y=randn(ndraws, nchains, 2), z=randn(ndraws, nchains, 3, 2)\n)\nidata1 = from_namedtuple(data1)\n\ndata2 = [[(x=rand(), y=randn(2), z=randn(3, 2)) for _ in 1:ndraws] for _ in 1:nchains];\nidata2 = from_namedtuple(data2) source"},{"id":332,"pagetitle":"InferenceData","title":"General functions","ref":"/InferenceObjects/stable/inference_data/#General-functions","content":" General functions"},{"id":333,"pagetitle":"InferenceData","title":"Base.cat","ref":"/InferenceObjects/stable/inference_data/#Base.cat","content":" Base.cat  —  Function cat(data::InferenceData...; [groups=keys(data[1]),] dims) -> InferenceData Concatenate  InferenceData  objects along the specified dimension  dims . Only the groups in  groups  are concatenated. Remaining groups are  merge d into the new  InferenceData  object. Examples Here is how we can concatenate all groups of two  InferenceData  objects along the existing  chain  dimension: julia> coords = (; a_dim=[\"x\", \"y\", \"z\"]);\n\njulia> dims = dims=(; a=[:a_dim]);\n\njulia> data = Dict(:a => randn(100, 4, 3), :b => randn(100, 4));\n\njulia> idata = from_dict(data; coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1 = cat(idata, idata; dims=:chain)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat1.posterior\n╭─────────────────╮\n│ 100×8×3 Dataset │\n├─────────────────┴──────────────────────────────────── dims ┐\n  ↓ draw ,\n  → chain,\n  ↗ a_dim Categorical{String} [\"x\", \"y\", \"z\"] ForwardOrdered\n├──────────────────────────────────────────────────── layers ┤\n  :a eltype: Float64 dims: draw, chain, a_dim size: 100×8×3\n  :b eltype: Float64 dims: draw, chain size: 100×8\n├────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2024-03-11T14:10:48.434\" Alternatively, we can concatenate along a new  run  dimension, which will be created. julia> idata_cat2 = cat(idata, idata; dims=:run)\nInferenceData with groups:\n  > posterior\n\njulia> idata_cat2.posterior\n╭───────────────────╮\n│ 100×4×3×2 Dataset │\n├───────────────────┴─────────────────────────────────── dims ┐\n  ↓ draw ,\n  → chain,\n  ↗ a_dim Categorical{String} [\"x\", \"y\", \"z\"] ForwardOrdered,\n  ⬔ run\n├─────────────────────────────────────────────────────────────┴ layers ┐\n  :a eltype: Float64 dims: draw, chain, a_dim, run size: 100×4×3×2\n  :b eltype: Float64 dims: draw, chain, run size: 100×4×2\n├──────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2024-03-11T14:10:48.434\" We can also concatenate only a subset of groups and merge the rest, which is useful when some groups are present only in some of the  InferenceData  objects or will be identical in all of them: julia> observed_data = Dict(:y => randn(10));\n\njulia> idata2 = from_dict(data; observed_data=observed_data, coords=coords, dims=dims)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3 = cat(idata, idata2; groups=(:posterior,), dims=:run)\nInferenceData with groups:\n  > posterior\n  > observed_data\n\njulia> idata_cat3.posterior\n╭───────────────────╮\n│ 100×4×3×2 Dataset │\n├───────────────────┴─────────────────────────────────── dims ┐\n  ↓ draw ,\n  → chain,\n  ↗ a_dim Categorical{String} [\"x\", \"y\", \"z\"] ForwardOrdered,\n  ⬔ run\n├─────────────────────────────────────────────────────────────┴ layers ┐\n  :a eltype: Float64 dims: draw, chain, a_dim, run size: 100×4×3×2\n  :b eltype: Float64 dims: draw, chain, run size: 100×4×2\n├──────────────────────────────────────────────────────────── metadata ┤\n  Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2024-03-11T14:10:48.434\"\n\njulia> idata_cat3.observed_data\n╭────────────────────╮\n│ 10-element Dataset │\n├────────────── dims ┤\n  ↓ y_dim_1\n├────────────────────┴─────────────── layers ┐\n  :y eltype: Float64 dims: y_dim_1 size: 10\n├────────────────────────────────────────────┴ metadata ┐\n  Dict{String, Any} with 1 entry:\n  \"created_at\" => \"2024-03-11T14:10:53.539\" source"},{"id":334,"pagetitle":"InferenceData","title":"Base.merge","ref":"/InferenceObjects/stable/inference_data/#Base.merge","content":" Base.merge  —  Function merge(data::InferenceData...) -> InferenceData Merge  InferenceData  objects. The result contains all groups in  data  and  others . If a group appears more than once, the one that occurs last is kept. See also:  cat Examples Here we merge an  InferenceData  containing only a posterior group with one containing only a prior group to create a new one containing both groups. julia> idata1 = from_dict(Dict(:a => randn(100, 4, 3), :b => randn(100, 4)))\nInferenceData with groups:\n  > posterior\n\njulia> idata2 = from_dict(; prior=Dict(:a => randn(100, 1, 3), :c => randn(100, 1)))\nInferenceData with groups:\n  > prior\n\njulia> idata_merged = merge(idata1, idata2)\nInferenceData with groups:\n  > posterior\n  > prior source"},{"id":335,"pagetitle":"InferenceData","title":"I/O extensions","ref":"/InferenceObjects/stable/inference_data/#I/O-extensions","content":" I/O extensions The following types of storage are provided via extensions."},{"id":336,"pagetitle":"InferenceData","title":"NetCDF I/O using NCDatasets.jl","ref":"/InferenceObjects/stable/inference_data/#NetCDF-I/O-using-NCDatasets.jl","content":" NetCDF I/O using NCDatasets.jl"},{"id":337,"pagetitle":"InferenceData","title":"InferenceObjects.from_netcdf","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.from_netcdf","content":" InferenceObjects.from_netcdf  —  Function from_netcdf(path::AbstractString; kwargs...) -> InferenceData Load an  InferenceData  from an unopened NetCDF file. Remaining  kwargs  are passed to  NCDatasets.NCDataset . This method loads data eagerly. To instead load data lazily, pass an opened  NCDataset  to  from_netcdf . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_netcdf(\"centered_eight.nc\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data from_netcdf(ds::NCDatasets.NCDataset; load_mode) -> InferenceData Load an  InferenceData  from an opened NetCDF file. load_mode  defaults to  :lazy , which avoids reading variables into memory. Operations on these arrays will be slow.  load_mode  can also be  :eager , which copies all variables into memory. It is then safe to close  ds . If  load_mode  is  :lazy  and  ds  is closed after constructing  InferenceData , using the variable arrays will have undefined behavior. Examples Here is how we might load an  InferenceData  from an  InferenceData  lazily from a web-hosted NetCDF file. julia> using HTTP, InferenceObjects, NCDatasets\n\njulia> resp = HTTP.get(\"https://github.com/arviz-devs/arviz_example_data/blob/main/data/centered_eight.nc?raw=true\");\n\njulia> ds = NCDataset(\"centered_eight\", \"r\"; memory = resp.body);\n\njulia> idata = from_netcdf(ds)\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > sample_stats\n  > prior\n  > observed_data\n\njulia> idata_copy = copy(idata); # disconnect from the loaded dataset\n\njulia> close(ds); source"},{"id":338,"pagetitle":"InferenceData","title":"InferenceObjects.to_netcdf","ref":"/InferenceObjects/stable/inference_data/#InferenceObjects.to_netcdf","content":" InferenceObjects.to_netcdf  —  Function to_netcdf(data, dest::AbstractString; group::Symbol=:posterior, kwargs...)\nto_netcdf(data, dest::NCDatasets.NCDataset; group::Symbol=:posterior) Write  data  to a NetCDF file. data  is any type that can be converted to an  InferenceData  using  convert_to_inference_data . If not an  InferenceData , then  group  specifies which group the data represents. dest  specifies either the path to the NetCDF file or an opened NetCDF file. If  dest  is a path, remaining  kwargs  are passed to  NCDatasets.NCDataset . Note This method requires that NCDatasets is loaded before it can be used. Examples julia> using InferenceObjects, NCDatasets\n\njulia> idata = from_namedtuple((; x = randn(4, 100, 3), z = randn(4, 100)))\nInferenceData with groups:\n  > posterior\n\njulia> to_netcdf(idata, \"data.nc\")\n\"data.nc\" source"},{"id":341,"pagetitle":"Home","title":"ArviZExampleData","ref":"/ArviZExampleData/stable/#ArviZExampleData","content":" ArviZExampleData This package provides utilities for loading datasets defined in the  arviz_example_data  repository. The resulting objects are  InferenceObjects.jl 's  InferenceData . These utilities are used in  ArviZ.jl ."},{"id":344,"pagetitle":"API","title":"API","ref":"/ArviZExampleData/stable/api/#API","content":" API"},{"id":345,"pagetitle":"API","title":"ArviZExampleData.describe_example_data","ref":"/ArviZExampleData/stable/api/#ArviZExampleData.describe_example_data","content":" ArviZExampleData.describe_example_data  —  Function describe_example_data(name) -> String Return a string containing descriptions of all available datasets. Examples julia> describe_example_data(\"radon\") |> println\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472 source"},{"id":346,"pagetitle":"API","title":"ArviZExampleData.load_example_data","ref":"/ArviZExampleData/stable/api/#ArviZExampleData.load_example_data","content":" ArviZExampleData.load_example_data  —  Function load_example_data(name; kwargs...) -> InferenceObjects.InferenceData\nload_example_data() -> Dict{String,AbstractFileMetadata} Load a local or remote pre-made dataset. kwargs  are forwarded to  InferenceObjects.from_netcdf . Pass no parameters to get a  Dict  listing all available datasets. Data files are handled by DataDeps.jl. A file is downloaded only when it is requested and then cached for future use. Examples julia> keys(load_example_data())\nKeySet for a OrderedCollections.OrderedDict{String, ArviZExampleData.AbstractFileMetadata} with 10 entries. Keys:\n  \"centered_eight\"\n  \"non_centered_eight\"\n  \"radon\"\n  \"rugby\"\n  \"rugby_field\"\n  \"regression1d\"\n  \"regression10d\"\n  \"classification1d\"\n  \"classification10d\"\n  \"glycan_torsion_angles\"\n\njulia> load_example_data(\"centered_eight\")\nInferenceData with groups:\n  > posterior\n  > posterior_predictive\n  > log_likelihood\n  > sample_stats\n  > prior\n  > prior_predictive\n  > observed_data\n  > constant_data source"},{"id":349,"pagetitle":"Datasets","title":"Datasets","ref":"/ArviZExampleData/stable/datasets/#Datasets","content":" Datasets The following shows the currently available example datasets: using ArviZExampleData\n\nprintln(describe_example_data()) centered_eight\n==============\n\nA centered parameterization of the eight schools model. Provided as an example of a model that NUTS has trouble fitting. Compare to `non_centered_eight`.\n\nThe eight schools model is a hierarchical model used for an analysis of the effectiveness of classes that were designed to improve students' performance on the Scholastic Aptitude Test.\n\nSee Bayesian Data Analysis (Gelman et. al.) for more details.\n\nlocal: /home/runner/.julia/artifacts/88acee5c1db592b660a577a21fa9bdc783ec49d7/arviz_example_data-0.2.0/data/centered_eight.nc\n\nnon_centered_eight\n==================\n\nA non-centered parameterization of the eight schools model. This is a hierarchical model where sampling problems may be fixed by a non-centered parametrization. Compare to `centered_eight`.\n\nThe eight schools model is a hierarchical model used for an analysis of the effectiveness of classes that were designed to improve students' performance on the Scholastic Aptitude Test.\n\nSee Bayesian Data Analysis (Gelman et. al.) for more details.\n\nlocal: /home/runner/.julia/artifacts/88acee5c1db592b660a577a21fa9bdc783ec49d7/arviz_example_data-0.2.0/data/non_centered_eight.nc\n\nradon\n=====\n\nRadon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household.\n\nThis example uses an EPA study of radon levels in houses in Minnesota to construct a model with a hierarchy over households within a county. The model includes estimates (gamma) for contextual effects of the uranium per household.\n\nSee Gelman and Hill (2006) for details on the example, or https://docs.pymc.io/notebooks/multilevel_modeling.html by Chris Fonnesbeck for details on this implementation.\n\nremote: http://ndownloader.figshare.com/files/24067472\n\nrugby\n=====\n\nThe Six Nations Championship is a yearly rugby competition between Italy, Ireland, Scotland, England, France and Wales. Fifteen games are played each year, representing all combinations of the six teams.\n\nThis example uses and includes results from 2014 - 2017, comprising 60 total games. It models latent parameters for each team's attack and defense, as well as a global parameter for home team advantage.\n\nSee https://github.com/arviz-devs/arviz_example_data/blob/main/code/rugby/rugby.ipynb for the whole model specification.\n\nremote: http://figshare.com/ndownloader/files/44916469\n\nrugby_field\n===========\n\nA variant of the 'rugby' example dataset. The Six Nations Championship is a yearly rugby competition between Italy, Ireland, Scotland, England, France and Wales. Fifteen games are played each year, representing all combinations of the six teams.\n\nThis example uses and includes results from 2014 - 2017, comprising 60 total games. It models latent parameters for each team's attack and defense, with each team having different values depending on them being home or away team.\n\nSee https://github.com/arviz-devs/arviz_example_data/blob/main/code/rugby_field/rugby_field.ipynb for the whole model specification.\n\nremote: http://figshare.com/ndownloader/files/44667112\n\nregression1d\n============\n\nA synthetic one dimensional linear regression dataset with latent slope, intercept, and noise (\"eps\"). One hundred data points, fit with PyMC3.\n\nTrue slope and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16254899\n\nregression10d\n=============\n\nA synthetic multi-dimensional (10 dimensions) linear regression dataset with latent weights (\"w\"), intercept, and noise (\"eps\"). Five hundred data points, fit with PyMC3.\n\nTrue weights and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16255736\n\nclassification1d\n================\n\nA synthetic one dimensional logistic regression dataset with latent slope and intercept, passed into a Bernoulli random variable. One hundred data points, fit with PyMC3.\n\nTrue slope and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16256678\n\nclassification10d\n=================\n\nA synthetic multi dimensional (10 dimensions) logistic regression dataset with latent weights (\"w\") and intercept, passed into a Bernoulli random variable. Five hundred data points, fit with PyMC3.\n\nTrue weights and intercept are included as deterministic variables.\n\nremote: http://ndownloader.figshare.com/files/16256681\n\nglycan_torsion_angles\n=====================\n\nTorsion angles phi and psi are critical for determining the three dimensional structure of bio-molecules. Combinations of phi and psi torsion angles that produce clashes between atoms in the bio-molecule result in high energy, unlikely structures.\n\nThis model uses a Von Mises distribution to propose torsion angles for the structure of a glycan molecule (pdb id: 2LIQ), and a Potential to estimate the proposed structure's energy. Said Potential is bound by Boltzman's law.\n\nremote: http://ndownloader.figshare.com/files/22882652"},{"id":352,"pagetitle":"For developers","title":"For developers","ref":"/ArviZExampleData/stable/for_developers/#For-developers","content":" For developers This package has  arviz_example_data  as a data dependency, which is included as an  artifact . When  arviz_example_data  is updated, and a new release is made,  Artifacts.toml  should be updated to point to the new tarball corresponding to the release: julia> using ArtifactUtils\n\njulia> version = v\"0.1.0\";\n\njulia> tarball_url = \"https://github.com/arviz-devs/arviz_example_data/archive/refs/tags/v$version.tar.gz\";\n\njulia> add_artifact!(\"Artifacts.toml\", \"arviz_example_data\", tarball_url; force=true);"}]