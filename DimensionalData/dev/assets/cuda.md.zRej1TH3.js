import{_ as a,c as s,o as i,V as e}from"./chunks/framework.-hqW-2cI.js";const y=JSON.parse('{"title":"CUDA & GPUs","description":"","frontmatter":{},"headers":[],"relativePath":"cuda.md","filePath":"cuda.md","lastUpdated":null}'),t={name:"cuda.md"},n=e(`<h1 id="CUDA-and-GPUs" tabindex="-1">CUDA &amp; GPUs <a class="header-anchor" href="#CUDA-and-GPUs" aria-label="Permalink to &quot;CUDA &amp; GPUs {#CUDA-and-GPUs}&quot;">â€‹</a></h1><p>Running regular julia code on GPUs is one of the most amazing things about the language. DimensionalData.jl leans into this as much as possible.</p><p>From the beginning DimensionalData.jl has had two GPU-related goals:</p><ol><li>Work seamlessly with Base julia broadcasts and other operations that already</li></ol><p>work on GPU.</p><ol><li>Work as arguments to custom GPU kernel funcions.</li></ol><p>This means any <code>AbstractDimArray</code> must be automatically moved to the gpu and its fields converted to GPU friendly forms whenever required, using <a href="https://github.com/JuliaGPU/Adapt.jl" target="_blank" rel="noreferrer">Adapt.jl</a>).</p><ul><li><p>The array data must converts to the correct GPU array backend when <code>Adapt.adapt(dimarray)</code> is called.</p></li><li><p>All DimensionalData.jl objects, except the actual parent array, need to be immutable <code>isbits</code> or convertable to them. This is one reason DimensionalData.jl uses <code>rebuild</code> and a functional style, rather than in-place modification of fields.</p></li><li><p>Symbols need to be moved to the type system <code>Name{:layer_name}()</code> replaces <code>:layer_name</code></p></li><li><p>Metadata dicts need to be stripped, they are often too difficult to convert, and not needed on GPU.</p></li></ul><p>As an example, DynamicGrids.jl uses <code>AbstractDimArray</code> for auxiliary model data that are passed into <a href="https://github.com/JuliaGPU/KernelAbstractions.jl" target="_blank" rel="noreferrer">KernelAbstractions.jl</a>/ <a href="https://github.com/JuliaGPU/CUDA.jl" target="_blank" rel="noreferrer">CUDA.jl</a> kernels.</p><p>Note: due to limitations of the machines available in our github actions CI, we <em>do not</em> currently test on GPU. But we should.</p><p>If anyone wants to set us up with CI that has a GPU, please make a PR!</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> DimensionalData, CUDA</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Create a Float32 array to use on the GPU</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">A </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> rand</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(Float32, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">X</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1000.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Y</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2000.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Move the parent data to the GPU with \`modify\` and the \`CuArray\` constructor:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cuA </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> modify</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(CuArray, A)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Broadcast to a new GPU array: it will still be a DimArray!</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cuA2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> cuA </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span></code></pre></div>`,12),l=[n];function r(h,p,o,d,k,c){return i(),s("div",null,l)}const m=a(t,[["render",r]]);export{y as __pageData,m as default};
